{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 99.26022304832713,
  "eval_steps": 500,
  "global_step": 13400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.07434944237918216,
      "grad_norm": 0.6509204506874084,
      "learning_rate": 1.9986666666666666e-05,
      "loss": 0.29,
      "step": 10
    },
    {
      "epoch": 0.14869888475836432,
      "grad_norm": 0.5490792393684387,
      "learning_rate": 1.9971851851851854e-05,
      "loss": 0.2609,
      "step": 20
    },
    {
      "epoch": 0.22304832713754646,
      "grad_norm": 0.5180911421775818,
      "learning_rate": 1.995703703703704e-05,
      "loss": 0.2606,
      "step": 30
    },
    {
      "epoch": 0.29739776951672864,
      "grad_norm": 0.3856959342956543,
      "learning_rate": 1.994222222222222e-05,
      "loss": 0.2486,
      "step": 40
    },
    {
      "epoch": 0.37174721189591076,
      "grad_norm": 0.43117403984069824,
      "learning_rate": 1.992740740740741e-05,
      "loss": 0.2439,
      "step": 50
    },
    {
      "epoch": 0.44609665427509293,
      "grad_norm": 0.4015258848667145,
      "learning_rate": 1.9912592592592596e-05,
      "loss": 0.221,
      "step": 60
    },
    {
      "epoch": 0.5204460966542751,
      "grad_norm": 0.34904372692108154,
      "learning_rate": 1.989777777777778e-05,
      "loss": 0.2112,
      "step": 70
    },
    {
      "epoch": 0.5947955390334573,
      "grad_norm": 0.24359622597694397,
      "learning_rate": 1.9882962962962964e-05,
      "loss": 0.2448,
      "step": 80
    },
    {
      "epoch": 0.6691449814126395,
      "grad_norm": 0.6464946269989014,
      "learning_rate": 1.986814814814815e-05,
      "loss": 0.2212,
      "step": 90
    },
    {
      "epoch": 0.7434944237918215,
      "grad_norm": 0.3698827624320984,
      "learning_rate": 1.9853333333333335e-05,
      "loss": 0.2136,
      "step": 100
    },
    {
      "epoch": 0.8178438661710037,
      "grad_norm": 0.3117164075374603,
      "learning_rate": 1.983851851851852e-05,
      "loss": 0.211,
      "step": 110
    },
    {
      "epoch": 0.8921933085501859,
      "grad_norm": 0.3737461566925049,
      "learning_rate": 1.9823703703703707e-05,
      "loss": 0.1807,
      "step": 120
    },
    {
      "epoch": 0.966542750929368,
      "grad_norm": 0.345855712890625,
      "learning_rate": 1.980888888888889e-05,
      "loss": 0.2054,
      "step": 130
    },
    {
      "epoch": 1.037174721189591,
      "grad_norm": 0.2826257348060608,
      "learning_rate": 1.9794074074074078e-05,
      "loss": 0.2133,
      "step": 140
    },
    {
      "epoch": 1.1115241635687731,
      "grad_norm": 0.28994059562683105,
      "learning_rate": 1.9779259259259262e-05,
      "loss": 0.1811,
      "step": 150
    },
    {
      "epoch": 1.1858736059479553,
      "grad_norm": 0.2672259509563446,
      "learning_rate": 1.9764444444444446e-05,
      "loss": 0.2083,
      "step": 160
    },
    {
      "epoch": 1.2602230483271375,
      "grad_norm": 0.3231388032436371,
      "learning_rate": 1.9749629629629633e-05,
      "loss": 0.1837,
      "step": 170
    },
    {
      "epoch": 1.3345724907063197,
      "grad_norm": 0.30745136737823486,
      "learning_rate": 1.9734814814814817e-05,
      "loss": 0.1763,
      "step": 180
    },
    {
      "epoch": 1.4089219330855018,
      "grad_norm": 0.28916871547698975,
      "learning_rate": 1.972e-05,
      "loss": 0.1951,
      "step": 190
    },
    {
      "epoch": 1.483271375464684,
      "grad_norm": 0.3216005265712738,
      "learning_rate": 1.9705185185185188e-05,
      "loss": 0.1826,
      "step": 200
    },
    {
      "epoch": 1.5576208178438662,
      "grad_norm": 0.3597724139690399,
      "learning_rate": 1.9690370370370372e-05,
      "loss": 0.1705,
      "step": 210
    },
    {
      "epoch": 1.6319702602230484,
      "grad_norm": 0.3063552677631378,
      "learning_rate": 1.9675555555555556e-05,
      "loss": 0.1758,
      "step": 220
    },
    {
      "epoch": 1.7063197026022305,
      "grad_norm": 0.3024512529373169,
      "learning_rate": 1.9660740740740743e-05,
      "loss": 0.1824,
      "step": 230
    },
    {
      "epoch": 1.7806691449814127,
      "grad_norm": 0.360331267118454,
      "learning_rate": 1.9645925925925927e-05,
      "loss": 0.1781,
      "step": 240
    },
    {
      "epoch": 1.8550185873605947,
      "grad_norm": 0.42251715064048767,
      "learning_rate": 1.963111111111111e-05,
      "loss": 0.1982,
      "step": 250
    },
    {
      "epoch": 1.929368029739777,
      "grad_norm": 0.33815914392471313,
      "learning_rate": 1.96162962962963e-05,
      "loss": 0.2006,
      "step": 260
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.44240787625312805,
      "learning_rate": 1.9601481481481483e-05,
      "loss": 0.1877,
      "step": 270
    },
    {
      "epoch": 2.074349442379182,
      "grad_norm": 0.3815724551677704,
      "learning_rate": 1.9586666666666667e-05,
      "loss": 0.1813,
      "step": 280
    },
    {
      "epoch": 2.1486988847583643,
      "grad_norm": 0.3460845947265625,
      "learning_rate": 1.9571851851851854e-05,
      "loss": 0.1802,
      "step": 290
    },
    {
      "epoch": 2.2230483271375463,
      "grad_norm": 0.2325858771800995,
      "learning_rate": 1.9557037037037038e-05,
      "loss": 0.1813,
      "step": 300
    },
    {
      "epoch": 2.2973977695167287,
      "grad_norm": 0.3425512909889221,
      "learning_rate": 1.9542222222222225e-05,
      "loss": 0.171,
      "step": 310
    },
    {
      "epoch": 2.3717472118959106,
      "grad_norm": 0.4303150773048401,
      "learning_rate": 1.952740740740741e-05,
      "loss": 0.171,
      "step": 320
    },
    {
      "epoch": 2.446096654275093,
      "grad_norm": 0.3277320861816406,
      "learning_rate": 1.9512592592592593e-05,
      "loss": 0.1551,
      "step": 330
    },
    {
      "epoch": 2.520446096654275,
      "grad_norm": 0.41543814539909363,
      "learning_rate": 1.949777777777778e-05,
      "loss": 0.1708,
      "step": 340
    },
    {
      "epoch": 2.5947955390334574,
      "grad_norm": 0.48815280199050903,
      "learning_rate": 1.9482962962962964e-05,
      "loss": 0.1738,
      "step": 350
    },
    {
      "epoch": 2.6691449814126393,
      "grad_norm": 0.30526503920555115,
      "learning_rate": 1.946814814814815e-05,
      "loss": 0.1954,
      "step": 360
    },
    {
      "epoch": 2.7434944237918213,
      "grad_norm": 0.3948207199573517,
      "learning_rate": 1.9453333333333336e-05,
      "loss": 0.1917,
      "step": 370
    },
    {
      "epoch": 2.8178438661710037,
      "grad_norm": 0.3460526764392853,
      "learning_rate": 1.943851851851852e-05,
      "loss": 0.1757,
      "step": 380
    },
    {
      "epoch": 2.892193308550186,
      "grad_norm": 0.2760305106639862,
      "learning_rate": 1.9423703703703704e-05,
      "loss": 0.1739,
      "step": 390
    },
    {
      "epoch": 2.966542750929368,
      "grad_norm": 0.2958475947380066,
      "learning_rate": 1.940888888888889e-05,
      "loss": 0.1645,
      "step": 400
    },
    {
      "epoch": 3.037174721189591,
      "grad_norm": 0.414378821849823,
      "learning_rate": 1.9394074074074075e-05,
      "loss": 0.1549,
      "step": 410
    },
    {
      "epoch": 3.1115241635687734,
      "grad_norm": 0.23683708906173706,
      "learning_rate": 1.937925925925926e-05,
      "loss": 0.1732,
      "step": 420
    },
    {
      "epoch": 3.1858736059479553,
      "grad_norm": 0.35535040497779846,
      "learning_rate": 1.9364444444444446e-05,
      "loss": 0.1599,
      "step": 430
    },
    {
      "epoch": 3.2602230483271377,
      "grad_norm": 0.46105965971946716,
      "learning_rate": 1.934962962962963e-05,
      "loss": 0.1749,
      "step": 440
    },
    {
      "epoch": 3.3345724907063197,
      "grad_norm": 0.2554346024990082,
      "learning_rate": 1.9334814814814817e-05,
      "loss": 0.1468,
      "step": 450
    },
    {
      "epoch": 3.4089219330855016,
      "grad_norm": 0.35041457414627075,
      "learning_rate": 1.932e-05,
      "loss": 0.165,
      "step": 460
    },
    {
      "epoch": 3.483271375464684,
      "grad_norm": 0.42959150671958923,
      "learning_rate": 1.9305185185185185e-05,
      "loss": 0.168,
      "step": 470
    },
    {
      "epoch": 3.5576208178438664,
      "grad_norm": 0.33948615193367004,
      "learning_rate": 1.9290370370370373e-05,
      "loss": 0.1646,
      "step": 480
    },
    {
      "epoch": 3.6319702602230484,
      "grad_norm": 0.28344133496284485,
      "learning_rate": 1.9275555555555557e-05,
      "loss": 0.16,
      "step": 490
    },
    {
      "epoch": 3.7063197026022303,
      "grad_norm": 0.4012501835823059,
      "learning_rate": 1.926074074074074e-05,
      "loss": 0.185,
      "step": 500
    },
    {
      "epoch": 3.7806691449814127,
      "grad_norm": 0.41912221908569336,
      "learning_rate": 1.9245925925925928e-05,
      "loss": 0.1895,
      "step": 510
    },
    {
      "epoch": 3.8550185873605947,
      "grad_norm": 0.27407464385032654,
      "learning_rate": 1.9231111111111115e-05,
      "loss": 0.1756,
      "step": 520
    },
    {
      "epoch": 3.929368029739777,
      "grad_norm": 0.3652651906013489,
      "learning_rate": 1.9216296296296296e-05,
      "loss": 0.1744,
      "step": 530
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.42634204030036926,
      "learning_rate": 1.9201481481481483e-05,
      "loss": 0.1643,
      "step": 540
    },
    {
      "epoch": 4.074349442379182,
      "grad_norm": 0.46613243222236633,
      "learning_rate": 1.918666666666667e-05,
      "loss": 0.1595,
      "step": 550
    },
    {
      "epoch": 4.148698884758364,
      "grad_norm": 0.38579368591308594,
      "learning_rate": 1.917185185185185e-05,
      "loss": 0.163,
      "step": 560
    },
    {
      "epoch": 4.223048327137547,
      "grad_norm": 0.31354400515556335,
      "learning_rate": 1.915703703703704e-05,
      "loss": 0.1671,
      "step": 570
    },
    {
      "epoch": 4.297397769516729,
      "grad_norm": 0.41671738028526306,
      "learning_rate": 1.9142222222222226e-05,
      "loss": 0.1773,
      "step": 580
    },
    {
      "epoch": 4.371747211895911,
      "grad_norm": 0.36763104796409607,
      "learning_rate": 1.9127407407407406e-05,
      "loss": 0.1499,
      "step": 590
    },
    {
      "epoch": 4.446096654275093,
      "grad_norm": 0.4180821180343628,
      "learning_rate": 1.9112592592592594e-05,
      "loss": 0.1606,
      "step": 600
    },
    {
      "epoch": 4.520446096654275,
      "grad_norm": 0.3701416254043579,
      "learning_rate": 1.909777777777778e-05,
      "loss": 0.1826,
      "step": 610
    },
    {
      "epoch": 4.594795539033457,
      "grad_norm": 0.33993691205978394,
      "learning_rate": 1.9082962962962965e-05,
      "loss": 0.1634,
      "step": 620
    },
    {
      "epoch": 4.669144981412639,
      "grad_norm": 0.4048694968223572,
      "learning_rate": 1.906814814814815e-05,
      "loss": 0.1819,
      "step": 630
    },
    {
      "epoch": 4.743494423791821,
      "grad_norm": 0.2931894063949585,
      "learning_rate": 1.9053333333333336e-05,
      "loss": 0.172,
      "step": 640
    },
    {
      "epoch": 4.817843866171003,
      "grad_norm": 0.49733254313468933,
      "learning_rate": 1.903851851851852e-05,
      "loss": 0.1654,
      "step": 650
    },
    {
      "epoch": 4.892193308550186,
      "grad_norm": 0.45007866621017456,
      "learning_rate": 1.9023703703703704e-05,
      "loss": 0.1498,
      "step": 660
    },
    {
      "epoch": 4.966542750929368,
      "grad_norm": 0.43353134393692017,
      "learning_rate": 1.900888888888889e-05,
      "loss": 0.1698,
      "step": 670
    },
    {
      "epoch": 5.037174721189591,
      "grad_norm": 0.3579436242580414,
      "learning_rate": 1.8994074074074075e-05,
      "loss": 0.1644,
      "step": 680
    },
    {
      "epoch": 5.111524163568773,
      "grad_norm": 0.43046316504478455,
      "learning_rate": 1.8979259259259263e-05,
      "loss": 0.1686,
      "step": 690
    },
    {
      "epoch": 5.185873605947956,
      "grad_norm": 0.37608009576797485,
      "learning_rate": 1.8964444444444447e-05,
      "loss": 0.1736,
      "step": 700
    },
    {
      "epoch": 5.260223048327138,
      "grad_norm": 0.4322211444377899,
      "learning_rate": 1.894962962962963e-05,
      "loss": 0.1664,
      "step": 710
    },
    {
      "epoch": 5.33457249070632,
      "grad_norm": 0.46317583322525024,
      "learning_rate": 1.8934814814814818e-05,
      "loss": 0.1604,
      "step": 720
    },
    {
      "epoch": 5.408921933085502,
      "grad_norm": 0.38307246565818787,
      "learning_rate": 1.8920000000000002e-05,
      "loss": 0.1613,
      "step": 730
    },
    {
      "epoch": 5.483271375464684,
      "grad_norm": 0.38906097412109375,
      "learning_rate": 1.8905185185185186e-05,
      "loss": 0.1589,
      "step": 740
    },
    {
      "epoch": 5.557620817843866,
      "grad_norm": 0.47961392998695374,
      "learning_rate": 1.8890370370370373e-05,
      "loss": 0.1514,
      "step": 750
    },
    {
      "epoch": 5.631970260223048,
      "grad_norm": 0.4297870397567749,
      "learning_rate": 1.8875555555555557e-05,
      "loss": 0.1698,
      "step": 760
    },
    {
      "epoch": 5.70631970260223,
      "grad_norm": 0.494995653629303,
      "learning_rate": 1.886074074074074e-05,
      "loss": 0.1739,
      "step": 770
    },
    {
      "epoch": 5.780669144981412,
      "grad_norm": 0.39114609360694885,
      "learning_rate": 1.884592592592593e-05,
      "loss": 0.1696,
      "step": 780
    },
    {
      "epoch": 5.855018587360595,
      "grad_norm": 0.4945548176765442,
      "learning_rate": 1.8831111111111112e-05,
      "loss": 0.1584,
      "step": 790
    },
    {
      "epoch": 5.929368029739777,
      "grad_norm": 0.3457413911819458,
      "learning_rate": 1.8816296296296296e-05,
      "loss": 0.1609,
      "step": 800
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.5805288553237915,
      "learning_rate": 1.8801481481481484e-05,
      "loss": 0.1605,
      "step": 810
    },
    {
      "epoch": 6.074349442379182,
      "grad_norm": 0.4410359263420105,
      "learning_rate": 1.8786666666666667e-05,
      "loss": 0.1677,
      "step": 820
    },
    {
      "epoch": 6.148698884758364,
      "grad_norm": 0.40789657831192017,
      "learning_rate": 1.8771851851851855e-05,
      "loss": 0.1709,
      "step": 830
    },
    {
      "epoch": 6.223048327137547,
      "grad_norm": 0.47912847995758057,
      "learning_rate": 1.875703703703704e-05,
      "loss": 0.1778,
      "step": 840
    },
    {
      "epoch": 6.297397769516729,
      "grad_norm": 0.3151934742927551,
      "learning_rate": 1.8742222222222223e-05,
      "loss": 0.1705,
      "step": 850
    },
    {
      "epoch": 6.371747211895911,
      "grad_norm": 0.5297712683677673,
      "learning_rate": 1.872740740740741e-05,
      "loss": 0.1608,
      "step": 860
    },
    {
      "epoch": 6.446096654275093,
      "grad_norm": 0.38088086247444153,
      "learning_rate": 1.8712592592592594e-05,
      "loss": 0.1365,
      "step": 870
    },
    {
      "epoch": 6.520446096654275,
      "grad_norm": 0.36072251200675964,
      "learning_rate": 1.8697777777777778e-05,
      "loss": 0.1672,
      "step": 880
    },
    {
      "epoch": 6.594795539033457,
      "grad_norm": 0.40420690178871155,
      "learning_rate": 1.8682962962962965e-05,
      "loss": 0.1721,
      "step": 890
    },
    {
      "epoch": 6.669144981412639,
      "grad_norm": 0.3984401524066925,
      "learning_rate": 1.866814814814815e-05,
      "loss": 0.1455,
      "step": 900
    },
    {
      "epoch": 6.743494423791821,
      "grad_norm": 0.5115908980369568,
      "learning_rate": 1.8653333333333333e-05,
      "loss": 0.1734,
      "step": 910
    },
    {
      "epoch": 6.817843866171003,
      "grad_norm": 0.48289746046066284,
      "learning_rate": 1.863851851851852e-05,
      "loss": 0.1712,
      "step": 920
    },
    {
      "epoch": 6.892193308550186,
      "grad_norm": 0.4604434072971344,
      "learning_rate": 1.8623703703703704e-05,
      "loss": 0.1542,
      "step": 930
    },
    {
      "epoch": 6.966542750929368,
      "grad_norm": 0.4046984910964966,
      "learning_rate": 1.860888888888889e-05,
      "loss": 0.1556,
      "step": 940
    },
    {
      "epoch": 7.037174721189591,
      "grad_norm": 0.3557681739330292,
      "learning_rate": 1.8594074074074076e-05,
      "loss": 0.1539,
      "step": 950
    },
    {
      "epoch": 7.111524163568773,
      "grad_norm": 0.3947390019893646,
      "learning_rate": 1.857925925925926e-05,
      "loss": 0.1496,
      "step": 960
    },
    {
      "epoch": 7.185873605947956,
      "grad_norm": 0.48873695731163025,
      "learning_rate": 1.8564444444444444e-05,
      "loss": 0.1486,
      "step": 970
    },
    {
      "epoch": 7.260223048327138,
      "grad_norm": 0.37936848402023315,
      "learning_rate": 1.854962962962963e-05,
      "loss": 0.1618,
      "step": 980
    },
    {
      "epoch": 7.33457249070632,
      "grad_norm": 0.41823720932006836,
      "learning_rate": 1.8534814814814815e-05,
      "loss": 0.1527,
      "step": 990
    },
    {
      "epoch": 7.408921933085502,
      "grad_norm": 0.29116472601890564,
      "learning_rate": 1.8520000000000002e-05,
      "loss": 0.158,
      "step": 1000
    },
    {
      "epoch": 7.483271375464684,
      "grad_norm": 0.4016987979412079,
      "learning_rate": 1.8505185185185186e-05,
      "loss": 0.1641,
      "step": 1010
    },
    {
      "epoch": 7.557620817843866,
      "grad_norm": 0.33678585290908813,
      "learning_rate": 1.8490370370370374e-05,
      "loss": 0.1646,
      "step": 1020
    },
    {
      "epoch": 7.631970260223048,
      "grad_norm": 0.4744092524051666,
      "learning_rate": 1.8475555555555557e-05,
      "loss": 0.1347,
      "step": 1030
    },
    {
      "epoch": 7.70631970260223,
      "grad_norm": 0.4984130859375,
      "learning_rate": 1.846074074074074e-05,
      "loss": 0.1742,
      "step": 1040
    },
    {
      "epoch": 7.780669144981412,
      "grad_norm": 0.4441162049770355,
      "learning_rate": 1.844592592592593e-05,
      "loss": 0.1766,
      "step": 1050
    },
    {
      "epoch": 7.855018587360595,
      "grad_norm": 0.4827544093132019,
      "learning_rate": 1.8431111111111113e-05,
      "loss": 0.1752,
      "step": 1060
    },
    {
      "epoch": 7.929368029739777,
      "grad_norm": 0.5046246647834778,
      "learning_rate": 1.84162962962963e-05,
      "loss": 0.1523,
      "step": 1070
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.6588957905769348,
      "learning_rate": 1.8401481481481484e-05,
      "loss": 0.1738,
      "step": 1080
    },
    {
      "epoch": 8.074349442379182,
      "grad_norm": 0.44207215309143066,
      "learning_rate": 1.8386666666666668e-05,
      "loss": 0.1499,
      "step": 1090
    },
    {
      "epoch": 8.148698884758364,
      "grad_norm": 0.45954006910324097,
      "learning_rate": 1.8371851851851855e-05,
      "loss": 0.1518,
      "step": 1100
    },
    {
      "epoch": 8.223048327137546,
      "grad_norm": 0.5941627621650696,
      "learning_rate": 1.835703703703704e-05,
      "loss": 0.1477,
      "step": 1110
    },
    {
      "epoch": 8.297397769516728,
      "grad_norm": 0.482963502407074,
      "learning_rate": 1.8342222222222223e-05,
      "loss": 0.1702,
      "step": 1120
    },
    {
      "epoch": 8.371747211895912,
      "grad_norm": 0.47147294878959656,
      "learning_rate": 1.832740740740741e-05,
      "loss": 0.1466,
      "step": 1130
    },
    {
      "epoch": 8.446096654275093,
      "grad_norm": 0.5476743578910828,
      "learning_rate": 1.8312592592592594e-05,
      "loss": 0.1686,
      "step": 1140
    },
    {
      "epoch": 8.520446096654275,
      "grad_norm": 0.3771396577358246,
      "learning_rate": 1.829777777777778e-05,
      "loss": 0.1653,
      "step": 1150
    },
    {
      "epoch": 8.594795539033457,
      "grad_norm": 0.4661332964897156,
      "learning_rate": 1.8282962962962966e-05,
      "loss": 0.1571,
      "step": 1160
    },
    {
      "epoch": 8.66914498141264,
      "grad_norm": 0.5345790982246399,
      "learning_rate": 1.826814814814815e-05,
      "loss": 0.148,
      "step": 1170
    },
    {
      "epoch": 8.743494423791821,
      "grad_norm": 0.5133805871009827,
      "learning_rate": 1.8253333333333334e-05,
      "loss": 0.1588,
      "step": 1180
    },
    {
      "epoch": 8.817843866171003,
      "grad_norm": 0.44190821051597595,
      "learning_rate": 1.823851851851852e-05,
      "loss": 0.1641,
      "step": 1190
    },
    {
      "epoch": 8.892193308550185,
      "grad_norm": 0.4666944146156311,
      "learning_rate": 1.8223703703703705e-05,
      "loss": 0.1776,
      "step": 1200
    },
    {
      "epoch": 8.966542750929367,
      "grad_norm": 0.48290783166885376,
      "learning_rate": 1.820888888888889e-05,
      "loss": 0.159,
      "step": 1210
    },
    {
      "epoch": 9.037174721189592,
      "grad_norm": 0.4456577003002167,
      "learning_rate": 1.8194074074074076e-05,
      "loss": 0.1744,
      "step": 1220
    },
    {
      "epoch": 9.111524163568774,
      "grad_norm": 0.45157793164253235,
      "learning_rate": 1.817925925925926e-05,
      "loss": 0.1596,
      "step": 1230
    },
    {
      "epoch": 9.185873605947956,
      "grad_norm": 0.4067098796367645,
      "learning_rate": 1.8164444444444447e-05,
      "loss": 0.1334,
      "step": 1240
    },
    {
      "epoch": 9.260223048327138,
      "grad_norm": 0.4081020951271057,
      "learning_rate": 1.814962962962963e-05,
      "loss": 0.1493,
      "step": 1250
    },
    {
      "epoch": 9.33457249070632,
      "grad_norm": 0.5595722794532776,
      "learning_rate": 1.8134814814814815e-05,
      "loss": 0.1728,
      "step": 1260
    },
    {
      "epoch": 9.408921933085502,
      "grad_norm": 0.3838607966899872,
      "learning_rate": 1.8120000000000003e-05,
      "loss": 0.1607,
      "step": 1270
    },
    {
      "epoch": 9.483271375464684,
      "grad_norm": 0.548076331615448,
      "learning_rate": 1.8105185185185187e-05,
      "loss": 0.1478,
      "step": 1280
    },
    {
      "epoch": 9.557620817843866,
      "grad_norm": 0.550390899181366,
      "learning_rate": 1.809037037037037e-05,
      "loss": 0.1582,
      "step": 1290
    },
    {
      "epoch": 9.63197026022305,
      "grad_norm": 0.3992627263069153,
      "learning_rate": 1.8075555555555558e-05,
      "loss": 0.1503,
      "step": 1300
    },
    {
      "epoch": 9.706319702602231,
      "grad_norm": 0.47520875930786133,
      "learning_rate": 1.8060740740740742e-05,
      "loss": 0.1604,
      "step": 1310
    },
    {
      "epoch": 9.780669144981413,
      "grad_norm": 0.5748857259750366,
      "learning_rate": 1.8045925925925926e-05,
      "loss": 0.1622,
      "step": 1320
    },
    {
      "epoch": 9.855018587360595,
      "grad_norm": 0.5006943941116333,
      "learning_rate": 1.8031111111111113e-05,
      "loss": 0.163,
      "step": 1330
    },
    {
      "epoch": 9.929368029739777,
      "grad_norm": 0.5347557067871094,
      "learning_rate": 1.8016296296296297e-05,
      "loss": 0.1732,
      "step": 1340
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.6066468358039856,
      "learning_rate": 1.800148148148148e-05,
      "loss": 0.1584,
      "step": 1350
    },
    {
      "epoch": 10.074349442379182,
      "grad_norm": 0.4516071081161499,
      "learning_rate": 1.798666666666667e-05,
      "loss": 0.1562,
      "step": 1360
    },
    {
      "epoch": 10.148698884758364,
      "grad_norm": 0.6744762659072876,
      "learning_rate": 1.7971851851851852e-05,
      "loss": 0.1643,
      "step": 1370
    },
    {
      "epoch": 10.223048327137546,
      "grad_norm": 0.532539427280426,
      "learning_rate": 1.795703703703704e-05,
      "loss": 0.1672,
      "step": 1380
    },
    {
      "epoch": 10.297397769516728,
      "grad_norm": 0.5452308654785156,
      "learning_rate": 1.7942222222222224e-05,
      "loss": 0.1477,
      "step": 1390
    },
    {
      "epoch": 10.371747211895912,
      "grad_norm": 0.460560142993927,
      "learning_rate": 1.7927407407407408e-05,
      "loss": 0.1684,
      "step": 1400
    },
    {
      "epoch": 10.446096654275093,
      "grad_norm": 0.46395817399024963,
      "learning_rate": 1.7912592592592595e-05,
      "loss": 0.1487,
      "step": 1410
    },
    {
      "epoch": 10.520446096654275,
      "grad_norm": 0.4683505594730377,
      "learning_rate": 1.789777777777778e-05,
      "loss": 0.1695,
      "step": 1420
    },
    {
      "epoch": 10.594795539033457,
      "grad_norm": 0.41002726554870605,
      "learning_rate": 1.7882962962962963e-05,
      "loss": 0.1653,
      "step": 1430
    },
    {
      "epoch": 10.66914498141264,
      "grad_norm": 0.5236505270004272,
      "learning_rate": 1.786814814814815e-05,
      "loss": 0.1682,
      "step": 1440
    },
    {
      "epoch": 10.743494423791821,
      "grad_norm": 0.5654604434967041,
      "learning_rate": 1.7853333333333337e-05,
      "loss": 0.1553,
      "step": 1450
    },
    {
      "epoch": 10.817843866171003,
      "grad_norm": 0.37488359212875366,
      "learning_rate": 1.7838518518518518e-05,
      "loss": 0.1308,
      "step": 1460
    },
    {
      "epoch": 10.892193308550185,
      "grad_norm": 0.6237924098968506,
      "learning_rate": 1.7823703703703705e-05,
      "loss": 0.1721,
      "step": 1470
    },
    {
      "epoch": 10.966542750929367,
      "grad_norm": 0.6092290878295898,
      "learning_rate": 1.7808888888888893e-05,
      "loss": 0.1481,
      "step": 1480
    },
    {
      "epoch": 11.037174721189592,
      "grad_norm": 0.4596635699272156,
      "learning_rate": 1.7794074074074073e-05,
      "loss": 0.1447,
      "step": 1490
    },
    {
      "epoch": 11.111524163568774,
      "grad_norm": 0.5363184809684753,
      "learning_rate": 1.777925925925926e-05,
      "loss": 0.1446,
      "step": 1500
    },
    {
      "epoch": 11.185873605947956,
      "grad_norm": 0.6001824140548706,
      "learning_rate": 1.7764444444444448e-05,
      "loss": 0.1536,
      "step": 1510
    },
    {
      "epoch": 11.260223048327138,
      "grad_norm": 0.47018229961395264,
      "learning_rate": 1.774962962962963e-05,
      "loss": 0.1347,
      "step": 1520
    },
    {
      "epoch": 11.33457249070632,
      "grad_norm": 0.4015512466430664,
      "learning_rate": 1.7734814814814816e-05,
      "loss": 0.1423,
      "step": 1530
    },
    {
      "epoch": 11.408921933085502,
      "grad_norm": 0.5738130211830139,
      "learning_rate": 1.7720000000000003e-05,
      "loss": 0.1615,
      "step": 1540
    },
    {
      "epoch": 11.483271375464684,
      "grad_norm": 0.48858901858329773,
      "learning_rate": 1.7705185185185187e-05,
      "loss": 0.1501,
      "step": 1550
    },
    {
      "epoch": 11.557620817843866,
      "grad_norm": 0.43685102462768555,
      "learning_rate": 1.769037037037037e-05,
      "loss": 0.1799,
      "step": 1560
    },
    {
      "epoch": 11.63197026022305,
      "grad_norm": 0.43284475803375244,
      "learning_rate": 1.767555555555556e-05,
      "loss": 0.1423,
      "step": 1570
    },
    {
      "epoch": 11.706319702602231,
      "grad_norm": 0.5905619859695435,
      "learning_rate": 1.7660740740740742e-05,
      "loss": 0.1864,
      "step": 1580
    },
    {
      "epoch": 11.780669144981413,
      "grad_norm": 0.6448813676834106,
      "learning_rate": 1.7645925925925926e-05,
      "loss": 0.1495,
      "step": 1590
    },
    {
      "epoch": 11.855018587360595,
      "grad_norm": 0.5516689419746399,
      "learning_rate": 1.7631111111111114e-05,
      "loss": 0.1494,
      "step": 1600
    },
    {
      "epoch": 11.929368029739777,
      "grad_norm": 0.3935609459877014,
      "learning_rate": 1.7616296296296298e-05,
      "loss": 0.1731,
      "step": 1610
    },
    {
      "epoch": 12.0,
      "grad_norm": 0.7351195216178894,
      "learning_rate": 1.7601481481481485e-05,
      "loss": 0.1601,
      "step": 1620
    },
    {
      "epoch": 12.074349442379182,
      "grad_norm": 0.5860719084739685,
      "learning_rate": 1.758666666666667e-05,
      "loss": 0.1613,
      "step": 1630
    },
    {
      "epoch": 12.148698884758364,
      "grad_norm": 0.389744371175766,
      "learning_rate": 1.7571851851851853e-05,
      "loss": 0.1602,
      "step": 1640
    },
    {
      "epoch": 12.223048327137546,
      "grad_norm": 0.43976500630378723,
      "learning_rate": 1.755703703703704e-05,
      "loss": 0.1585,
      "step": 1650
    },
    {
      "epoch": 12.297397769516728,
      "grad_norm": 0.5417287349700928,
      "learning_rate": 1.7542222222222224e-05,
      "loss": 0.1503,
      "step": 1660
    },
    {
      "epoch": 12.371747211895912,
      "grad_norm": 0.5591156482696533,
      "learning_rate": 1.7527407407407408e-05,
      "loss": 0.1465,
      "step": 1670
    },
    {
      "epoch": 12.446096654275093,
      "grad_norm": 0.4884878396987915,
      "learning_rate": 1.7512592592592595e-05,
      "loss": 0.1675,
      "step": 1680
    },
    {
      "epoch": 12.520446096654275,
      "grad_norm": 0.4638814628124237,
      "learning_rate": 1.749777777777778e-05,
      "loss": 0.146,
      "step": 1690
    },
    {
      "epoch": 12.594795539033457,
      "grad_norm": 0.3881007134914398,
      "learning_rate": 1.7482962962962963e-05,
      "loss": 0.1476,
      "step": 1700
    },
    {
      "epoch": 12.66914498141264,
      "grad_norm": 0.48412230610847473,
      "learning_rate": 1.746814814814815e-05,
      "loss": 0.1599,
      "step": 1710
    },
    {
      "epoch": 12.743494423791821,
      "grad_norm": 0.5140769481658936,
      "learning_rate": 1.7453333333333335e-05,
      "loss": 0.1498,
      "step": 1720
    },
    {
      "epoch": 12.817843866171003,
      "grad_norm": 0.5136166214942932,
      "learning_rate": 1.743851851851852e-05,
      "loss": 0.1691,
      "step": 1730
    },
    {
      "epoch": 12.892193308550185,
      "grad_norm": 0.546910285949707,
      "learning_rate": 1.7423703703703706e-05,
      "loss": 0.1619,
      "step": 1740
    },
    {
      "epoch": 12.966542750929367,
      "grad_norm": 0.4457130432128906,
      "learning_rate": 1.740888888888889e-05,
      "loss": 0.1359,
      "step": 1750
    },
    {
      "epoch": 13.037174721189592,
      "grad_norm": 0.519187867641449,
      "learning_rate": 1.7394074074074077e-05,
      "loss": 0.1711,
      "step": 1760
    },
    {
      "epoch": 13.111524163568774,
      "grad_norm": 0.47224465012550354,
      "learning_rate": 1.737925925925926e-05,
      "loss": 0.1402,
      "step": 1770
    },
    {
      "epoch": 13.185873605947956,
      "grad_norm": 0.6364449858665466,
      "learning_rate": 1.7364444444444445e-05,
      "loss": 0.1532,
      "step": 1780
    },
    {
      "epoch": 13.260223048327138,
      "grad_norm": 0.5840186476707458,
      "learning_rate": 1.7349629629629632e-05,
      "loss": 0.1666,
      "step": 1790
    },
    {
      "epoch": 13.33457249070632,
      "grad_norm": 0.5354214310646057,
      "learning_rate": 1.7334814814814816e-05,
      "loss": 0.1466,
      "step": 1800
    },
    {
      "epoch": 13.408921933085502,
      "grad_norm": 0.5404545068740845,
      "learning_rate": 1.732e-05,
      "loss": 0.1477,
      "step": 1810
    },
    {
      "epoch": 13.483271375464684,
      "grad_norm": 0.7329846620559692,
      "learning_rate": 1.7305185185185188e-05,
      "loss": 0.1525,
      "step": 1820
    },
    {
      "epoch": 13.557620817843866,
      "grad_norm": 0.4445570111274719,
      "learning_rate": 1.729037037037037e-05,
      "loss": 0.1635,
      "step": 1830
    },
    {
      "epoch": 13.63197026022305,
      "grad_norm": 0.4468563199043274,
      "learning_rate": 1.7275555555555555e-05,
      "loss": 0.168,
      "step": 1840
    },
    {
      "epoch": 13.706319702602231,
      "grad_norm": 0.5867989659309387,
      "learning_rate": 1.7260740740740743e-05,
      "loss": 0.1434,
      "step": 1850
    },
    {
      "epoch": 13.780669144981413,
      "grad_norm": 0.6033596992492676,
      "learning_rate": 1.7245925925925927e-05,
      "loss": 0.1539,
      "step": 1860
    },
    {
      "epoch": 13.855018587360595,
      "grad_norm": 0.6009159684181213,
      "learning_rate": 1.723111111111111e-05,
      "loss": 0.1518,
      "step": 1870
    },
    {
      "epoch": 13.929368029739777,
      "grad_norm": 0.594029426574707,
      "learning_rate": 1.7216296296296298e-05,
      "loss": 0.1529,
      "step": 1880
    },
    {
      "epoch": 14.0,
      "grad_norm": 0.8096229434013367,
      "learning_rate": 1.7201481481481482e-05,
      "loss": 0.1528,
      "step": 1890
    },
    {
      "epoch": 14.074349442379182,
      "grad_norm": 0.5705353021621704,
      "learning_rate": 1.7186666666666666e-05,
      "loss": 0.1641,
      "step": 1900
    },
    {
      "epoch": 14.148698884758364,
      "grad_norm": 0.478465735912323,
      "learning_rate": 1.7171851851851853e-05,
      "loss": 0.1481,
      "step": 1910
    },
    {
      "epoch": 14.223048327137546,
      "grad_norm": 0.5125715136528015,
      "learning_rate": 1.7157037037037037e-05,
      "loss": 0.1574,
      "step": 1920
    },
    {
      "epoch": 14.297397769516728,
      "grad_norm": 0.45154237747192383,
      "learning_rate": 1.7142222222222224e-05,
      "loss": 0.1567,
      "step": 1930
    },
    {
      "epoch": 14.371747211895912,
      "grad_norm": 0.5256346464157104,
      "learning_rate": 1.712740740740741e-05,
      "loss": 0.152,
      "step": 1940
    },
    {
      "epoch": 14.446096654275093,
      "grad_norm": 0.5393983125686646,
      "learning_rate": 1.7112592592592592e-05,
      "loss": 0.147,
      "step": 1950
    },
    {
      "epoch": 14.520446096654275,
      "grad_norm": 0.5661730170249939,
      "learning_rate": 1.709777777777778e-05,
      "loss": 0.1628,
      "step": 1960
    },
    {
      "epoch": 14.594795539033457,
      "grad_norm": 0.5143943428993225,
      "learning_rate": 1.7082962962962964e-05,
      "loss": 0.1482,
      "step": 1970
    },
    {
      "epoch": 14.66914498141264,
      "grad_norm": 0.4655561149120331,
      "learning_rate": 1.7068148148148148e-05,
      "loss": 0.1396,
      "step": 1980
    },
    {
      "epoch": 14.743494423791821,
      "grad_norm": 0.545764684677124,
      "learning_rate": 1.7053333333333335e-05,
      "loss": 0.1443,
      "step": 1990
    },
    {
      "epoch": 14.817843866171003,
      "grad_norm": 0.5506648421287537,
      "learning_rate": 1.7038518518518522e-05,
      "loss": 0.1653,
      "step": 2000
    },
    {
      "epoch": 14.892193308550185,
      "grad_norm": 0.6997569799423218,
      "learning_rate": 1.7023703703703703e-05,
      "loss": 0.1512,
      "step": 2010
    },
    {
      "epoch": 14.966542750929367,
      "grad_norm": 0.6482635140419006,
      "learning_rate": 1.700888888888889e-05,
      "loss": 0.1612,
      "step": 2020
    },
    {
      "epoch": 15.037174721189592,
      "grad_norm": 0.5198907256126404,
      "learning_rate": 1.6994074074074078e-05,
      "loss": 0.16,
      "step": 2030
    },
    {
      "epoch": 15.111524163568774,
      "grad_norm": 0.598322331905365,
      "learning_rate": 1.697925925925926e-05,
      "loss": 0.1512,
      "step": 2040
    },
    {
      "epoch": 15.185873605947956,
      "grad_norm": 0.4553258419036865,
      "learning_rate": 1.6964444444444445e-05,
      "loss": 0.1569,
      "step": 2050
    },
    {
      "epoch": 15.260223048327138,
      "grad_norm": 0.8372469544410706,
      "learning_rate": 1.6949629629629633e-05,
      "loss": 0.1496,
      "step": 2060
    },
    {
      "epoch": 15.33457249070632,
      "grad_norm": 0.4425181746482849,
      "learning_rate": 1.6934814814814817e-05,
      "loss": 0.1475,
      "step": 2070
    },
    {
      "epoch": 15.408921933085502,
      "grad_norm": 0.456220418214798,
      "learning_rate": 1.692e-05,
      "loss": 0.1602,
      "step": 2080
    },
    {
      "epoch": 15.483271375464684,
      "grad_norm": 0.6873511672019958,
      "learning_rate": 1.6905185185185188e-05,
      "loss": 0.1466,
      "step": 2090
    },
    {
      "epoch": 15.557620817843866,
      "grad_norm": 0.5732847452163696,
      "learning_rate": 1.6890370370370372e-05,
      "loss": 0.1459,
      "step": 2100
    },
    {
      "epoch": 15.63197026022305,
      "grad_norm": 0.6063703894615173,
      "learning_rate": 1.6875555555555556e-05,
      "loss": 0.1469,
      "step": 2110
    },
    {
      "epoch": 15.706319702602231,
      "grad_norm": 0.5810242295265198,
      "learning_rate": 1.6860740740740743e-05,
      "loss": 0.1563,
      "step": 2120
    },
    {
      "epoch": 15.780669144981413,
      "grad_norm": 0.5387036204338074,
      "learning_rate": 1.6845925925925927e-05,
      "loss": 0.1555,
      "step": 2130
    },
    {
      "epoch": 15.855018587360595,
      "grad_norm": 0.575778067111969,
      "learning_rate": 1.683111111111111e-05,
      "loss": 0.1544,
      "step": 2140
    },
    {
      "epoch": 15.929368029739777,
      "grad_norm": 0.5349811315536499,
      "learning_rate": 1.68162962962963e-05,
      "loss": 0.1564,
      "step": 2150
    },
    {
      "epoch": 16.0,
      "grad_norm": 0.6230655312538147,
      "learning_rate": 1.6801481481481482e-05,
      "loss": 0.1561,
      "step": 2160
    },
    {
      "epoch": 16.074349442379184,
      "grad_norm": 0.5881768465042114,
      "learning_rate": 1.678666666666667e-05,
      "loss": 0.1603,
      "step": 2170
    },
    {
      "epoch": 16.148698884758364,
      "grad_norm": 0.474129319190979,
      "learning_rate": 1.6771851851851854e-05,
      "loss": 0.155,
      "step": 2180
    },
    {
      "epoch": 16.223048327137548,
      "grad_norm": 0.5481453537940979,
      "learning_rate": 1.6757037037037038e-05,
      "loss": 0.1588,
      "step": 2190
    },
    {
      "epoch": 16.297397769516728,
      "grad_norm": 0.7675701975822449,
      "learning_rate": 1.6742222222222225e-05,
      "loss": 0.1502,
      "step": 2200
    },
    {
      "epoch": 16.37174721189591,
      "grad_norm": 0.5562478303909302,
      "learning_rate": 1.672740740740741e-05,
      "loss": 0.1483,
      "step": 2210
    },
    {
      "epoch": 16.44609665427509,
      "grad_norm": 0.5353226065635681,
      "learning_rate": 1.6712592592592593e-05,
      "loss": 0.1537,
      "step": 2220
    },
    {
      "epoch": 16.520446096654275,
      "grad_norm": 0.5002792477607727,
      "learning_rate": 1.669777777777778e-05,
      "loss": 0.1634,
      "step": 2230
    },
    {
      "epoch": 16.594795539033456,
      "grad_norm": 0.600141704082489,
      "learning_rate": 1.6682962962962964e-05,
      "loss": 0.1468,
      "step": 2240
    },
    {
      "epoch": 16.66914498141264,
      "grad_norm": 0.5432508587837219,
      "learning_rate": 1.6668148148148148e-05,
      "loss": 0.1588,
      "step": 2250
    },
    {
      "epoch": 16.743494423791823,
      "grad_norm": 0.4443097710609436,
      "learning_rate": 1.6653333333333335e-05,
      "loss": 0.1333,
      "step": 2260
    },
    {
      "epoch": 16.817843866171003,
      "grad_norm": 0.5197753310203552,
      "learning_rate": 1.663851851851852e-05,
      "loss": 0.1483,
      "step": 2270
    },
    {
      "epoch": 16.892193308550187,
      "grad_norm": 0.5831020474433899,
      "learning_rate": 1.6623703703703703e-05,
      "loss": 0.1398,
      "step": 2280
    },
    {
      "epoch": 16.966542750929367,
      "grad_norm": 0.430117666721344,
      "learning_rate": 1.660888888888889e-05,
      "loss": 0.1716,
      "step": 2290
    },
    {
      "epoch": 17.03717472118959,
      "grad_norm": 0.6407066583633423,
      "learning_rate": 1.6594074074074075e-05,
      "loss": 0.1492,
      "step": 2300
    },
    {
      "epoch": 17.111524163568774,
      "grad_norm": 0.5410178899765015,
      "learning_rate": 1.6579259259259262e-05,
      "loss": 0.1239,
      "step": 2310
    },
    {
      "epoch": 17.185873605947954,
      "grad_norm": 0.7589020133018494,
      "learning_rate": 1.6564444444444446e-05,
      "loss": 0.1493,
      "step": 2320
    },
    {
      "epoch": 17.260223048327138,
      "grad_norm": 0.5675458908081055,
      "learning_rate": 1.654962962962963e-05,
      "loss": 0.1523,
      "step": 2330
    },
    {
      "epoch": 17.33457249070632,
      "grad_norm": 0.47158217430114746,
      "learning_rate": 1.6534814814814817e-05,
      "loss": 0.1461,
      "step": 2340
    },
    {
      "epoch": 17.4089219330855,
      "grad_norm": 0.5413014888763428,
      "learning_rate": 1.652e-05,
      "loss": 0.1561,
      "step": 2350
    },
    {
      "epoch": 17.483271375464685,
      "grad_norm": 0.5239994525909424,
      "learning_rate": 1.6505185185185185e-05,
      "loss": 0.1551,
      "step": 2360
    },
    {
      "epoch": 17.557620817843866,
      "grad_norm": 0.7240706086158752,
      "learning_rate": 1.6490370370370372e-05,
      "loss": 0.1704,
      "step": 2370
    },
    {
      "epoch": 17.63197026022305,
      "grad_norm": 0.5819339156150818,
      "learning_rate": 1.6475555555555556e-05,
      "loss": 0.1541,
      "step": 2380
    },
    {
      "epoch": 17.70631970260223,
      "grad_norm": 0.4900754690170288,
      "learning_rate": 1.646074074074074e-05,
      "loss": 0.1631,
      "step": 2390
    },
    {
      "epoch": 17.780669144981413,
      "grad_norm": 0.717401385307312,
      "learning_rate": 1.6445925925925928e-05,
      "loss": 0.1714,
      "step": 2400
    },
    {
      "epoch": 17.855018587360593,
      "grad_norm": 0.5610508322715759,
      "learning_rate": 1.6431111111111115e-05,
      "loss": 0.1401,
      "step": 2410
    },
    {
      "epoch": 17.929368029739777,
      "grad_norm": 0.6234515309333801,
      "learning_rate": 1.6416296296296295e-05,
      "loss": 0.1454,
      "step": 2420
    },
    {
      "epoch": 18.0,
      "grad_norm": 0.6428711414337158,
      "learning_rate": 1.6401481481481483e-05,
      "loss": 0.1326,
      "step": 2430
    },
    {
      "epoch": 18.074349442379184,
      "grad_norm": 0.487802654504776,
      "learning_rate": 1.638666666666667e-05,
      "loss": 0.1484,
      "step": 2440
    },
    {
      "epoch": 18.148698884758364,
      "grad_norm": 0.7361755967140198,
      "learning_rate": 1.637185185185185e-05,
      "loss": 0.1797,
      "step": 2450
    },
    {
      "epoch": 18.223048327137548,
      "grad_norm": 0.7832903861999512,
      "learning_rate": 1.6357037037037038e-05,
      "loss": 0.1567,
      "step": 2460
    },
    {
      "epoch": 18.297397769516728,
      "grad_norm": 0.6355164647102356,
      "learning_rate": 1.6342222222222225e-05,
      "loss": 0.1354,
      "step": 2470
    },
    {
      "epoch": 18.37174721189591,
      "grad_norm": 0.5511205196380615,
      "learning_rate": 1.632740740740741e-05,
      "loss": 0.1493,
      "step": 2480
    },
    {
      "epoch": 18.44609665427509,
      "grad_norm": 0.5949139595031738,
      "learning_rate": 1.6312592592592593e-05,
      "loss": 0.1376,
      "step": 2490
    },
    {
      "epoch": 18.520446096654275,
      "grad_norm": 0.6481142044067383,
      "learning_rate": 1.629777777777778e-05,
      "loss": 0.1334,
      "step": 2500
    },
    {
      "epoch": 18.594795539033456,
      "grad_norm": 0.6159104704856873,
      "learning_rate": 1.6282962962962965e-05,
      "loss": 0.1525,
      "step": 2510
    },
    {
      "epoch": 18.66914498141264,
      "grad_norm": 0.6971268057823181,
      "learning_rate": 1.626814814814815e-05,
      "loss": 0.1696,
      "step": 2520
    },
    {
      "epoch": 18.743494423791823,
      "grad_norm": 0.5682424902915955,
      "learning_rate": 1.6253333333333336e-05,
      "loss": 0.1448,
      "step": 2530
    },
    {
      "epoch": 18.817843866171003,
      "grad_norm": 0.8050957322120667,
      "learning_rate": 1.623851851851852e-05,
      "loss": 0.1461,
      "step": 2540
    },
    {
      "epoch": 18.892193308550187,
      "grad_norm": 0.5419776439666748,
      "learning_rate": 1.6223703703703707e-05,
      "loss": 0.1388,
      "step": 2550
    },
    {
      "epoch": 18.966542750929367,
      "grad_norm": 0.544243574142456,
      "learning_rate": 1.620888888888889e-05,
      "loss": 0.1491,
      "step": 2560
    },
    {
      "epoch": 19.03717472118959,
      "grad_norm": 0.5350467562675476,
      "learning_rate": 1.6194074074074075e-05,
      "loss": 0.1757,
      "step": 2570
    },
    {
      "epoch": 19.111524163568774,
      "grad_norm": 0.7144536972045898,
      "learning_rate": 1.6179259259259262e-05,
      "loss": 0.1379,
      "step": 2580
    },
    {
      "epoch": 19.185873605947954,
      "grad_norm": 0.669241726398468,
      "learning_rate": 1.6164444444444446e-05,
      "loss": 0.1688,
      "step": 2590
    },
    {
      "epoch": 19.260223048327138,
      "grad_norm": 0.6698132753372192,
      "learning_rate": 1.614962962962963e-05,
      "loss": 0.1667,
      "step": 2600
    },
    {
      "epoch": 19.33457249070632,
      "grad_norm": 0.42616721987724304,
      "learning_rate": 1.6134814814814818e-05,
      "loss": 0.1399,
      "step": 2610
    },
    {
      "epoch": 19.4089219330855,
      "grad_norm": 0.47025972604751587,
      "learning_rate": 1.612e-05,
      "loss": 0.1601,
      "step": 2620
    },
    {
      "epoch": 19.483271375464685,
      "grad_norm": 0.5890398621559143,
      "learning_rate": 1.6105185185185185e-05,
      "loss": 0.1458,
      "step": 2630
    },
    {
      "epoch": 19.557620817843866,
      "grad_norm": 0.5913223028182983,
      "learning_rate": 1.6090370370370373e-05,
      "loss": 0.1452,
      "step": 2640
    },
    {
      "epoch": 19.63197026022305,
      "grad_norm": 0.6869933009147644,
      "learning_rate": 1.6075555555555557e-05,
      "loss": 0.146,
      "step": 2650
    },
    {
      "epoch": 19.70631970260223,
      "grad_norm": 0.6625498533248901,
      "learning_rate": 1.606074074074074e-05,
      "loss": 0.1352,
      "step": 2660
    },
    {
      "epoch": 19.780669144981413,
      "grad_norm": 0.6188751459121704,
      "learning_rate": 1.6045925925925928e-05,
      "loss": 0.1466,
      "step": 2670
    },
    {
      "epoch": 19.855018587360593,
      "grad_norm": 0.7217994332313538,
      "learning_rate": 1.6031111111111112e-05,
      "loss": 0.1497,
      "step": 2680
    },
    {
      "epoch": 19.929368029739777,
      "grad_norm": 0.9314121603965759,
      "learning_rate": 1.60162962962963e-05,
      "loss": 0.1502,
      "step": 2690
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.8075697422027588,
      "learning_rate": 1.6001481481481483e-05,
      "loss": 0.1614,
      "step": 2700
    },
    {
      "epoch": 20.074349442379184,
      "grad_norm": 0.6120270490646362,
      "learning_rate": 1.5986666666666667e-05,
      "loss": 0.1524,
      "step": 2710
    },
    {
      "epoch": 20.148698884758364,
      "grad_norm": 0.7223735451698303,
      "learning_rate": 1.5971851851851855e-05,
      "loss": 0.1633,
      "step": 2720
    },
    {
      "epoch": 20.223048327137548,
      "grad_norm": 0.493823766708374,
      "learning_rate": 1.595703703703704e-05,
      "loss": 0.1372,
      "step": 2730
    },
    {
      "epoch": 20.297397769516728,
      "grad_norm": 0.4499818682670593,
      "learning_rate": 1.5942222222222222e-05,
      "loss": 0.1423,
      "step": 2740
    },
    {
      "epoch": 20.37174721189591,
      "grad_norm": 0.6380050182342529,
      "learning_rate": 1.592740740740741e-05,
      "loss": 0.1452,
      "step": 2750
    },
    {
      "epoch": 20.44609665427509,
      "grad_norm": 0.6036418080329895,
      "learning_rate": 1.5912592592592594e-05,
      "loss": 0.1383,
      "step": 2760
    },
    {
      "epoch": 20.520446096654275,
      "grad_norm": 0.6825639605522156,
      "learning_rate": 1.5897777777777778e-05,
      "loss": 0.1474,
      "step": 2770
    },
    {
      "epoch": 20.594795539033456,
      "grad_norm": 0.6788952350616455,
      "learning_rate": 1.5882962962962965e-05,
      "loss": 0.1541,
      "step": 2780
    },
    {
      "epoch": 20.66914498141264,
      "grad_norm": 0.47429564595222473,
      "learning_rate": 1.586814814814815e-05,
      "loss": 0.151,
      "step": 2790
    },
    {
      "epoch": 20.743494423791823,
      "grad_norm": 0.6418371796607971,
      "learning_rate": 1.5853333333333333e-05,
      "loss": 0.1591,
      "step": 2800
    },
    {
      "epoch": 20.817843866171003,
      "grad_norm": 0.4587412476539612,
      "learning_rate": 1.583851851851852e-05,
      "loss": 0.1407,
      "step": 2810
    },
    {
      "epoch": 20.892193308550187,
      "grad_norm": 0.5573743581771851,
      "learning_rate": 1.5823703703703704e-05,
      "loss": 0.1433,
      "step": 2820
    },
    {
      "epoch": 20.966542750929367,
      "grad_norm": 0.6758216619491577,
      "learning_rate": 1.5808888888888888e-05,
      "loss": 0.1639,
      "step": 2830
    },
    {
      "epoch": 21.03717472118959,
      "grad_norm": 0.5522332787513733,
      "learning_rate": 1.5794074074074075e-05,
      "loss": 0.1517,
      "step": 2840
    },
    {
      "epoch": 21.111524163568774,
      "grad_norm": 0.6418924927711487,
      "learning_rate": 1.577925925925926e-05,
      "loss": 0.1472,
      "step": 2850
    },
    {
      "epoch": 21.185873605947954,
      "grad_norm": 0.6540785431861877,
      "learning_rate": 1.5764444444444447e-05,
      "loss": 0.1493,
      "step": 2860
    },
    {
      "epoch": 21.260223048327138,
      "grad_norm": 0.5801339745521545,
      "learning_rate": 1.574962962962963e-05,
      "loss": 0.1394,
      "step": 2870
    },
    {
      "epoch": 21.33457249070632,
      "grad_norm": 0.5614282488822937,
      "learning_rate": 1.5734814814814815e-05,
      "loss": 0.1632,
      "step": 2880
    },
    {
      "epoch": 21.4089219330855,
      "grad_norm": 0.7303045392036438,
      "learning_rate": 1.5720000000000002e-05,
      "loss": 0.1494,
      "step": 2890
    },
    {
      "epoch": 21.483271375464685,
      "grad_norm": 0.7544832825660706,
      "learning_rate": 1.5705185185185186e-05,
      "loss": 0.1444,
      "step": 2900
    },
    {
      "epoch": 21.557620817843866,
      "grad_norm": 0.5626559257507324,
      "learning_rate": 1.569037037037037e-05,
      "loss": 0.1358,
      "step": 2910
    },
    {
      "epoch": 21.63197026022305,
      "grad_norm": 0.5904011130332947,
      "learning_rate": 1.5675555555555557e-05,
      "loss": 0.1512,
      "step": 2920
    },
    {
      "epoch": 21.70631970260223,
      "grad_norm": 0.48527368903160095,
      "learning_rate": 1.5660740740740745e-05,
      "loss": 0.1708,
      "step": 2930
    },
    {
      "epoch": 21.780669144981413,
      "grad_norm": 0.6494051814079285,
      "learning_rate": 1.5645925925925925e-05,
      "loss": 0.1453,
      "step": 2940
    },
    {
      "epoch": 21.855018587360593,
      "grad_norm": 0.7825102806091309,
      "learning_rate": 1.5631111111111112e-05,
      "loss": 0.134,
      "step": 2950
    },
    {
      "epoch": 21.929368029739777,
      "grad_norm": 0.6132560968399048,
      "learning_rate": 1.56162962962963e-05,
      "loss": 0.1729,
      "step": 2960
    },
    {
      "epoch": 22.0,
      "grad_norm": 1.0009925365447998,
      "learning_rate": 1.560148148148148e-05,
      "loss": 0.137,
      "step": 2970
    },
    {
      "epoch": 22.074349442379184,
      "grad_norm": 0.741037130355835,
      "learning_rate": 1.5586666666666668e-05,
      "loss": 0.1334,
      "step": 2980
    },
    {
      "epoch": 22.148698884758364,
      "grad_norm": 0.6136552691459656,
      "learning_rate": 1.5571851851851855e-05,
      "loss": 0.161,
      "step": 2990
    },
    {
      "epoch": 22.223048327137548,
      "grad_norm": 0.5877834558486938,
      "learning_rate": 1.555703703703704e-05,
      "loss": 0.1499,
      "step": 3000
    },
    {
      "epoch": 22.297397769516728,
      "grad_norm": 0.561063826084137,
      "learning_rate": 1.5542222222222223e-05,
      "loss": 0.154,
      "step": 3010
    },
    {
      "epoch": 22.37174721189591,
      "grad_norm": 0.662532389163971,
      "learning_rate": 1.552740740740741e-05,
      "loss": 0.1502,
      "step": 3020
    },
    {
      "epoch": 22.44609665427509,
      "grad_norm": 0.6613772511482239,
      "learning_rate": 1.5512592592592594e-05,
      "loss": 0.1541,
      "step": 3030
    },
    {
      "epoch": 22.520446096654275,
      "grad_norm": 0.5631482601165771,
      "learning_rate": 1.5497777777777778e-05,
      "loss": 0.1715,
      "step": 3040
    },
    {
      "epoch": 22.594795539033456,
      "grad_norm": 0.738094687461853,
      "learning_rate": 1.5482962962962965e-05,
      "loss": 0.1378,
      "step": 3050
    },
    {
      "epoch": 22.66914498141264,
      "grad_norm": 0.7748939394950867,
      "learning_rate": 1.546814814814815e-05,
      "loss": 0.1402,
      "step": 3060
    },
    {
      "epoch": 22.743494423791823,
      "grad_norm": 0.5434868335723877,
      "learning_rate": 1.5453333333333333e-05,
      "loss": 0.1567,
      "step": 3070
    },
    {
      "epoch": 22.817843866171003,
      "grad_norm": 0.6210565567016602,
      "learning_rate": 1.543851851851852e-05,
      "loss": 0.1329,
      "step": 3080
    },
    {
      "epoch": 22.892193308550187,
      "grad_norm": 0.5351431369781494,
      "learning_rate": 1.5423703703703705e-05,
      "loss": 0.1484,
      "step": 3090
    },
    {
      "epoch": 22.966542750929367,
      "grad_norm": 0.7250980138778687,
      "learning_rate": 1.5408888888888892e-05,
      "loss": 0.156,
      "step": 3100
    },
    {
      "epoch": 23.03717472118959,
      "grad_norm": 0.5898270606994629,
      "learning_rate": 1.5394074074074076e-05,
      "loss": 0.1154,
      "step": 3110
    },
    {
      "epoch": 23.111524163568774,
      "grad_norm": 0.5485193729400635,
      "learning_rate": 1.537925925925926e-05,
      "loss": 0.1691,
      "step": 3120
    },
    {
      "epoch": 23.185873605947954,
      "grad_norm": 0.6472620964050293,
      "learning_rate": 1.5364444444444447e-05,
      "loss": 0.1437,
      "step": 3130
    },
    {
      "epoch": 23.260223048327138,
      "grad_norm": 0.6246379613876343,
      "learning_rate": 1.534962962962963e-05,
      "loss": 0.1454,
      "step": 3140
    },
    {
      "epoch": 23.33457249070632,
      "grad_norm": 0.5638519525527954,
      "learning_rate": 1.5334814814814815e-05,
      "loss": 0.1624,
      "step": 3150
    },
    {
      "epoch": 23.4089219330855,
      "grad_norm": 0.5323517918586731,
      "learning_rate": 1.5320000000000002e-05,
      "loss": 0.1425,
      "step": 3160
    },
    {
      "epoch": 23.483271375464685,
      "grad_norm": 0.5946701169013977,
      "learning_rate": 1.5305185185185186e-05,
      "loss": 0.1372,
      "step": 3170
    },
    {
      "epoch": 23.557620817843866,
      "grad_norm": 0.6407949924468994,
      "learning_rate": 1.529037037037037e-05,
      "loss": 0.1569,
      "step": 3180
    },
    {
      "epoch": 23.63197026022305,
      "grad_norm": 0.5856043100357056,
      "learning_rate": 1.5275555555555558e-05,
      "loss": 0.1371,
      "step": 3190
    },
    {
      "epoch": 23.70631970260223,
      "grad_norm": 0.879518985748291,
      "learning_rate": 1.526074074074074e-05,
      "loss": 0.1476,
      "step": 3200
    },
    {
      "epoch": 23.780669144981413,
      "grad_norm": 0.6875478029251099,
      "learning_rate": 1.5245925925925926e-05,
      "loss": 0.1474,
      "step": 3210
    },
    {
      "epoch": 23.855018587360593,
      "grad_norm": 0.542873740196228,
      "learning_rate": 1.5231111111111113e-05,
      "loss": 0.1388,
      "step": 3220
    },
    {
      "epoch": 23.929368029739777,
      "grad_norm": 0.6032382249832153,
      "learning_rate": 1.5216296296296298e-05,
      "loss": 0.1546,
      "step": 3230
    },
    {
      "epoch": 24.0,
      "grad_norm": 0.8613442182540894,
      "learning_rate": 1.5201481481481484e-05,
      "loss": 0.1525,
      "step": 3240
    },
    {
      "epoch": 24.074349442379184,
      "grad_norm": 0.5277543663978577,
      "learning_rate": 1.5186666666666668e-05,
      "loss": 0.1477,
      "step": 3250
    },
    {
      "epoch": 24.148698884758364,
      "grad_norm": 0.7605741620063782,
      "learning_rate": 1.5171851851851854e-05,
      "loss": 0.1476,
      "step": 3260
    },
    {
      "epoch": 24.223048327137548,
      "grad_norm": 0.5436217784881592,
      "learning_rate": 1.515703703703704e-05,
      "loss": 0.1579,
      "step": 3270
    },
    {
      "epoch": 24.297397769516728,
      "grad_norm": 0.8243942856788635,
      "learning_rate": 1.5142222222222223e-05,
      "loss": 0.1483,
      "step": 3280
    },
    {
      "epoch": 24.37174721189591,
      "grad_norm": 0.7334823608398438,
      "learning_rate": 1.5127407407407409e-05,
      "loss": 0.1453,
      "step": 3290
    },
    {
      "epoch": 24.44609665427509,
      "grad_norm": 0.7276442050933838,
      "learning_rate": 1.5112592592592595e-05,
      "loss": 0.1458,
      "step": 3300
    },
    {
      "epoch": 24.520446096654275,
      "grad_norm": 0.6423379182815552,
      "learning_rate": 1.509777777777778e-05,
      "loss": 0.1456,
      "step": 3310
    },
    {
      "epoch": 24.594795539033456,
      "grad_norm": 0.6577500700950623,
      "learning_rate": 1.5082962962962964e-05,
      "loss": 0.1679,
      "step": 3320
    },
    {
      "epoch": 24.66914498141264,
      "grad_norm": 0.7867868542671204,
      "learning_rate": 1.506814814814815e-05,
      "loss": 0.1313,
      "step": 3330
    },
    {
      "epoch": 24.743494423791823,
      "grad_norm": 0.748625636100769,
      "learning_rate": 1.5053333333333335e-05,
      "loss": 0.1614,
      "step": 3340
    },
    {
      "epoch": 24.817843866171003,
      "grad_norm": 0.6888548731803894,
      "learning_rate": 1.503851851851852e-05,
      "loss": 0.1456,
      "step": 3350
    },
    {
      "epoch": 24.892193308550187,
      "grad_norm": 0.5017921328544617,
      "learning_rate": 1.5023703703703705e-05,
      "loss": 0.1363,
      "step": 3360
    },
    {
      "epoch": 24.966542750929367,
      "grad_norm": 0.5741376280784607,
      "learning_rate": 1.500888888888889e-05,
      "loss": 0.1355,
      "step": 3370
    },
    {
      "epoch": 25.03717472118959,
      "grad_norm": 0.548521876335144,
      "learning_rate": 1.4994074074074075e-05,
      "loss": 0.1357,
      "step": 3380
    },
    {
      "epoch": 25.111524163568774,
      "grad_norm": 0.5954136848449707,
      "learning_rate": 1.497925925925926e-05,
      "loss": 0.1424,
      "step": 3390
    },
    {
      "epoch": 25.185873605947954,
      "grad_norm": 0.5778107047080994,
      "learning_rate": 1.4964444444444446e-05,
      "loss": 0.1342,
      "step": 3400
    },
    {
      "epoch": 25.260223048327138,
      "grad_norm": 0.6880170702934265,
      "learning_rate": 1.4949629629629632e-05,
      "loss": 0.1579,
      "step": 3410
    },
    {
      "epoch": 25.33457249070632,
      "grad_norm": 0.847070038318634,
      "learning_rate": 1.4934814814814816e-05,
      "loss": 0.1503,
      "step": 3420
    },
    {
      "epoch": 25.4089219330855,
      "grad_norm": 0.7623428702354431,
      "learning_rate": 1.4920000000000001e-05,
      "loss": 0.132,
      "step": 3430
    },
    {
      "epoch": 25.483271375464685,
      "grad_norm": 0.6535709500312805,
      "learning_rate": 1.4905185185185187e-05,
      "loss": 0.1595,
      "step": 3440
    },
    {
      "epoch": 25.557620817843866,
      "grad_norm": 0.58426833152771,
      "learning_rate": 1.489037037037037e-05,
      "loss": 0.1408,
      "step": 3450
    },
    {
      "epoch": 25.63197026022305,
      "grad_norm": 0.7421553730964661,
      "learning_rate": 1.4875555555555556e-05,
      "loss": 0.1573,
      "step": 3460
    },
    {
      "epoch": 25.70631970260223,
      "grad_norm": 0.7809867858886719,
      "learning_rate": 1.4860740740740742e-05,
      "loss": 0.1604,
      "step": 3470
    },
    {
      "epoch": 25.780669144981413,
      "grad_norm": 0.6506426930427551,
      "learning_rate": 1.4845925925925928e-05,
      "loss": 0.1238,
      "step": 3480
    },
    {
      "epoch": 25.855018587360593,
      "grad_norm": 0.7372991442680359,
      "learning_rate": 1.4831111111111112e-05,
      "loss": 0.1365,
      "step": 3490
    },
    {
      "epoch": 25.929368029739777,
      "grad_norm": 0.6616683006286621,
      "learning_rate": 1.4816296296296297e-05,
      "loss": 0.1631,
      "step": 3500
    },
    {
      "epoch": 26.0,
      "grad_norm": 1.0225354433059692,
      "learning_rate": 1.4801481481481483e-05,
      "loss": 0.1618,
      "step": 3510
    },
    {
      "epoch": 26.074349442379184,
      "grad_norm": 0.8698092103004456,
      "learning_rate": 1.4786666666666667e-05,
      "loss": 0.1436,
      "step": 3520
    },
    {
      "epoch": 26.148698884758364,
      "grad_norm": 0.8041232228279114,
      "learning_rate": 1.4771851851851852e-05,
      "loss": 0.1312,
      "step": 3530
    },
    {
      "epoch": 26.223048327137548,
      "grad_norm": 0.6114137768745422,
      "learning_rate": 1.4757037037037038e-05,
      "loss": 0.1357,
      "step": 3540
    },
    {
      "epoch": 26.297397769516728,
      "grad_norm": 0.6014777421951294,
      "learning_rate": 1.4742222222222224e-05,
      "loss": 0.1522,
      "step": 3550
    },
    {
      "epoch": 26.37174721189591,
      "grad_norm": 0.641634464263916,
      "learning_rate": 1.4727407407407408e-05,
      "loss": 0.1573,
      "step": 3560
    },
    {
      "epoch": 26.44609665427509,
      "grad_norm": 0.5663663148880005,
      "learning_rate": 1.4712592592592593e-05,
      "loss": 0.1379,
      "step": 3570
    },
    {
      "epoch": 26.520446096654275,
      "grad_norm": 0.7606702446937561,
      "learning_rate": 1.4697777777777779e-05,
      "loss": 0.1346,
      "step": 3580
    },
    {
      "epoch": 26.594795539033456,
      "grad_norm": 0.6960086226463318,
      "learning_rate": 1.4682962962962963e-05,
      "loss": 0.1503,
      "step": 3590
    },
    {
      "epoch": 26.66914498141264,
      "grad_norm": 0.6922869682312012,
      "learning_rate": 1.4668148148148149e-05,
      "loss": 0.1303,
      "step": 3600
    },
    {
      "epoch": 26.743494423791823,
      "grad_norm": 0.6689713001251221,
      "learning_rate": 1.4653333333333334e-05,
      "loss": 0.1456,
      "step": 3610
    },
    {
      "epoch": 26.817843866171003,
      "grad_norm": 0.6163557767868042,
      "learning_rate": 1.4638518518518522e-05,
      "loss": 0.1485,
      "step": 3620
    },
    {
      "epoch": 26.892193308550187,
      "grad_norm": 0.7087953686714172,
      "learning_rate": 1.4623703703703704e-05,
      "loss": 0.1633,
      "step": 3630
    },
    {
      "epoch": 26.966542750929367,
      "grad_norm": 0.5353021621704102,
      "learning_rate": 1.460888888888889e-05,
      "loss": 0.1616,
      "step": 3640
    },
    {
      "epoch": 27.03717472118959,
      "grad_norm": 0.7419509291648865,
      "learning_rate": 1.4594074074074077e-05,
      "loss": 0.1575,
      "step": 3650
    },
    {
      "epoch": 27.111524163568774,
      "grad_norm": 0.8330119848251343,
      "learning_rate": 1.4579259259259259e-05,
      "loss": 0.1549,
      "step": 3660
    },
    {
      "epoch": 27.185873605947954,
      "grad_norm": 0.590820848941803,
      "learning_rate": 1.4564444444444445e-05,
      "loss": 0.1214,
      "step": 3670
    },
    {
      "epoch": 27.260223048327138,
      "grad_norm": 0.7259414792060852,
      "learning_rate": 1.4549629629629632e-05,
      "loss": 0.1551,
      "step": 3680
    },
    {
      "epoch": 27.33457249070632,
      "grad_norm": 0.6640204191207886,
      "learning_rate": 1.4534814814814814e-05,
      "loss": 0.1396,
      "step": 3690
    },
    {
      "epoch": 27.4089219330855,
      "grad_norm": 0.7328646779060364,
      "learning_rate": 1.4520000000000002e-05,
      "loss": 0.1422,
      "step": 3700
    },
    {
      "epoch": 27.483271375464685,
      "grad_norm": 0.7890786528587341,
      "learning_rate": 1.4505185185185187e-05,
      "loss": 0.1451,
      "step": 3710
    },
    {
      "epoch": 27.557620817843866,
      "grad_norm": 0.763083279132843,
      "learning_rate": 1.4490370370370373e-05,
      "loss": 0.1354,
      "step": 3720
    },
    {
      "epoch": 27.63197026022305,
      "grad_norm": 0.8619109988212585,
      "learning_rate": 1.4475555555555557e-05,
      "loss": 0.1445,
      "step": 3730
    },
    {
      "epoch": 27.70631970260223,
      "grad_norm": 0.7185838222503662,
      "learning_rate": 1.4460740740740742e-05,
      "loss": 0.1607,
      "step": 3740
    },
    {
      "epoch": 27.780669144981413,
      "grad_norm": 0.7397500872612,
      "learning_rate": 1.4445925925925928e-05,
      "loss": 0.1523,
      "step": 3750
    },
    {
      "epoch": 27.855018587360593,
      "grad_norm": 0.4978778064250946,
      "learning_rate": 1.4431111111111112e-05,
      "loss": 0.1566,
      "step": 3760
    },
    {
      "epoch": 27.929368029739777,
      "grad_norm": 0.8012253046035767,
      "learning_rate": 1.4416296296296298e-05,
      "loss": 0.1355,
      "step": 3770
    },
    {
      "epoch": 28.0,
      "grad_norm": 1.0277400016784668,
      "learning_rate": 1.4401481481481483e-05,
      "loss": 0.1506,
      "step": 3780
    },
    {
      "epoch": 28.074349442379184,
      "grad_norm": 0.9150741696357727,
      "learning_rate": 1.4386666666666669e-05,
      "loss": 0.1612,
      "step": 3790
    },
    {
      "epoch": 28.148698884758364,
      "grad_norm": 0.864963173866272,
      "learning_rate": 1.4371851851851853e-05,
      "loss": 0.1438,
      "step": 3800
    },
    {
      "epoch": 28.223048327137548,
      "grad_norm": 0.6770164966583252,
      "learning_rate": 1.4357037037037039e-05,
      "loss": 0.1323,
      "step": 3810
    },
    {
      "epoch": 28.297397769516728,
      "grad_norm": 0.6868869662284851,
      "learning_rate": 1.4342222222222224e-05,
      "loss": 0.1481,
      "step": 3820
    },
    {
      "epoch": 28.37174721189591,
      "grad_norm": 0.632509171962738,
      "learning_rate": 1.4327407407407408e-05,
      "loss": 0.1405,
      "step": 3830
    },
    {
      "epoch": 28.44609665427509,
      "grad_norm": 0.7720786333084106,
      "learning_rate": 1.4312592592592594e-05,
      "loss": 0.1681,
      "step": 3840
    },
    {
      "epoch": 28.520446096654275,
      "grad_norm": 0.6160032153129578,
      "learning_rate": 1.429777777777778e-05,
      "loss": 0.1534,
      "step": 3850
    },
    {
      "epoch": 28.594795539033456,
      "grad_norm": 0.5882054567337036,
      "learning_rate": 1.4282962962962965e-05,
      "loss": 0.1377,
      "step": 3860
    },
    {
      "epoch": 28.66914498141264,
      "grad_norm": 0.6536287665367126,
      "learning_rate": 1.4268148148148149e-05,
      "loss": 0.135,
      "step": 3870
    },
    {
      "epoch": 28.743494423791823,
      "grad_norm": 0.6234597563743591,
      "learning_rate": 1.4253333333333335e-05,
      "loss": 0.1383,
      "step": 3880
    },
    {
      "epoch": 28.817843866171003,
      "grad_norm": 0.6565032005310059,
      "learning_rate": 1.423851851851852e-05,
      "loss": 0.1394,
      "step": 3890
    },
    {
      "epoch": 28.892193308550187,
      "grad_norm": 0.7342352271080017,
      "learning_rate": 1.4223703703703704e-05,
      "loss": 0.157,
      "step": 3900
    },
    {
      "epoch": 28.966542750929367,
      "grad_norm": 0.7521103024482727,
      "learning_rate": 1.420888888888889e-05,
      "loss": 0.1429,
      "step": 3910
    },
    {
      "epoch": 29.03717472118959,
      "grad_norm": 0.6229523420333862,
      "learning_rate": 1.4194074074074076e-05,
      "loss": 0.1323,
      "step": 3920
    },
    {
      "epoch": 29.111524163568774,
      "grad_norm": 0.7629832625389099,
      "learning_rate": 1.4179259259259261e-05,
      "loss": 0.1315,
      "step": 3930
    },
    {
      "epoch": 29.185873605947954,
      "grad_norm": 0.8537513017654419,
      "learning_rate": 1.4164444444444445e-05,
      "loss": 0.1592,
      "step": 3940
    },
    {
      "epoch": 29.260223048327138,
      "grad_norm": 0.8366225957870483,
      "learning_rate": 1.414962962962963e-05,
      "loss": 0.1633,
      "step": 3950
    },
    {
      "epoch": 29.33457249070632,
      "grad_norm": 0.789057195186615,
      "learning_rate": 1.4134814814814816e-05,
      "loss": 0.1461,
      "step": 3960
    },
    {
      "epoch": 29.4089219330855,
      "grad_norm": 0.6626969575881958,
      "learning_rate": 1.412e-05,
      "loss": 0.1541,
      "step": 3970
    },
    {
      "epoch": 29.483271375464685,
      "grad_norm": 0.6936658024787903,
      "learning_rate": 1.4105185185185186e-05,
      "loss": 0.1362,
      "step": 3980
    },
    {
      "epoch": 29.557620817843866,
      "grad_norm": 0.6435779333114624,
      "learning_rate": 1.4090370370370372e-05,
      "loss": 0.1325,
      "step": 3990
    },
    {
      "epoch": 29.63197026022305,
      "grad_norm": 0.7088273167610168,
      "learning_rate": 1.4075555555555556e-05,
      "loss": 0.1395,
      "step": 4000
    },
    {
      "epoch": 29.70631970260223,
      "grad_norm": 0.7985978126525879,
      "learning_rate": 1.4060740740740741e-05,
      "loss": 0.1494,
      "step": 4010
    },
    {
      "epoch": 29.780669144981413,
      "grad_norm": 0.8180415034294128,
      "learning_rate": 1.4045925925925927e-05,
      "loss": 0.142,
      "step": 4020
    },
    {
      "epoch": 29.855018587360593,
      "grad_norm": 0.6279222965240479,
      "learning_rate": 1.4031111111111112e-05,
      "loss": 0.138,
      "step": 4030
    },
    {
      "epoch": 29.929368029739777,
      "grad_norm": 1.1823132038116455,
      "learning_rate": 1.4016296296296296e-05,
      "loss": 0.1456,
      "step": 4040
    },
    {
      "epoch": 30.0,
      "grad_norm": 0.9437673091888428,
      "learning_rate": 1.4001481481481482e-05,
      "loss": 0.151,
      "step": 4050
    },
    {
      "epoch": 30.074349442379184,
      "grad_norm": 0.5652852654457092,
      "learning_rate": 1.3986666666666668e-05,
      "loss": 0.1579,
      "step": 4060
    },
    {
      "epoch": 30.148698884758364,
      "grad_norm": 0.8058739900588989,
      "learning_rate": 1.3971851851851852e-05,
      "loss": 0.1447,
      "step": 4070
    },
    {
      "epoch": 30.223048327137548,
      "grad_norm": 0.5725260972976685,
      "learning_rate": 1.3957037037037037e-05,
      "loss": 0.1324,
      "step": 4080
    },
    {
      "epoch": 30.297397769516728,
      "grad_norm": 0.580084502696991,
      "learning_rate": 1.3942222222222223e-05,
      "loss": 0.1537,
      "step": 4090
    },
    {
      "epoch": 30.37174721189591,
      "grad_norm": 0.6443207859992981,
      "learning_rate": 1.392740740740741e-05,
      "loss": 0.1437,
      "step": 4100
    },
    {
      "epoch": 30.44609665427509,
      "grad_norm": 0.676125168800354,
      "learning_rate": 1.3912592592592593e-05,
      "loss": 0.1486,
      "step": 4110
    },
    {
      "epoch": 30.520446096654275,
      "grad_norm": 0.645348072052002,
      "learning_rate": 1.3897777777777778e-05,
      "loss": 0.1381,
      "step": 4120
    },
    {
      "epoch": 30.594795539033456,
      "grad_norm": 0.7771977782249451,
      "learning_rate": 1.3882962962962966e-05,
      "loss": 0.1275,
      "step": 4130
    },
    {
      "epoch": 30.66914498141264,
      "grad_norm": 0.6648851633071899,
      "learning_rate": 1.3868148148148148e-05,
      "loss": 0.1445,
      "step": 4140
    },
    {
      "epoch": 30.743494423791823,
      "grad_norm": 0.7747958898544312,
      "learning_rate": 1.3853333333333333e-05,
      "loss": 0.1377,
      "step": 4150
    },
    {
      "epoch": 30.817843866171003,
      "grad_norm": 1.029205560684204,
      "learning_rate": 1.383851851851852e-05,
      "loss": 0.1452,
      "step": 4160
    },
    {
      "epoch": 30.892193308550187,
      "grad_norm": 0.6092110872268677,
      "learning_rate": 1.3823703703703706e-05,
      "loss": 0.1499,
      "step": 4170
    },
    {
      "epoch": 30.966542750929367,
      "grad_norm": 0.9472110867500305,
      "learning_rate": 1.3808888888888889e-05,
      "loss": 0.1587,
      "step": 4180
    },
    {
      "epoch": 31.03717472118959,
      "grad_norm": 0.7490692138671875,
      "learning_rate": 1.3794074074074076e-05,
      "loss": 0.1468,
      "step": 4190
    },
    {
      "epoch": 31.111524163568774,
      "grad_norm": 0.7166115641593933,
      "learning_rate": 1.3779259259259262e-05,
      "loss": 0.1414,
      "step": 4200
    },
    {
      "epoch": 31.185873605947954,
      "grad_norm": 0.6789184212684631,
      "learning_rate": 1.3764444444444446e-05,
      "loss": 0.1426,
      "step": 4210
    },
    {
      "epoch": 31.260223048327138,
      "grad_norm": 0.7063342928886414,
      "learning_rate": 1.3749629629629631e-05,
      "loss": 0.1454,
      "step": 4220
    },
    {
      "epoch": 31.33457249070632,
      "grad_norm": 0.5569403767585754,
      "learning_rate": 1.3734814814814817e-05,
      "loss": 0.1336,
      "step": 4230
    },
    {
      "epoch": 31.4089219330855,
      "grad_norm": 0.7114928364753723,
      "learning_rate": 1.3720000000000002e-05,
      "loss": 0.1349,
      "step": 4240
    },
    {
      "epoch": 31.483271375464685,
      "grad_norm": 0.821897029876709,
      "learning_rate": 1.3705185185185186e-05,
      "loss": 0.1523,
      "step": 4250
    },
    {
      "epoch": 31.557620817843866,
      "grad_norm": 0.8296846151351929,
      "learning_rate": 1.3690370370370372e-05,
      "loss": 0.1566,
      "step": 4260
    },
    {
      "epoch": 31.63197026022305,
      "grad_norm": 0.7587635517120361,
      "learning_rate": 1.3675555555555558e-05,
      "loss": 0.1492,
      "step": 4270
    },
    {
      "epoch": 31.70631970260223,
      "grad_norm": 0.8069402575492859,
      "learning_rate": 1.3660740740740742e-05,
      "loss": 0.1335,
      "step": 4280
    },
    {
      "epoch": 31.780669144981413,
      "grad_norm": 0.7247507572174072,
      "learning_rate": 1.3645925925925927e-05,
      "loss": 0.1307,
      "step": 4290
    },
    {
      "epoch": 31.855018587360593,
      "grad_norm": 0.6936160922050476,
      "learning_rate": 1.3631111111111113e-05,
      "loss": 0.1635,
      "step": 4300
    },
    {
      "epoch": 31.929368029739777,
      "grad_norm": 0.7763386368751526,
      "learning_rate": 1.3616296296296297e-05,
      "loss": 0.1395,
      "step": 4310
    },
    {
      "epoch": 32.0,
      "grad_norm": 0.8545891642570496,
      "learning_rate": 1.3601481481481483e-05,
      "loss": 0.1524,
      "step": 4320
    },
    {
      "epoch": 32.07434944237918,
      "grad_norm": 0.6504292488098145,
      "learning_rate": 1.3586666666666668e-05,
      "loss": 0.145,
      "step": 4330
    },
    {
      "epoch": 32.14869888475837,
      "grad_norm": 0.7646320462226868,
      "learning_rate": 1.3571851851851854e-05,
      "loss": 0.1462,
      "step": 4340
    },
    {
      "epoch": 32.22304832713755,
      "grad_norm": 0.8521661162376404,
      "learning_rate": 1.3557037037037038e-05,
      "loss": 0.1566,
      "step": 4350
    },
    {
      "epoch": 32.29739776951673,
      "grad_norm": 0.7400633096694946,
      "learning_rate": 1.3542222222222223e-05,
      "loss": 0.1357,
      "step": 4360
    },
    {
      "epoch": 32.37174721189591,
      "grad_norm": 0.7720184326171875,
      "learning_rate": 1.3527407407407409e-05,
      "loss": 0.157,
      "step": 4370
    },
    {
      "epoch": 32.446096654275095,
      "grad_norm": 0.7130242586135864,
      "learning_rate": 1.3512592592592593e-05,
      "loss": 0.1491,
      "step": 4380
    },
    {
      "epoch": 32.520446096654275,
      "grad_norm": 0.8706707954406738,
      "learning_rate": 1.3497777777777779e-05,
      "loss": 0.1434,
      "step": 4390
    },
    {
      "epoch": 32.594795539033456,
      "grad_norm": 0.5427700281143188,
      "learning_rate": 1.3482962962962964e-05,
      "loss": 0.1289,
      "step": 4400
    },
    {
      "epoch": 32.66914498141264,
      "grad_norm": 0.604102611541748,
      "learning_rate": 1.346814814814815e-05,
      "loss": 0.1382,
      "step": 4410
    },
    {
      "epoch": 32.74349442379182,
      "grad_norm": 0.7174469232559204,
      "learning_rate": 1.3453333333333334e-05,
      "loss": 0.149,
      "step": 4420
    },
    {
      "epoch": 32.817843866171,
      "grad_norm": 0.7315161228179932,
      "learning_rate": 1.343851851851852e-05,
      "loss": 0.1458,
      "step": 4430
    },
    {
      "epoch": 32.89219330855018,
      "grad_norm": 0.6919483542442322,
      "learning_rate": 1.3423703703703705e-05,
      "loss": 0.1429,
      "step": 4440
    },
    {
      "epoch": 32.96654275092937,
      "grad_norm": 0.9165557622909546,
      "learning_rate": 1.3408888888888889e-05,
      "loss": 0.1369,
      "step": 4450
    },
    {
      "epoch": 33.037174721189594,
      "grad_norm": 0.5888960957527161,
      "learning_rate": 1.3394074074074075e-05,
      "loss": 0.128,
      "step": 4460
    },
    {
      "epoch": 33.111524163568774,
      "grad_norm": 0.795966386795044,
      "learning_rate": 1.337925925925926e-05,
      "loss": 0.1338,
      "step": 4470
    },
    {
      "epoch": 33.185873605947954,
      "grad_norm": 0.9098661541938782,
      "learning_rate": 1.3364444444444446e-05,
      "loss": 0.1474,
      "step": 4480
    },
    {
      "epoch": 33.260223048327134,
      "grad_norm": 0.7467618584632874,
      "learning_rate": 1.334962962962963e-05,
      "loss": 0.1333,
      "step": 4490
    },
    {
      "epoch": 33.33457249070632,
      "grad_norm": 0.8179594278335571,
      "learning_rate": 1.3334814814814816e-05,
      "loss": 0.1413,
      "step": 4500
    },
    {
      "epoch": 33.4089219330855,
      "grad_norm": 0.7345480918884277,
      "learning_rate": 1.3320000000000001e-05,
      "loss": 0.1637,
      "step": 4510
    },
    {
      "epoch": 33.48327137546468,
      "grad_norm": 0.7069793343544006,
      "learning_rate": 1.3305185185185185e-05,
      "loss": 0.1403,
      "step": 4520
    },
    {
      "epoch": 33.55762081784387,
      "grad_norm": 0.7587235569953918,
      "learning_rate": 1.329037037037037e-05,
      "loss": 0.147,
      "step": 4530
    },
    {
      "epoch": 33.63197026022305,
      "grad_norm": 0.5884326696395874,
      "learning_rate": 1.3275555555555556e-05,
      "loss": 0.1517,
      "step": 4540
    },
    {
      "epoch": 33.70631970260223,
      "grad_norm": 0.7348270416259766,
      "learning_rate": 1.3260740740740742e-05,
      "loss": 0.1624,
      "step": 4550
    },
    {
      "epoch": 33.78066914498141,
      "grad_norm": 0.712558925151825,
      "learning_rate": 1.3245925925925926e-05,
      "loss": 0.1372,
      "step": 4560
    },
    {
      "epoch": 33.8550185873606,
      "grad_norm": 0.6401731967926025,
      "learning_rate": 1.3231111111111112e-05,
      "loss": 0.1341,
      "step": 4570
    },
    {
      "epoch": 33.92936802973978,
      "grad_norm": 0.6037857532501221,
      "learning_rate": 1.3216296296296299e-05,
      "loss": 0.1339,
      "step": 4580
    },
    {
      "epoch": 34.0,
      "grad_norm": 1.1426119804382324,
      "learning_rate": 1.3201481481481481e-05,
      "loss": 0.1436,
      "step": 4590
    },
    {
      "epoch": 34.07434944237918,
      "grad_norm": 0.8542184233665466,
      "learning_rate": 1.3186666666666667e-05,
      "loss": 0.1365,
      "step": 4600
    },
    {
      "epoch": 34.14869888475837,
      "grad_norm": 0.6725401282310486,
      "learning_rate": 1.3171851851851854e-05,
      "loss": 0.1466,
      "step": 4610
    },
    {
      "epoch": 34.22304832713755,
      "grad_norm": 0.7466338276863098,
      "learning_rate": 1.3157037037037036e-05,
      "loss": 0.1365,
      "step": 4620
    },
    {
      "epoch": 34.29739776951673,
      "grad_norm": 0.7204360961914062,
      "learning_rate": 1.3142222222222222e-05,
      "loss": 0.1414,
      "step": 4630
    },
    {
      "epoch": 34.37174721189591,
      "grad_norm": 0.7772339582443237,
      "learning_rate": 1.312740740740741e-05,
      "loss": 0.1438,
      "step": 4640
    },
    {
      "epoch": 34.446096654275095,
      "grad_norm": 0.7773754000663757,
      "learning_rate": 1.3112592592592595e-05,
      "loss": 0.1336,
      "step": 4650
    },
    {
      "epoch": 34.520446096654275,
      "grad_norm": 0.9378052353858948,
      "learning_rate": 1.3097777777777777e-05,
      "loss": 0.1424,
      "step": 4660
    },
    {
      "epoch": 34.594795539033456,
      "grad_norm": 0.6563813090324402,
      "learning_rate": 1.3082962962962965e-05,
      "loss": 0.1549,
      "step": 4670
    },
    {
      "epoch": 34.66914498141264,
      "grad_norm": 0.6820051670074463,
      "learning_rate": 1.306814814814815e-05,
      "loss": 0.137,
      "step": 4680
    },
    {
      "epoch": 34.74349442379182,
      "grad_norm": 0.7759554386138916,
      "learning_rate": 1.3053333333333333e-05,
      "loss": 0.1536,
      "step": 4690
    },
    {
      "epoch": 34.817843866171,
      "grad_norm": 0.7295448184013367,
      "learning_rate": 1.303851851851852e-05,
      "loss": 0.1475,
      "step": 4700
    },
    {
      "epoch": 34.89219330855018,
      "grad_norm": 0.832973062992096,
      "learning_rate": 1.3023703703703706e-05,
      "loss": 0.1215,
      "step": 4710
    },
    {
      "epoch": 34.96654275092937,
      "grad_norm": 0.8498154878616333,
      "learning_rate": 1.3008888888888891e-05,
      "loss": 0.1509,
      "step": 4720
    },
    {
      "epoch": 35.037174721189594,
      "grad_norm": 0.8227860927581787,
      "learning_rate": 1.2994074074074075e-05,
      "loss": 0.1739,
      "step": 4730
    },
    {
      "epoch": 35.111524163568774,
      "grad_norm": 0.8126972913742065,
      "learning_rate": 1.297925925925926e-05,
      "loss": 0.1395,
      "step": 4740
    },
    {
      "epoch": 35.185873605947954,
      "grad_norm": 0.912212073802948,
      "learning_rate": 1.2964444444444446e-05,
      "loss": 0.151,
      "step": 4750
    },
    {
      "epoch": 35.260223048327134,
      "grad_norm": 0.7223882079124451,
      "learning_rate": 1.294962962962963e-05,
      "loss": 0.1434,
      "step": 4760
    },
    {
      "epoch": 35.33457249070632,
      "grad_norm": 0.9794180989265442,
      "learning_rate": 1.2934814814814816e-05,
      "loss": 0.1697,
      "step": 4770
    },
    {
      "epoch": 35.4089219330855,
      "grad_norm": 0.9095349311828613,
      "learning_rate": 1.2920000000000002e-05,
      "loss": 0.1432,
      "step": 4780
    },
    {
      "epoch": 35.48327137546468,
      "grad_norm": 0.8325972557067871,
      "learning_rate": 1.2905185185185187e-05,
      "loss": 0.1316,
      "step": 4790
    },
    {
      "epoch": 35.55762081784387,
      "grad_norm": 1.3086261749267578,
      "learning_rate": 1.2890370370370371e-05,
      "loss": 0.1496,
      "step": 4800
    },
    {
      "epoch": 35.63197026022305,
      "grad_norm": 0.8656348586082458,
      "learning_rate": 1.2875555555555557e-05,
      "loss": 0.1315,
      "step": 4810
    },
    {
      "epoch": 35.70631970260223,
      "grad_norm": 0.7601463794708252,
      "learning_rate": 1.2860740740740743e-05,
      "loss": 0.1441,
      "step": 4820
    },
    {
      "epoch": 35.78066914498141,
      "grad_norm": 0.8192383646965027,
      "learning_rate": 1.2845925925925926e-05,
      "loss": 0.1323,
      "step": 4830
    },
    {
      "epoch": 35.8550185873606,
      "grad_norm": 0.6263335347175598,
      "learning_rate": 1.2831111111111112e-05,
      "loss": 0.1379,
      "step": 4840
    },
    {
      "epoch": 35.92936802973978,
      "grad_norm": 0.6698663830757141,
      "learning_rate": 1.2816296296296298e-05,
      "loss": 0.1259,
      "step": 4850
    },
    {
      "epoch": 36.0,
      "grad_norm": 1.1996450424194336,
      "learning_rate": 1.2801481481481483e-05,
      "loss": 0.1472,
      "step": 4860
    },
    {
      "epoch": 36.07434944237918,
      "grad_norm": 0.6755689978599548,
      "learning_rate": 1.2786666666666667e-05,
      "loss": 0.1357,
      "step": 4870
    },
    {
      "epoch": 36.14869888475837,
      "grad_norm": 0.7996018528938293,
      "learning_rate": 1.2771851851851853e-05,
      "loss": 0.1508,
      "step": 4880
    },
    {
      "epoch": 36.22304832713755,
      "grad_norm": 0.779646635055542,
      "learning_rate": 1.2757037037037039e-05,
      "loss": 0.1523,
      "step": 4890
    },
    {
      "epoch": 36.29739776951673,
      "grad_norm": 0.9639499187469482,
      "learning_rate": 1.2742222222222223e-05,
      "loss": 0.1481,
      "step": 4900
    },
    {
      "epoch": 36.37174721189591,
      "grad_norm": 0.7014511823654175,
      "learning_rate": 1.2727407407407408e-05,
      "loss": 0.1431,
      "step": 4910
    },
    {
      "epoch": 36.446096654275095,
      "grad_norm": 0.620001494884491,
      "learning_rate": 1.2712592592592594e-05,
      "loss": 0.1268,
      "step": 4920
    },
    {
      "epoch": 36.520446096654275,
      "grad_norm": 0.7198531627655029,
      "learning_rate": 1.2697777777777778e-05,
      "loss": 0.1439,
      "step": 4930
    },
    {
      "epoch": 36.594795539033456,
      "grad_norm": 0.9308790564537048,
      "learning_rate": 1.2682962962962963e-05,
      "loss": 0.1416,
      "step": 4940
    },
    {
      "epoch": 36.66914498141264,
      "grad_norm": 0.9115259647369385,
      "learning_rate": 1.2668148148148149e-05,
      "loss": 0.1505,
      "step": 4950
    },
    {
      "epoch": 36.74349442379182,
      "grad_norm": 0.6416258215904236,
      "learning_rate": 1.2653333333333335e-05,
      "loss": 0.1488,
      "step": 4960
    },
    {
      "epoch": 36.817843866171,
      "grad_norm": 0.5585889220237732,
      "learning_rate": 1.2638518518518519e-05,
      "loss": 0.1297,
      "step": 4970
    },
    {
      "epoch": 36.89219330855018,
      "grad_norm": 0.7188727855682373,
      "learning_rate": 1.2623703703703704e-05,
      "loss": 0.1469,
      "step": 4980
    },
    {
      "epoch": 36.96654275092937,
      "grad_norm": 0.7306336164474487,
      "learning_rate": 1.260888888888889e-05,
      "loss": 0.1408,
      "step": 4990
    },
    {
      "epoch": 37.037174721189594,
      "grad_norm": 0.7815173268318176,
      "learning_rate": 1.2594074074074074e-05,
      "loss": 0.1236,
      "step": 5000
    },
    {
      "epoch": 37.111524163568774,
      "grad_norm": 0.8389245867729187,
      "learning_rate": 1.257925925925926e-05,
      "loss": 0.1291,
      "step": 5010
    },
    {
      "epoch": 37.185873605947954,
      "grad_norm": 0.8392107486724854,
      "learning_rate": 1.2564444444444445e-05,
      "loss": 0.1274,
      "step": 5020
    },
    {
      "epoch": 37.260223048327134,
      "grad_norm": 0.8033727407455444,
      "learning_rate": 1.254962962962963e-05,
      "loss": 0.147,
      "step": 5030
    },
    {
      "epoch": 37.33457249070632,
      "grad_norm": 1.1600043773651123,
      "learning_rate": 1.2534814814814815e-05,
      "loss": 0.1436,
      "step": 5040
    },
    {
      "epoch": 37.4089219330855,
      "grad_norm": 0.7946189641952515,
      "learning_rate": 1.252e-05,
      "loss": 0.1626,
      "step": 5050
    },
    {
      "epoch": 37.48327137546468,
      "grad_norm": 0.6500661373138428,
      "learning_rate": 1.2505185185185186e-05,
      "loss": 0.1424,
      "step": 5060
    },
    {
      "epoch": 37.55762081784387,
      "grad_norm": 0.9198545217514038,
      "learning_rate": 1.249037037037037e-05,
      "loss": 0.1473,
      "step": 5070
    },
    {
      "epoch": 37.63197026022305,
      "grad_norm": 0.7546985149383545,
      "learning_rate": 1.2475555555555556e-05,
      "loss": 0.1326,
      "step": 5080
    },
    {
      "epoch": 37.70631970260223,
      "grad_norm": 0.7415857911109924,
      "learning_rate": 1.2460740740740743e-05,
      "loss": 0.155,
      "step": 5090
    },
    {
      "epoch": 37.78066914498141,
      "grad_norm": 0.7547574639320374,
      "learning_rate": 1.2445925925925929e-05,
      "loss": 0.1369,
      "step": 5100
    },
    {
      "epoch": 37.8550185873606,
      "grad_norm": 0.713020920753479,
      "learning_rate": 1.2431111111111111e-05,
      "loss": 0.1356,
      "step": 5110
    },
    {
      "epoch": 37.92936802973978,
      "grad_norm": 0.8239905834197998,
      "learning_rate": 1.2416296296296298e-05,
      "loss": 0.1416,
      "step": 5120
    },
    {
      "epoch": 38.0,
      "grad_norm": 0.8165380358695984,
      "learning_rate": 1.2401481481481484e-05,
      "loss": 0.1449,
      "step": 5130
    },
    {
      "epoch": 38.07434944237918,
      "grad_norm": 0.8123170733451843,
      "learning_rate": 1.2386666666666666e-05,
      "loss": 0.1535,
      "step": 5140
    },
    {
      "epoch": 38.14869888475837,
      "grad_norm": 0.8387107253074646,
      "learning_rate": 1.2371851851851853e-05,
      "loss": 0.153,
      "step": 5150
    },
    {
      "epoch": 38.22304832713755,
      "grad_norm": 0.7942659854888916,
      "learning_rate": 1.2357037037037039e-05,
      "loss": 0.1398,
      "step": 5160
    },
    {
      "epoch": 38.29739776951673,
      "grad_norm": 0.7689914703369141,
      "learning_rate": 1.2342222222222225e-05,
      "loss": 0.153,
      "step": 5170
    },
    {
      "epoch": 38.37174721189591,
      "grad_norm": 0.6632983088493347,
      "learning_rate": 1.2327407407407409e-05,
      "loss": 0.1425,
      "step": 5180
    },
    {
      "epoch": 38.446096654275095,
      "grad_norm": 0.7620285749435425,
      "learning_rate": 1.2312592592592594e-05,
      "loss": 0.1524,
      "step": 5190
    },
    {
      "epoch": 38.520446096654275,
      "grad_norm": 0.6866893768310547,
      "learning_rate": 1.229777777777778e-05,
      "loss": 0.1276,
      "step": 5200
    },
    {
      "epoch": 38.594795539033456,
      "grad_norm": 0.8321846723556519,
      "learning_rate": 1.2282962962962964e-05,
      "loss": 0.1293,
      "step": 5210
    },
    {
      "epoch": 38.66914498141264,
      "grad_norm": 0.8454651236534119,
      "learning_rate": 1.226814814814815e-05,
      "loss": 0.1355,
      "step": 5220
    },
    {
      "epoch": 38.74349442379182,
      "grad_norm": 0.7763826847076416,
      "learning_rate": 1.2253333333333335e-05,
      "loss": 0.1463,
      "step": 5230
    },
    {
      "epoch": 38.817843866171,
      "grad_norm": 0.6621485948562622,
      "learning_rate": 1.2238518518518519e-05,
      "loss": 0.1227,
      "step": 5240
    },
    {
      "epoch": 38.89219330855018,
      "grad_norm": 1.034730076789856,
      "learning_rate": 1.2223703703703705e-05,
      "loss": 0.1376,
      "step": 5250
    },
    {
      "epoch": 38.96654275092937,
      "grad_norm": 0.6690853834152222,
      "learning_rate": 1.220888888888889e-05,
      "loss": 0.1415,
      "step": 5260
    },
    {
      "epoch": 39.037174721189594,
      "grad_norm": 0.8323618173599243,
      "learning_rate": 1.2194074074074076e-05,
      "loss": 0.1359,
      "step": 5270
    },
    {
      "epoch": 39.111524163568774,
      "grad_norm": 0.8897619843482971,
      "learning_rate": 1.217925925925926e-05,
      "loss": 0.1401,
      "step": 5280
    },
    {
      "epoch": 39.185873605947954,
      "grad_norm": 0.8982594013214111,
      "learning_rate": 1.2164444444444446e-05,
      "loss": 0.1264,
      "step": 5290
    },
    {
      "epoch": 39.260223048327134,
      "grad_norm": 0.6864228248596191,
      "learning_rate": 1.2149629629629631e-05,
      "loss": 0.1348,
      "step": 5300
    },
    {
      "epoch": 39.33457249070632,
      "grad_norm": 0.9453427195549011,
      "learning_rate": 1.2134814814814815e-05,
      "loss": 0.1404,
      "step": 5310
    },
    {
      "epoch": 39.4089219330855,
      "grad_norm": 0.6978464722633362,
      "learning_rate": 1.2120000000000001e-05,
      "loss": 0.127,
      "step": 5320
    },
    {
      "epoch": 39.48327137546468,
      "grad_norm": 0.7035659551620483,
      "learning_rate": 1.2105185185185186e-05,
      "loss": 0.1657,
      "step": 5330
    },
    {
      "epoch": 39.55762081784387,
      "grad_norm": 0.7685458660125732,
      "learning_rate": 1.2090370370370372e-05,
      "loss": 0.1499,
      "step": 5340
    },
    {
      "epoch": 39.63197026022305,
      "grad_norm": 0.7804647088050842,
      "learning_rate": 1.2075555555555556e-05,
      "loss": 0.1382,
      "step": 5350
    },
    {
      "epoch": 39.70631970260223,
      "grad_norm": 0.8790193200111389,
      "learning_rate": 1.2060740740740742e-05,
      "loss": 0.1382,
      "step": 5360
    },
    {
      "epoch": 39.78066914498141,
      "grad_norm": 0.8009659647941589,
      "learning_rate": 1.2045925925925927e-05,
      "loss": 0.1505,
      "step": 5370
    },
    {
      "epoch": 39.8550185873606,
      "grad_norm": 0.8711134195327759,
      "learning_rate": 1.2031111111111111e-05,
      "loss": 0.1304,
      "step": 5380
    },
    {
      "epoch": 39.92936802973978,
      "grad_norm": 0.9221194982528687,
      "learning_rate": 1.2016296296296297e-05,
      "loss": 0.1459,
      "step": 5390
    },
    {
      "epoch": 40.0,
      "grad_norm": 1.1878141164779663,
      "learning_rate": 1.2001481481481483e-05,
      "loss": 0.1446,
      "step": 5400
    },
    {
      "epoch": 40.07434944237918,
      "grad_norm": 0.6714752316474915,
      "learning_rate": 1.1986666666666668e-05,
      "loss": 0.1535,
      "step": 5410
    },
    {
      "epoch": 40.14869888475837,
      "grad_norm": 0.7389535307884216,
      "learning_rate": 1.1971851851851852e-05,
      "loss": 0.1495,
      "step": 5420
    },
    {
      "epoch": 40.22304832713755,
      "grad_norm": 0.7668905258178711,
      "learning_rate": 1.1957037037037038e-05,
      "loss": 0.1437,
      "step": 5430
    },
    {
      "epoch": 40.29739776951673,
      "grad_norm": 0.831672728061676,
      "learning_rate": 1.1942222222222223e-05,
      "loss": 0.1433,
      "step": 5440
    },
    {
      "epoch": 40.37174721189591,
      "grad_norm": 0.7801321744918823,
      "learning_rate": 1.1927407407407407e-05,
      "loss": 0.136,
      "step": 5450
    },
    {
      "epoch": 40.446096654275095,
      "grad_norm": 0.7041829228401184,
      "learning_rate": 1.1912592592592593e-05,
      "loss": 0.1315,
      "step": 5460
    },
    {
      "epoch": 40.520446096654275,
      "grad_norm": 0.7807779908180237,
      "learning_rate": 1.1897777777777779e-05,
      "loss": 0.1556,
      "step": 5470
    },
    {
      "epoch": 40.594795539033456,
      "grad_norm": 0.6863876581192017,
      "learning_rate": 1.1882962962962964e-05,
      "loss": 0.1277,
      "step": 5480
    },
    {
      "epoch": 40.66914498141264,
      "grad_norm": 0.7229515314102173,
      "learning_rate": 1.1868148148148148e-05,
      "loss": 0.1556,
      "step": 5490
    },
    {
      "epoch": 40.74349442379182,
      "grad_norm": 0.8720845580101013,
      "learning_rate": 1.1853333333333334e-05,
      "loss": 0.1199,
      "step": 5500
    },
    {
      "epoch": 40.817843866171,
      "grad_norm": 0.8878641128540039,
      "learning_rate": 1.183851851851852e-05,
      "loss": 0.1373,
      "step": 5510
    },
    {
      "epoch": 40.89219330855018,
      "grad_norm": 0.8241648077964783,
      "learning_rate": 1.1823703703703703e-05,
      "loss": 0.1443,
      "step": 5520
    },
    {
      "epoch": 40.96654275092937,
      "grad_norm": 1.0139250755310059,
      "learning_rate": 1.1808888888888889e-05,
      "loss": 0.1312,
      "step": 5530
    },
    {
      "epoch": 41.037174721189594,
      "grad_norm": 0.9473370313644409,
      "learning_rate": 1.1794074074074075e-05,
      "loss": 0.1412,
      "step": 5540
    },
    {
      "epoch": 41.111524163568774,
      "grad_norm": 0.740938127040863,
      "learning_rate": 1.1779259259259259e-05,
      "loss": 0.1354,
      "step": 5550
    },
    {
      "epoch": 41.185873605947954,
      "grad_norm": 0.7971646189689636,
      "learning_rate": 1.1764444444444444e-05,
      "loss": 0.1345,
      "step": 5560
    },
    {
      "epoch": 41.260223048327134,
      "grad_norm": 0.8201735019683838,
      "learning_rate": 1.174962962962963e-05,
      "loss": 0.1278,
      "step": 5570
    },
    {
      "epoch": 41.33457249070632,
      "grad_norm": 0.9075198769569397,
      "learning_rate": 1.1734814814814817e-05,
      "loss": 0.1457,
      "step": 5580
    },
    {
      "epoch": 41.4089219330855,
      "grad_norm": 0.8470796942710876,
      "learning_rate": 1.172e-05,
      "loss": 0.1387,
      "step": 5590
    },
    {
      "epoch": 41.48327137546468,
      "grad_norm": 0.9408391714096069,
      "learning_rate": 1.1705185185185187e-05,
      "loss": 0.1399,
      "step": 5600
    },
    {
      "epoch": 41.55762081784387,
      "grad_norm": 0.6895107626914978,
      "learning_rate": 1.1690370370370373e-05,
      "loss": 0.1314,
      "step": 5610
    },
    {
      "epoch": 41.63197026022305,
      "grad_norm": 0.8981918692588806,
      "learning_rate": 1.1675555555555555e-05,
      "loss": 0.1419,
      "step": 5620
    },
    {
      "epoch": 41.70631970260223,
      "grad_norm": 0.7601481080055237,
      "learning_rate": 1.1660740740740742e-05,
      "loss": 0.167,
      "step": 5630
    },
    {
      "epoch": 41.78066914498141,
      "grad_norm": 0.7966218590736389,
      "learning_rate": 1.1645925925925928e-05,
      "loss": 0.134,
      "step": 5640
    },
    {
      "epoch": 41.8550185873606,
      "grad_norm": 0.821584165096283,
      "learning_rate": 1.1631111111111113e-05,
      "loss": 0.1465,
      "step": 5650
    },
    {
      "epoch": 41.92936802973978,
      "grad_norm": 0.6366214156150818,
      "learning_rate": 1.1616296296296297e-05,
      "loss": 0.1434,
      "step": 5660
    },
    {
      "epoch": 42.0,
      "grad_norm": 0.9801803827285767,
      "learning_rate": 1.1601481481481483e-05,
      "loss": 0.1378,
      "step": 5670
    },
    {
      "epoch": 42.07434944237918,
      "grad_norm": 0.6396491527557373,
      "learning_rate": 1.1586666666666669e-05,
      "loss": 0.1355,
      "step": 5680
    },
    {
      "epoch": 42.14869888475837,
      "grad_norm": 1.141607642173767,
      "learning_rate": 1.1571851851851853e-05,
      "loss": 0.1435,
      "step": 5690
    },
    {
      "epoch": 42.22304832713755,
      "grad_norm": 0.7940685749053955,
      "learning_rate": 1.1557037037037038e-05,
      "loss": 0.1398,
      "step": 5700
    },
    {
      "epoch": 42.29739776951673,
      "grad_norm": 0.9179121255874634,
      "learning_rate": 1.1542222222222224e-05,
      "loss": 0.1387,
      "step": 5710
    },
    {
      "epoch": 42.37174721189591,
      "grad_norm": 0.9401230216026306,
      "learning_rate": 1.152740740740741e-05,
      "loss": 0.1337,
      "step": 5720
    },
    {
      "epoch": 42.446096654275095,
      "grad_norm": 0.6834712028503418,
      "learning_rate": 1.1512592592592593e-05,
      "loss": 0.1466,
      "step": 5730
    },
    {
      "epoch": 42.520446096654275,
      "grad_norm": 0.778611958026886,
      "learning_rate": 1.1497777777777779e-05,
      "loss": 0.1342,
      "step": 5740
    },
    {
      "epoch": 42.594795539033456,
      "grad_norm": 0.9715090394020081,
      "learning_rate": 1.1482962962962965e-05,
      "loss": 0.1472,
      "step": 5750
    },
    {
      "epoch": 42.66914498141264,
      "grad_norm": 0.8588351011276245,
      "learning_rate": 1.1468148148148149e-05,
      "loss": 0.1475,
      "step": 5760
    },
    {
      "epoch": 42.74349442379182,
      "grad_norm": 0.7188964486122131,
      "learning_rate": 1.1453333333333334e-05,
      "loss": 0.1521,
      "step": 5770
    },
    {
      "epoch": 42.817843866171,
      "grad_norm": 0.9022670388221741,
      "learning_rate": 1.143851851851852e-05,
      "loss": 0.1304,
      "step": 5780
    },
    {
      "epoch": 42.89219330855018,
      "grad_norm": 0.7569448351860046,
      "learning_rate": 1.1423703703703706e-05,
      "loss": 0.1311,
      "step": 5790
    },
    {
      "epoch": 42.96654275092937,
      "grad_norm": 0.8492839932441711,
      "learning_rate": 1.140888888888889e-05,
      "loss": 0.1335,
      "step": 5800
    },
    {
      "epoch": 43.037174721189594,
      "grad_norm": 0.8150979280471802,
      "learning_rate": 1.1394074074074075e-05,
      "loss": 0.1558,
      "step": 5810
    },
    {
      "epoch": 43.111524163568774,
      "grad_norm": 0.7894749641418457,
      "learning_rate": 1.1379259259259261e-05,
      "loss": 0.1465,
      "step": 5820
    },
    {
      "epoch": 43.185873605947954,
      "grad_norm": 0.7321264743804932,
      "learning_rate": 1.1364444444444445e-05,
      "loss": 0.1484,
      "step": 5830
    },
    {
      "epoch": 43.260223048327134,
      "grad_norm": 1.0157814025878906,
      "learning_rate": 1.134962962962963e-05,
      "loss": 0.1425,
      "step": 5840
    },
    {
      "epoch": 43.33457249070632,
      "grad_norm": 0.7966969609260559,
      "learning_rate": 1.1334814814814816e-05,
      "loss": 0.1358,
      "step": 5850
    },
    {
      "epoch": 43.4089219330855,
      "grad_norm": 1.0489351749420166,
      "learning_rate": 1.132e-05,
      "loss": 0.1639,
      "step": 5860
    },
    {
      "epoch": 43.48327137546468,
      "grad_norm": 0.6841955780982971,
      "learning_rate": 1.1305185185185186e-05,
      "loss": 0.1182,
      "step": 5870
    },
    {
      "epoch": 43.55762081784387,
      "grad_norm": 0.7842550277709961,
      "learning_rate": 1.1290370370370371e-05,
      "loss": 0.1419,
      "step": 5880
    },
    {
      "epoch": 43.63197026022305,
      "grad_norm": 0.8645634651184082,
      "learning_rate": 1.1275555555555557e-05,
      "loss": 0.1226,
      "step": 5890
    },
    {
      "epoch": 43.70631970260223,
      "grad_norm": 0.7788944840431213,
      "learning_rate": 1.1260740740740741e-05,
      "loss": 0.1418,
      "step": 5900
    },
    {
      "epoch": 43.78066914498141,
      "grad_norm": 0.8314153552055359,
      "learning_rate": 1.1245925925925927e-05,
      "loss": 0.1208,
      "step": 5910
    },
    {
      "epoch": 43.8550185873606,
      "grad_norm": 0.7658310532569885,
      "learning_rate": 1.1231111111111112e-05,
      "loss": 0.1413,
      "step": 5920
    },
    {
      "epoch": 43.92936802973978,
      "grad_norm": 0.8322836756706238,
      "learning_rate": 1.1216296296296296e-05,
      "loss": 0.1563,
      "step": 5930
    },
    {
      "epoch": 44.0,
      "grad_norm": 0.915708065032959,
      "learning_rate": 1.1201481481481482e-05,
      "loss": 0.1244,
      "step": 5940
    },
    {
      "epoch": 44.07434944237918,
      "grad_norm": 0.7047370076179504,
      "learning_rate": 1.1186666666666667e-05,
      "loss": 0.1514,
      "step": 5950
    },
    {
      "epoch": 44.14869888475837,
      "grad_norm": 0.9351638555526733,
      "learning_rate": 1.1171851851851853e-05,
      "loss": 0.1235,
      "step": 5960
    },
    {
      "epoch": 44.22304832713755,
      "grad_norm": 0.6310678124427795,
      "learning_rate": 1.1157037037037037e-05,
      "loss": 0.1336,
      "step": 5970
    },
    {
      "epoch": 44.29739776951673,
      "grad_norm": 0.9353320598602295,
      "learning_rate": 1.1142222222222223e-05,
      "loss": 0.1412,
      "step": 5980
    },
    {
      "epoch": 44.37174721189591,
      "grad_norm": 0.6952980756759644,
      "learning_rate": 1.1127407407407408e-05,
      "loss": 0.1462,
      "step": 5990
    },
    {
      "epoch": 44.446096654275095,
      "grad_norm": 0.768598735332489,
      "learning_rate": 1.1112592592592592e-05,
      "loss": 0.1261,
      "step": 6000
    },
    {
      "epoch": 44.520446096654275,
      "grad_norm": 0.7376413345336914,
      "learning_rate": 1.1097777777777778e-05,
      "loss": 0.1476,
      "step": 6010
    },
    {
      "epoch": 44.594795539033456,
      "grad_norm": 0.7190436720848083,
      "learning_rate": 1.1082962962962964e-05,
      "loss": 0.1423,
      "step": 6020
    },
    {
      "epoch": 44.66914498141264,
      "grad_norm": 0.8230137825012207,
      "learning_rate": 1.1068148148148151e-05,
      "loss": 0.1644,
      "step": 6030
    },
    {
      "epoch": 44.74349442379182,
      "grad_norm": 0.8278799057006836,
      "learning_rate": 1.1053333333333333e-05,
      "loss": 0.1324,
      "step": 6040
    },
    {
      "epoch": 44.817843866171,
      "grad_norm": 0.7631744146347046,
      "learning_rate": 1.1038518518518519e-05,
      "loss": 0.1343,
      "step": 6050
    },
    {
      "epoch": 44.89219330855018,
      "grad_norm": 0.8974156975746155,
      "learning_rate": 1.1023703703703706e-05,
      "loss": 0.1271,
      "step": 6060
    },
    {
      "epoch": 44.96654275092937,
      "grad_norm": 0.678455650806427,
      "learning_rate": 1.1008888888888888e-05,
      "loss": 0.1533,
      "step": 6070
    },
    {
      "epoch": 45.037174721189594,
      "grad_norm": 0.8536475300788879,
      "learning_rate": 1.0994074074074074e-05,
      "loss": 0.1263,
      "step": 6080
    },
    {
      "epoch": 45.111524163568774,
      "grad_norm": 0.9030718207359314,
      "learning_rate": 1.0979259259259261e-05,
      "loss": 0.1304,
      "step": 6090
    },
    {
      "epoch": 45.185873605947954,
      "grad_norm": 0.7098236083984375,
      "learning_rate": 1.0964444444444447e-05,
      "loss": 0.1258,
      "step": 6100
    },
    {
      "epoch": 45.260223048327134,
      "grad_norm": 0.8781617283821106,
      "learning_rate": 1.0949629629629631e-05,
      "loss": 0.1504,
      "step": 6110
    },
    {
      "epoch": 45.33457249070632,
      "grad_norm": 0.7046459317207336,
      "learning_rate": 1.0934814814814817e-05,
      "loss": 0.1296,
      "step": 6120
    },
    {
      "epoch": 45.4089219330855,
      "grad_norm": 0.911868155002594,
      "learning_rate": 1.0920000000000002e-05,
      "loss": 0.1379,
      "step": 6130
    },
    {
      "epoch": 45.48327137546468,
      "grad_norm": 0.8887191414833069,
      "learning_rate": 1.0905185185185186e-05,
      "loss": 0.1342,
      "step": 6140
    },
    {
      "epoch": 45.55762081784387,
      "grad_norm": 0.8299227952957153,
      "learning_rate": 1.0890370370370372e-05,
      "loss": 0.1484,
      "step": 6150
    },
    {
      "epoch": 45.63197026022305,
      "grad_norm": 0.8028130531311035,
      "learning_rate": 1.0875555555555557e-05,
      "loss": 0.146,
      "step": 6160
    },
    {
      "epoch": 45.70631970260223,
      "grad_norm": 0.9069734811782837,
      "learning_rate": 1.0860740740740741e-05,
      "loss": 0.1355,
      "step": 6170
    },
    {
      "epoch": 45.78066914498141,
      "grad_norm": 0.8649283051490784,
      "learning_rate": 1.0845925925925927e-05,
      "loss": 0.149,
      "step": 6180
    },
    {
      "epoch": 45.8550185873606,
      "grad_norm": 0.6322265267372131,
      "learning_rate": 1.0831111111111113e-05,
      "loss": 0.1597,
      "step": 6190
    },
    {
      "epoch": 45.92936802973978,
      "grad_norm": 0.8323211669921875,
      "learning_rate": 1.0816296296296298e-05,
      "loss": 0.135,
      "step": 6200
    },
    {
      "epoch": 46.0,
      "grad_norm": 1.1417790651321411,
      "learning_rate": 1.0801481481481482e-05,
      "loss": 0.13,
      "step": 6210
    },
    {
      "epoch": 46.07434944237918,
      "grad_norm": 0.9085761904716492,
      "learning_rate": 1.0786666666666668e-05,
      "loss": 0.1484,
      "step": 6220
    },
    {
      "epoch": 46.14869888475837,
      "grad_norm": 1.1030857563018799,
      "learning_rate": 1.0771851851851853e-05,
      "loss": 0.1371,
      "step": 6230
    },
    {
      "epoch": 46.22304832713755,
      "grad_norm": 0.8168609738349915,
      "learning_rate": 1.0757037037037037e-05,
      "loss": 0.1316,
      "step": 6240
    },
    {
      "epoch": 46.29739776951673,
      "grad_norm": 0.718845784664154,
      "learning_rate": 1.0742222222222223e-05,
      "loss": 0.1249,
      "step": 6250
    },
    {
      "epoch": 46.37174721189591,
      "grad_norm": 0.8524371981620789,
      "learning_rate": 1.0727407407407409e-05,
      "loss": 0.1473,
      "step": 6260
    },
    {
      "epoch": 46.446096654275095,
      "grad_norm": 1.0894261598587036,
      "learning_rate": 1.0712592592592594e-05,
      "loss": 0.1341,
      "step": 6270
    },
    {
      "epoch": 46.520446096654275,
      "grad_norm": 0.98198002576828,
      "learning_rate": 1.0697777777777778e-05,
      "loss": 0.1257,
      "step": 6280
    },
    {
      "epoch": 46.594795539033456,
      "grad_norm": 0.859621524810791,
      "learning_rate": 1.0682962962962964e-05,
      "loss": 0.1379,
      "step": 6290
    },
    {
      "epoch": 46.66914498141264,
      "grad_norm": 0.8369400501251221,
      "learning_rate": 1.066814814814815e-05,
      "loss": 0.141,
      "step": 6300
    },
    {
      "epoch": 46.74349442379182,
      "grad_norm": 0.8018566966056824,
      "learning_rate": 1.0653333333333334e-05,
      "loss": 0.1368,
      "step": 6310
    },
    {
      "epoch": 46.817843866171,
      "grad_norm": 0.8134254813194275,
      "learning_rate": 1.063851851851852e-05,
      "loss": 0.1483,
      "step": 6320
    },
    {
      "epoch": 46.89219330855018,
      "grad_norm": 0.8268395066261292,
      "learning_rate": 1.0623703703703705e-05,
      "loss": 0.1529,
      "step": 6330
    },
    {
      "epoch": 46.96654275092937,
      "grad_norm": 0.7625230550765991,
      "learning_rate": 1.060888888888889e-05,
      "loss": 0.1463,
      "step": 6340
    },
    {
      "epoch": 47.037174721189594,
      "grad_norm": 0.9333414435386658,
      "learning_rate": 1.0594074074074074e-05,
      "loss": 0.1217,
      "step": 6350
    },
    {
      "epoch": 47.111524163568774,
      "grad_norm": 0.9263480305671692,
      "learning_rate": 1.057925925925926e-05,
      "loss": 0.121,
      "step": 6360
    },
    {
      "epoch": 47.185873605947954,
      "grad_norm": 0.8047862648963928,
      "learning_rate": 1.0564444444444446e-05,
      "loss": 0.1494,
      "step": 6370
    },
    {
      "epoch": 47.260223048327134,
      "grad_norm": 1.0145810842514038,
      "learning_rate": 1.054962962962963e-05,
      "loss": 0.1399,
      "step": 6380
    },
    {
      "epoch": 47.33457249070632,
      "grad_norm": 0.7211349010467529,
      "learning_rate": 1.0534814814814815e-05,
      "loss": 0.1149,
      "step": 6390
    },
    {
      "epoch": 47.4089219330855,
      "grad_norm": 0.7547202110290527,
      "learning_rate": 1.0520000000000001e-05,
      "loss": 0.1376,
      "step": 6400
    },
    {
      "epoch": 47.48327137546468,
      "grad_norm": 0.7661087512969971,
      "learning_rate": 1.0505185185185187e-05,
      "loss": 0.1419,
      "step": 6410
    },
    {
      "epoch": 47.55762081784387,
      "grad_norm": 0.807858943939209,
      "learning_rate": 1.049037037037037e-05,
      "loss": 0.1367,
      "step": 6420
    },
    {
      "epoch": 47.63197026022305,
      "grad_norm": 0.9209626913070679,
      "learning_rate": 1.0475555555555556e-05,
      "loss": 0.1524,
      "step": 6430
    },
    {
      "epoch": 47.70631970260223,
      "grad_norm": 0.9801616072654724,
      "learning_rate": 1.0460740740740742e-05,
      "loss": 0.141,
      "step": 6440
    },
    {
      "epoch": 47.78066914498141,
      "grad_norm": 0.8803663849830627,
      "learning_rate": 1.0445925925925926e-05,
      "loss": 0.1399,
      "step": 6450
    },
    {
      "epoch": 47.8550185873606,
      "grad_norm": 0.8209445476531982,
      "learning_rate": 1.0431111111111111e-05,
      "loss": 0.1323,
      "step": 6460
    },
    {
      "epoch": 47.92936802973978,
      "grad_norm": 1.0482362508773804,
      "learning_rate": 1.0416296296296297e-05,
      "loss": 0.1609,
      "step": 6470
    },
    {
      "epoch": 48.0,
      "grad_norm": 1.2680350542068481,
      "learning_rate": 1.0401481481481481e-05,
      "loss": 0.1371,
      "step": 6480
    },
    {
      "epoch": 48.07434944237918,
      "grad_norm": 0.9238502979278564,
      "learning_rate": 1.0386666666666667e-05,
      "loss": 0.1341,
      "step": 6490
    },
    {
      "epoch": 48.14869888475837,
      "grad_norm": 0.9558349251747131,
      "learning_rate": 1.0371851851851852e-05,
      "loss": 0.1571,
      "step": 6500
    },
    {
      "epoch": 48.22304832713755,
      "grad_norm": 0.8480324149131775,
      "learning_rate": 1.035703703703704e-05,
      "loss": 0.1264,
      "step": 6510
    },
    {
      "epoch": 48.29739776951673,
      "grad_norm": 0.9546270966529846,
      "learning_rate": 1.0342222222222222e-05,
      "loss": 0.124,
      "step": 6520
    },
    {
      "epoch": 48.37174721189591,
      "grad_norm": 0.8991891741752625,
      "learning_rate": 1.0327407407407407e-05,
      "loss": 0.1427,
      "step": 6530
    },
    {
      "epoch": 48.446096654275095,
      "grad_norm": 0.8555591702461243,
      "learning_rate": 1.0312592592592595e-05,
      "loss": 0.1375,
      "step": 6540
    },
    {
      "epoch": 48.520446096654275,
      "grad_norm": 0.8541957139968872,
      "learning_rate": 1.0297777777777777e-05,
      "loss": 0.1482,
      "step": 6550
    },
    {
      "epoch": 48.594795539033456,
      "grad_norm": 0.7867615818977356,
      "learning_rate": 1.0282962962962963e-05,
      "loss": 0.1384,
      "step": 6560
    },
    {
      "epoch": 48.66914498141264,
      "grad_norm": 0.7747805118560791,
      "learning_rate": 1.026814814814815e-05,
      "loss": 0.1391,
      "step": 6570
    },
    {
      "epoch": 48.74349442379182,
      "grad_norm": 0.7002618908882141,
      "learning_rate": 1.0253333333333336e-05,
      "loss": 0.1513,
      "step": 6580
    },
    {
      "epoch": 48.817843866171,
      "grad_norm": 1.0067975521087646,
      "learning_rate": 1.0238518518518518e-05,
      "loss": 0.1322,
      "step": 6590
    },
    {
      "epoch": 48.89219330855018,
      "grad_norm": 0.89662104845047,
      "learning_rate": 1.0223703703703705e-05,
      "loss": 0.1306,
      "step": 6600
    },
    {
      "epoch": 48.96654275092937,
      "grad_norm": 0.9420503377914429,
      "learning_rate": 1.0208888888888891e-05,
      "loss": 0.1329,
      "step": 6610
    },
    {
      "epoch": 49.037174721189594,
      "grad_norm": 1.006476640701294,
      "learning_rate": 1.0194074074074075e-05,
      "loss": 0.1579,
      "step": 6620
    },
    {
      "epoch": 49.111524163568774,
      "grad_norm": 0.7323735356330872,
      "learning_rate": 1.017925925925926e-05,
      "loss": 0.1347,
      "step": 6630
    },
    {
      "epoch": 49.185873605947954,
      "grad_norm": 0.9191493391990662,
      "learning_rate": 1.0164444444444446e-05,
      "loss": 0.1509,
      "step": 6640
    },
    {
      "epoch": 49.260223048327134,
      "grad_norm": 0.9332489967346191,
      "learning_rate": 1.0149629629629632e-05,
      "loss": 0.1355,
      "step": 6650
    },
    {
      "epoch": 49.33457249070632,
      "grad_norm": 0.9539236426353455,
      "learning_rate": 1.0134814814814816e-05,
      "loss": 0.1484,
      "step": 6660
    },
    {
      "epoch": 49.4089219330855,
      "grad_norm": 0.7352766394615173,
      "learning_rate": 1.0120000000000001e-05,
      "loss": 0.1291,
      "step": 6670
    },
    {
      "epoch": 49.48327137546468,
      "grad_norm": 0.9481037855148315,
      "learning_rate": 1.0105185185185187e-05,
      "loss": 0.1472,
      "step": 6680
    },
    {
      "epoch": 49.55762081784387,
      "grad_norm": 0.8429032564163208,
      "learning_rate": 1.0090370370370371e-05,
      "loss": 0.1226,
      "step": 6690
    },
    {
      "epoch": 49.63197026022305,
      "grad_norm": 1.0061721801757812,
      "learning_rate": 1.0075555555555557e-05,
      "loss": 0.1318,
      "step": 6700
    },
    {
      "epoch": 49.70631970260223,
      "grad_norm": 1.086080551147461,
      "learning_rate": 1.0060740740740742e-05,
      "loss": 0.1348,
      "step": 6710
    },
    {
      "epoch": 49.78066914498141,
      "grad_norm": 0.9727036356925964,
      "learning_rate": 1.0045925925925928e-05,
      "loss": 0.1286,
      "step": 6720
    },
    {
      "epoch": 49.8550185873606,
      "grad_norm": 0.8132127523422241,
      "learning_rate": 1.0031111111111112e-05,
      "loss": 0.1473,
      "step": 6730
    },
    {
      "epoch": 49.92936802973978,
      "grad_norm": 0.8147075772285461,
      "learning_rate": 1.0016296296296297e-05,
      "loss": 0.1322,
      "step": 6740
    },
    {
      "epoch": 50.0,
      "grad_norm": 1.1198883056640625,
      "learning_rate": 1.0001481481481483e-05,
      "loss": 0.135,
      "step": 6750
    },
    {
      "epoch": 50.07434944237918,
      "grad_norm": 1.3352012634277344,
      "learning_rate": 9.986666666666667e-06,
      "loss": 0.146,
      "step": 6760
    },
    {
      "epoch": 50.14869888475837,
      "grad_norm": 0.9523757696151733,
      "learning_rate": 9.971851851851853e-06,
      "loss": 0.1327,
      "step": 6770
    },
    {
      "epoch": 50.22304832713755,
      "grad_norm": 0.7463290095329285,
      "learning_rate": 9.957037037037038e-06,
      "loss": 0.1234,
      "step": 6780
    },
    {
      "epoch": 50.29739776951673,
      "grad_norm": 0.9058478474617004,
      "learning_rate": 9.942222222222222e-06,
      "loss": 0.1387,
      "step": 6790
    },
    {
      "epoch": 50.37174721189591,
      "grad_norm": 0.9399113059043884,
      "learning_rate": 9.927407407407408e-06,
      "loss": 0.1592,
      "step": 6800
    },
    {
      "epoch": 50.446096654275095,
      "grad_norm": 0.853466808795929,
      "learning_rate": 9.912592592592594e-06,
      "loss": 0.1178,
      "step": 6810
    },
    {
      "epoch": 50.520446096654275,
      "grad_norm": 0.9450603723526001,
      "learning_rate": 9.89777777777778e-06,
      "loss": 0.1422,
      "step": 6820
    },
    {
      "epoch": 50.594795539033456,
      "grad_norm": 0.9845181107521057,
      "learning_rate": 9.882962962962965e-06,
      "loss": 0.1416,
      "step": 6830
    },
    {
      "epoch": 50.66914498141264,
      "grad_norm": 0.8653901815414429,
      "learning_rate": 9.868148148148149e-06,
      "loss": 0.1504,
      "step": 6840
    },
    {
      "epoch": 50.74349442379182,
      "grad_norm": 0.9522251486778259,
      "learning_rate": 9.853333333333334e-06,
      "loss": 0.1304,
      "step": 6850
    },
    {
      "epoch": 50.817843866171,
      "grad_norm": 0.863749086856842,
      "learning_rate": 9.83851851851852e-06,
      "loss": 0.139,
      "step": 6860
    },
    {
      "epoch": 50.89219330855018,
      "grad_norm": 0.6945528984069824,
      "learning_rate": 9.823703703703704e-06,
      "loss": 0.1308,
      "step": 6870
    },
    {
      "epoch": 50.96654275092937,
      "grad_norm": 0.8380102515220642,
      "learning_rate": 9.80888888888889e-06,
      "loss": 0.1381,
      "step": 6880
    },
    {
      "epoch": 51.037174721189594,
      "grad_norm": 0.9028263092041016,
      "learning_rate": 9.794074074074075e-06,
      "loss": 0.1475,
      "step": 6890
    },
    {
      "epoch": 51.111524163568774,
      "grad_norm": 1.1616730690002441,
      "learning_rate": 9.779259259259261e-06,
      "loss": 0.1401,
      "step": 6900
    },
    {
      "epoch": 51.185873605947954,
      "grad_norm": 1.03068208694458,
      "learning_rate": 9.764444444444445e-06,
      "loss": 0.1431,
      "step": 6910
    },
    {
      "epoch": 51.260223048327134,
      "grad_norm": 0.7302062511444092,
      "learning_rate": 9.74962962962963e-06,
      "loss": 0.1361,
      "step": 6920
    },
    {
      "epoch": 51.33457249070632,
      "grad_norm": 0.7835609316825867,
      "learning_rate": 9.734814814814816e-06,
      "loss": 0.121,
      "step": 6930
    },
    {
      "epoch": 51.4089219330855,
      "grad_norm": 0.9801380038261414,
      "learning_rate": 9.72e-06,
      "loss": 0.1314,
      "step": 6940
    },
    {
      "epoch": 51.48327137546468,
      "grad_norm": 1.118414044380188,
      "learning_rate": 9.705185185185186e-06,
      "loss": 0.1283,
      "step": 6950
    },
    {
      "epoch": 51.55762081784387,
      "grad_norm": 1.1875308752059937,
      "learning_rate": 9.690370370370371e-06,
      "loss": 0.1417,
      "step": 6960
    },
    {
      "epoch": 51.63197026022305,
      "grad_norm": 0.7686910629272461,
      "learning_rate": 9.675555555555555e-06,
      "loss": 0.1356,
      "step": 6970
    },
    {
      "epoch": 51.70631970260223,
      "grad_norm": 0.884617805480957,
      "learning_rate": 9.660740740740741e-06,
      "loss": 0.1379,
      "step": 6980
    },
    {
      "epoch": 51.78066914498141,
      "grad_norm": 0.9699535965919495,
      "learning_rate": 9.645925925925927e-06,
      "loss": 0.1347,
      "step": 6990
    },
    {
      "epoch": 51.8550185873606,
      "grad_norm": 0.8562527298927307,
      "learning_rate": 9.631111111111112e-06,
      "loss": 0.1491,
      "step": 7000
    },
    {
      "epoch": 51.92936802973978,
      "grad_norm": 1.112673044204712,
      "learning_rate": 9.616296296296296e-06,
      "loss": 0.1481,
      "step": 7010
    },
    {
      "epoch": 52.0,
      "grad_norm": 1.2927191257476807,
      "learning_rate": 9.601481481481484e-06,
      "loss": 0.1255,
      "step": 7020
    },
    {
      "epoch": 52.07434944237918,
      "grad_norm": 0.8117806911468506,
      "learning_rate": 9.586666666666667e-06,
      "loss": 0.1334,
      "step": 7030
    },
    {
      "epoch": 52.14869888475837,
      "grad_norm": 0.7779557704925537,
      "learning_rate": 9.571851851851851e-06,
      "loss": 0.1175,
      "step": 7040
    },
    {
      "epoch": 52.22304832713755,
      "grad_norm": 0.8453189730644226,
      "learning_rate": 9.557037037037039e-06,
      "loss": 0.14,
      "step": 7050
    },
    {
      "epoch": 52.29739776951673,
      "grad_norm": 1.1738003492355347,
      "learning_rate": 9.542222222222223e-06,
      "loss": 0.1215,
      "step": 7060
    },
    {
      "epoch": 52.37174721189591,
      "grad_norm": 0.866019606590271,
      "learning_rate": 9.527407407407408e-06,
      "loss": 0.1392,
      "step": 7070
    },
    {
      "epoch": 52.446096654275095,
      "grad_norm": 0.7839297652244568,
      "learning_rate": 9.512592592592594e-06,
      "loss": 0.1345,
      "step": 7080
    },
    {
      "epoch": 52.520446096654275,
      "grad_norm": 0.7452407479286194,
      "learning_rate": 9.49777777777778e-06,
      "loss": 0.1206,
      "step": 7090
    },
    {
      "epoch": 52.594795539033456,
      "grad_norm": 0.951769232749939,
      "learning_rate": 9.482962962962964e-06,
      "loss": 0.1425,
      "step": 7100
    },
    {
      "epoch": 52.66914498141264,
      "grad_norm": 1.087256908416748,
      "learning_rate": 9.46814814814815e-06,
      "loss": 0.1359,
      "step": 7110
    },
    {
      "epoch": 52.74349442379182,
      "grad_norm": 0.9588824510574341,
      "learning_rate": 9.453333333333335e-06,
      "loss": 0.1559,
      "step": 7120
    },
    {
      "epoch": 52.817843866171,
      "grad_norm": 1.0326780080795288,
      "learning_rate": 9.438518518518519e-06,
      "loss": 0.1393,
      "step": 7130
    },
    {
      "epoch": 52.89219330855018,
      "grad_norm": 1.1020870208740234,
      "learning_rate": 9.423703703703704e-06,
      "loss": 0.1419,
      "step": 7140
    },
    {
      "epoch": 52.96654275092937,
      "grad_norm": 0.9467802047729492,
      "learning_rate": 9.40888888888889e-06,
      "loss": 0.1532,
      "step": 7150
    },
    {
      "epoch": 53.037174721189594,
      "grad_norm": 0.8205514550209045,
      "learning_rate": 9.394074074074074e-06,
      "loss": 0.1564,
      "step": 7160
    },
    {
      "epoch": 53.111524163568774,
      "grad_norm": 0.9358595609664917,
      "learning_rate": 9.37925925925926e-06,
      "loss": 0.1349,
      "step": 7170
    },
    {
      "epoch": 53.185873605947954,
      "grad_norm": 0.9468976259231567,
      "learning_rate": 9.364444444444445e-06,
      "loss": 0.1355,
      "step": 7180
    },
    {
      "epoch": 53.260223048327134,
      "grad_norm": 0.9131446480751038,
      "learning_rate": 9.349629629629631e-06,
      "loss": 0.1328,
      "step": 7190
    },
    {
      "epoch": 53.33457249070632,
      "grad_norm": 0.9552294611930847,
      "learning_rate": 9.334814814814815e-06,
      "loss": 0.1459,
      "step": 7200
    },
    {
      "epoch": 53.4089219330855,
      "grad_norm": 0.756952702999115,
      "learning_rate": 9.32e-06,
      "loss": 0.1441,
      "step": 7210
    },
    {
      "epoch": 53.48327137546468,
      "grad_norm": 0.8491516709327698,
      "learning_rate": 9.305185185185186e-06,
      "loss": 0.1369,
      "step": 7220
    },
    {
      "epoch": 53.55762081784387,
      "grad_norm": 0.9348886609077454,
      "learning_rate": 9.29037037037037e-06,
      "loss": 0.1289,
      "step": 7230
    },
    {
      "epoch": 53.63197026022305,
      "grad_norm": 1.0103565454483032,
      "learning_rate": 9.275555555555556e-06,
      "loss": 0.1373,
      "step": 7240
    },
    {
      "epoch": 53.70631970260223,
      "grad_norm": 0.9236006736755371,
      "learning_rate": 9.260740740740741e-06,
      "loss": 0.1322,
      "step": 7250
    },
    {
      "epoch": 53.78066914498141,
      "grad_norm": 0.9401602149009705,
      "learning_rate": 9.245925925925927e-06,
      "loss": 0.1341,
      "step": 7260
    },
    {
      "epoch": 53.8550185873606,
      "grad_norm": 1.0549323558807373,
      "learning_rate": 9.231111111111111e-06,
      "loss": 0.1431,
      "step": 7270
    },
    {
      "epoch": 53.92936802973978,
      "grad_norm": 0.9007129073143005,
      "learning_rate": 9.216296296296297e-06,
      "loss": 0.1458,
      "step": 7280
    },
    {
      "epoch": 54.0,
      "grad_norm": 0.9745675921440125,
      "learning_rate": 9.201481481481482e-06,
      "loss": 0.128,
      "step": 7290
    },
    {
      "epoch": 54.07434944237918,
      "grad_norm": 0.938447117805481,
      "learning_rate": 9.186666666666666e-06,
      "loss": 0.1383,
      "step": 7300
    },
    {
      "epoch": 54.14869888475837,
      "grad_norm": 0.8086972236633301,
      "learning_rate": 9.171851851851854e-06,
      "loss": 0.1345,
      "step": 7310
    },
    {
      "epoch": 54.22304832713755,
      "grad_norm": 1.050424337387085,
      "learning_rate": 9.157037037037038e-06,
      "loss": 0.129,
      "step": 7320
    },
    {
      "epoch": 54.29739776951673,
      "grad_norm": 0.9582006931304932,
      "learning_rate": 9.142222222222223e-06,
      "loss": 0.1247,
      "step": 7330
    },
    {
      "epoch": 54.37174721189591,
      "grad_norm": 0.9521201252937317,
      "learning_rate": 9.127407407407409e-06,
      "loss": 0.1362,
      "step": 7340
    },
    {
      "epoch": 54.446096654275095,
      "grad_norm": 0.8336952328681946,
      "learning_rate": 9.112592592592593e-06,
      "loss": 0.1532,
      "step": 7350
    },
    {
      "epoch": 54.520446096654275,
      "grad_norm": 0.8822499513626099,
      "learning_rate": 9.097777777777778e-06,
      "loss": 0.1363,
      "step": 7360
    },
    {
      "epoch": 54.594795539033456,
      "grad_norm": 0.7878130078315735,
      "learning_rate": 9.082962962962964e-06,
      "loss": 0.1396,
      "step": 7370
    },
    {
      "epoch": 54.66914498141264,
      "grad_norm": 0.926338791847229,
      "learning_rate": 9.06814814814815e-06,
      "loss": 0.1314,
      "step": 7380
    },
    {
      "epoch": 54.74349442379182,
      "grad_norm": 0.8925914764404297,
      "learning_rate": 9.053333333333334e-06,
      "loss": 0.1399,
      "step": 7390
    },
    {
      "epoch": 54.817843866171,
      "grad_norm": 1.1654950380325317,
      "learning_rate": 9.03851851851852e-06,
      "loss": 0.1488,
      "step": 7400
    },
    {
      "epoch": 54.89219330855018,
      "grad_norm": 0.9221197962760925,
      "learning_rate": 9.023703703703705e-06,
      "loss": 0.1313,
      "step": 7410
    },
    {
      "epoch": 54.96654275092937,
      "grad_norm": 0.8245996236801147,
      "learning_rate": 9.008888888888889e-06,
      "loss": 0.1273,
      "step": 7420
    },
    {
      "epoch": 55.037174721189594,
      "grad_norm": 0.9584295153617859,
      "learning_rate": 8.994074074074074e-06,
      "loss": 0.1416,
      "step": 7430
    },
    {
      "epoch": 55.111524163568774,
      "grad_norm": 0.7857438325881958,
      "learning_rate": 8.98074074074074e-06,
      "loss": 0.1373,
      "step": 7440
    },
    {
      "epoch": 55.185873605947954,
      "grad_norm": 0.8994535207748413,
      "learning_rate": 8.965925925925926e-06,
      "loss": 0.1333,
      "step": 7450
    },
    {
      "epoch": 55.260223048327134,
      "grad_norm": 0.9591015577316284,
      "learning_rate": 8.951111111111112e-06,
      "loss": 0.1291,
      "step": 7460
    },
    {
      "epoch": 55.33457249070632,
      "grad_norm": 0.8168487548828125,
      "learning_rate": 8.936296296296298e-06,
      "loss": 0.1306,
      "step": 7470
    },
    {
      "epoch": 55.4089219330855,
      "grad_norm": 0.943463921546936,
      "learning_rate": 8.921481481481482e-06,
      "loss": 0.1418,
      "step": 7480
    },
    {
      "epoch": 55.48327137546468,
      "grad_norm": 0.9843823313713074,
      "learning_rate": 8.906666666666667e-06,
      "loss": 0.1403,
      "step": 7490
    },
    {
      "epoch": 55.55762081784387,
      "grad_norm": 0.9536477327346802,
      "learning_rate": 8.891851851851853e-06,
      "loss": 0.1235,
      "step": 7500
    },
    {
      "epoch": 55.63197026022305,
      "grad_norm": 0.7974286079406738,
      "learning_rate": 8.877037037037037e-06,
      "loss": 0.124,
      "step": 7510
    },
    {
      "epoch": 55.70631970260223,
      "grad_norm": 0.9886064529418945,
      "learning_rate": 8.862222222222222e-06,
      "loss": 0.1478,
      "step": 7520
    },
    {
      "epoch": 55.78066914498141,
      "grad_norm": 0.9048755764961243,
      "learning_rate": 8.847407407407408e-06,
      "loss": 0.1591,
      "step": 7530
    },
    {
      "epoch": 55.8550185873606,
      "grad_norm": 0.8760892152786255,
      "learning_rate": 8.832592592592594e-06,
      "loss": 0.1404,
      "step": 7540
    },
    {
      "epoch": 55.92936802973978,
      "grad_norm": 1.0525037050247192,
      "learning_rate": 8.817777777777778e-06,
      "loss": 0.126,
      "step": 7550
    },
    {
      "epoch": 56.0,
      "grad_norm": 1.2780561447143555,
      "learning_rate": 8.802962962962963e-06,
      "loss": 0.1379,
      "step": 7560
    },
    {
      "epoch": 56.07434944237918,
      "grad_norm": 1.1636890172958374,
      "learning_rate": 8.788148148148149e-06,
      "loss": 0.132,
      "step": 7570
    },
    {
      "epoch": 56.14869888475837,
      "grad_norm": 0.8459168076515198,
      "learning_rate": 8.773333333333333e-06,
      "loss": 0.1363,
      "step": 7580
    },
    {
      "epoch": 56.22304832713755,
      "grad_norm": 0.8546632528305054,
      "learning_rate": 8.75851851851852e-06,
      "loss": 0.1297,
      "step": 7590
    },
    {
      "epoch": 56.29739776951673,
      "grad_norm": 0.9568312764167786,
      "learning_rate": 8.743703703703704e-06,
      "loss": 0.1473,
      "step": 7600
    },
    {
      "epoch": 56.37174721189591,
      "grad_norm": 1.0676562786102295,
      "learning_rate": 8.72888888888889e-06,
      "loss": 0.137,
      "step": 7610
    },
    {
      "epoch": 56.446096654275095,
      "grad_norm": 0.9254785776138306,
      "learning_rate": 8.714074074074075e-06,
      "loss": 0.1438,
      "step": 7620
    },
    {
      "epoch": 56.520446096654275,
      "grad_norm": 0.7868849039077759,
      "learning_rate": 8.69925925925926e-06,
      "loss": 0.1427,
      "step": 7630
    },
    {
      "epoch": 56.594795539033456,
      "grad_norm": 0.7344621419906616,
      "learning_rate": 8.684444444444445e-06,
      "loss": 0.1282,
      "step": 7640
    },
    {
      "epoch": 56.66914498141264,
      "grad_norm": 0.8812679648399353,
      "learning_rate": 8.66962962962963e-06,
      "loss": 0.1361,
      "step": 7650
    },
    {
      "epoch": 56.74349442379182,
      "grad_norm": 0.7859197854995728,
      "learning_rate": 8.654814814814816e-06,
      "loss": 0.1283,
      "step": 7660
    },
    {
      "epoch": 56.817843866171,
      "grad_norm": 0.7410948276519775,
      "learning_rate": 8.64e-06,
      "loss": 0.1352,
      "step": 7670
    },
    {
      "epoch": 56.89219330855018,
      "grad_norm": 0.8855707049369812,
      "learning_rate": 8.625185185185186e-06,
      "loss": 0.1401,
      "step": 7680
    },
    {
      "epoch": 56.96654275092937,
      "grad_norm": 1.0516910552978516,
      "learning_rate": 8.610370370370372e-06,
      "loss": 0.1255,
      "step": 7690
    },
    {
      "epoch": 57.037174721189594,
      "grad_norm": 1.1467565298080444,
      "learning_rate": 8.595555555555556e-06,
      "loss": 0.1507,
      "step": 7700
    },
    {
      "epoch": 57.111524163568774,
      "grad_norm": 0.9090612530708313,
      "learning_rate": 8.580740740740741e-06,
      "loss": 0.1289,
      "step": 7710
    },
    {
      "epoch": 57.185873605947954,
      "grad_norm": 0.8554579019546509,
      "learning_rate": 8.565925925925927e-06,
      "loss": 0.1414,
      "step": 7720
    },
    {
      "epoch": 57.260223048327134,
      "grad_norm": 0.8708510994911194,
      "learning_rate": 8.551111111111112e-06,
      "loss": 0.1334,
      "step": 7730
    },
    {
      "epoch": 57.33457249070632,
      "grad_norm": 0.7250352501869202,
      "learning_rate": 8.536296296296296e-06,
      "loss": 0.1561,
      "step": 7740
    },
    {
      "epoch": 57.4089219330855,
      "grad_norm": 0.9724922776222229,
      "learning_rate": 8.521481481481482e-06,
      "loss": 0.1428,
      "step": 7750
    },
    {
      "epoch": 57.48327137546468,
      "grad_norm": 0.816588819026947,
      "learning_rate": 8.506666666666668e-06,
      "loss": 0.1325,
      "step": 7760
    },
    {
      "epoch": 57.55762081784387,
      "grad_norm": 0.7233825325965881,
      "learning_rate": 8.491851851851852e-06,
      "loss": 0.1119,
      "step": 7770
    },
    {
      "epoch": 57.63197026022305,
      "grad_norm": 1.1979000568389893,
      "learning_rate": 8.477037037037037e-06,
      "loss": 0.1219,
      "step": 7780
    },
    {
      "epoch": 57.70631970260223,
      "grad_norm": 0.8874994516372681,
      "learning_rate": 8.462222222222223e-06,
      "loss": 0.1507,
      "step": 7790
    },
    {
      "epoch": 57.78066914498141,
      "grad_norm": 0.8512725830078125,
      "learning_rate": 8.447407407407409e-06,
      "loss": 0.1445,
      "step": 7800
    },
    {
      "epoch": 57.8550185873606,
      "grad_norm": 0.9447998404502869,
      "learning_rate": 8.432592592592594e-06,
      "loss": 0.1413,
      "step": 7810
    },
    {
      "epoch": 57.92936802973978,
      "grad_norm": 0.8087071180343628,
      "learning_rate": 8.417777777777778e-06,
      "loss": 0.125,
      "step": 7820
    },
    {
      "epoch": 58.0,
      "grad_norm": 1.0741773843765259,
      "learning_rate": 8.402962962962964e-06,
      "loss": 0.1288,
      "step": 7830
    },
    {
      "epoch": 58.07434944237918,
      "grad_norm": 0.9994354844093323,
      "learning_rate": 8.38814814814815e-06,
      "loss": 0.1321,
      "step": 7840
    },
    {
      "epoch": 58.14869888475837,
      "grad_norm": 1.1213293075561523,
      "learning_rate": 8.373333333333335e-06,
      "loss": 0.1198,
      "step": 7850
    },
    {
      "epoch": 58.22304832713755,
      "grad_norm": 0.9883849024772644,
      "learning_rate": 8.358518518518519e-06,
      "loss": 0.1455,
      "step": 7860
    },
    {
      "epoch": 58.29739776951673,
      "grad_norm": 0.8441290259361267,
      "learning_rate": 8.343703703703705e-06,
      "loss": 0.119,
      "step": 7870
    },
    {
      "epoch": 58.37174721189591,
      "grad_norm": 0.9662801623344421,
      "learning_rate": 8.32888888888889e-06,
      "loss": 0.1294,
      "step": 7880
    },
    {
      "epoch": 58.446096654275095,
      "grad_norm": 0.9543864727020264,
      "learning_rate": 8.314074074074074e-06,
      "loss": 0.1365,
      "step": 7890
    },
    {
      "epoch": 58.520446096654275,
      "grad_norm": 0.9623522758483887,
      "learning_rate": 8.29925925925926e-06,
      "loss": 0.1354,
      "step": 7900
    },
    {
      "epoch": 58.594795539033456,
      "grad_norm": 1.112492561340332,
      "learning_rate": 8.284444444444446e-06,
      "loss": 0.1464,
      "step": 7910
    },
    {
      "epoch": 58.66914498141264,
      "grad_norm": 1.1369909048080444,
      "learning_rate": 8.269629629629631e-06,
      "loss": 0.1376,
      "step": 7920
    },
    {
      "epoch": 58.74349442379182,
      "grad_norm": 1.0095837116241455,
      "learning_rate": 8.254814814814815e-06,
      "loss": 0.1343,
      "step": 7930
    },
    {
      "epoch": 58.817843866171,
      "grad_norm": 0.685044527053833,
      "learning_rate": 8.24e-06,
      "loss": 0.1493,
      "step": 7940
    },
    {
      "epoch": 58.89219330855018,
      "grad_norm": 0.7712293267250061,
      "learning_rate": 8.225185185185186e-06,
      "loss": 0.1347,
      "step": 7950
    },
    {
      "epoch": 58.96654275092937,
      "grad_norm": 0.870575487613678,
      "learning_rate": 8.21037037037037e-06,
      "loss": 0.1439,
      "step": 7960
    },
    {
      "epoch": 59.037174721189594,
      "grad_norm": 1.1007084846496582,
      "learning_rate": 8.195555555555556e-06,
      "loss": 0.1361,
      "step": 7970
    },
    {
      "epoch": 59.111524163568774,
      "grad_norm": 0.933307945728302,
      "learning_rate": 8.180740740740742e-06,
      "loss": 0.1211,
      "step": 7980
    },
    {
      "epoch": 59.185873605947954,
      "grad_norm": 1.301749348640442,
      "learning_rate": 8.165925925925927e-06,
      "loss": 0.1315,
      "step": 7990
    },
    {
      "epoch": 59.260223048327134,
      "grad_norm": 0.7696842551231384,
      "learning_rate": 8.151111111111111e-06,
      "loss": 0.1353,
      "step": 8000
    },
    {
      "epoch": 59.33457249070632,
      "grad_norm": 0.8837804794311523,
      "learning_rate": 8.136296296296297e-06,
      "loss": 0.1362,
      "step": 8010
    },
    {
      "epoch": 59.4089219330855,
      "grad_norm": 0.8614231944084167,
      "learning_rate": 8.121481481481482e-06,
      "loss": 0.1384,
      "step": 8020
    },
    {
      "epoch": 59.48327137546468,
      "grad_norm": 0.9351144433021545,
      "learning_rate": 8.106666666666666e-06,
      "loss": 0.1418,
      "step": 8030
    },
    {
      "epoch": 59.55762081784387,
      "grad_norm": 0.9790388941764832,
      "learning_rate": 8.091851851851854e-06,
      "loss": 0.1417,
      "step": 8040
    },
    {
      "epoch": 59.63197026022305,
      "grad_norm": 1.1763379573822021,
      "learning_rate": 8.077037037037038e-06,
      "loss": 0.1422,
      "step": 8050
    },
    {
      "epoch": 59.70631970260223,
      "grad_norm": 0.7816461324691772,
      "learning_rate": 8.062222222222222e-06,
      "loss": 0.1259,
      "step": 8060
    },
    {
      "epoch": 59.78066914498141,
      "grad_norm": 0.9276143908500671,
      "learning_rate": 8.047407407407409e-06,
      "loss": 0.1387,
      "step": 8070
    },
    {
      "epoch": 59.8550185873606,
      "grad_norm": 1.0775407552719116,
      "learning_rate": 8.032592592592593e-06,
      "loss": 0.1481,
      "step": 8080
    },
    {
      "epoch": 59.92936802973978,
      "grad_norm": 0.7976852655410767,
      "learning_rate": 8.017777777777779e-06,
      "loss": 0.1112,
      "step": 8090
    },
    {
      "epoch": 60.0,
      "grad_norm": 1.4647890329360962,
      "learning_rate": 8.002962962962964e-06,
      "loss": 0.1593,
      "step": 8100
    },
    {
      "epoch": 60.07434944237918,
      "grad_norm": 0.8014658689498901,
      "learning_rate": 7.98814814814815e-06,
      "loss": 0.1205,
      "step": 8110
    },
    {
      "epoch": 60.14869888475837,
      "grad_norm": 0.9570603370666504,
      "learning_rate": 7.973333333333334e-06,
      "loss": 0.1245,
      "step": 8120
    },
    {
      "epoch": 60.22304832713755,
      "grad_norm": 0.8611772656440735,
      "learning_rate": 7.95851851851852e-06,
      "loss": 0.1352,
      "step": 8130
    },
    {
      "epoch": 60.29739776951673,
      "grad_norm": 0.9261406064033508,
      "learning_rate": 7.943703703703705e-06,
      "loss": 0.1611,
      "step": 8140
    },
    {
      "epoch": 60.37174721189591,
      "grad_norm": 1.0465322732925415,
      "learning_rate": 7.928888888888889e-06,
      "loss": 0.1259,
      "step": 8150
    },
    {
      "epoch": 60.446096654275095,
      "grad_norm": 0.9165458679199219,
      "learning_rate": 7.914074074074075e-06,
      "loss": 0.1485,
      "step": 8160
    },
    {
      "epoch": 60.520446096654275,
      "grad_norm": 1.010022759437561,
      "learning_rate": 7.89925925925926e-06,
      "loss": 0.1438,
      "step": 8170
    },
    {
      "epoch": 60.594795539033456,
      "grad_norm": 0.9888521432876587,
      "learning_rate": 7.884444444444444e-06,
      "loss": 0.1498,
      "step": 8180
    },
    {
      "epoch": 60.66914498141264,
      "grad_norm": 0.6987169981002808,
      "learning_rate": 7.86962962962963e-06,
      "loss": 0.1365,
      "step": 8190
    },
    {
      "epoch": 60.74349442379182,
      "grad_norm": 0.9065334796905518,
      "learning_rate": 7.854814814814816e-06,
      "loss": 0.1152,
      "step": 8200
    },
    {
      "epoch": 60.817843866171,
      "grad_norm": 1.0105297565460205,
      "learning_rate": 7.840000000000001e-06,
      "loss": 0.121,
      "step": 8210
    },
    {
      "epoch": 60.89219330855018,
      "grad_norm": 0.9071646928787231,
      "learning_rate": 7.825185185185185e-06,
      "loss": 0.1287,
      "step": 8220
    },
    {
      "epoch": 60.96654275092937,
      "grad_norm": 1.1994404792785645,
      "learning_rate": 7.81037037037037e-06,
      "loss": 0.141,
      "step": 8230
    },
    {
      "epoch": 61.037174721189594,
      "grad_norm": 1.2886689901351929,
      "learning_rate": 7.795555555555556e-06,
      "loss": 0.1329,
      "step": 8240
    },
    {
      "epoch": 61.111524163568774,
      "grad_norm": 0.8769834637641907,
      "learning_rate": 7.78074074074074e-06,
      "loss": 0.1251,
      "step": 8250
    },
    {
      "epoch": 61.185873605947954,
      "grad_norm": 0.8556068539619446,
      "learning_rate": 7.765925925925926e-06,
      "loss": 0.1283,
      "step": 8260
    },
    {
      "epoch": 61.260223048327134,
      "grad_norm": 0.9277496933937073,
      "learning_rate": 7.751111111111112e-06,
      "loss": 0.1373,
      "step": 8270
    },
    {
      "epoch": 61.33457249070632,
      "grad_norm": 1.0180792808532715,
      "learning_rate": 7.736296296296297e-06,
      "loss": 0.1414,
      "step": 8280
    },
    {
      "epoch": 61.4089219330855,
      "grad_norm": 1.0400068759918213,
      "learning_rate": 7.721481481481481e-06,
      "loss": 0.1292,
      "step": 8290
    },
    {
      "epoch": 61.48327137546468,
      "grad_norm": 0.7998511791229248,
      "learning_rate": 7.706666666666669e-06,
      "loss": 0.1384,
      "step": 8300
    },
    {
      "epoch": 61.55762081784387,
      "grad_norm": 0.988477885723114,
      "learning_rate": 7.691851851851852e-06,
      "loss": 0.1274,
      "step": 8310
    },
    {
      "epoch": 61.63197026022305,
      "grad_norm": 1.3821049928665161,
      "learning_rate": 7.677037037037036e-06,
      "loss": 0.1295,
      "step": 8320
    },
    {
      "epoch": 61.70631970260223,
      "grad_norm": 0.9056165218353271,
      "learning_rate": 7.662222222222224e-06,
      "loss": 0.1351,
      "step": 8330
    },
    {
      "epoch": 61.78066914498141,
      "grad_norm": 1.0947041511535645,
      "learning_rate": 7.647407407407408e-06,
      "loss": 0.1435,
      "step": 8340
    },
    {
      "epoch": 61.8550185873606,
      "grad_norm": 0.8097559809684753,
      "learning_rate": 7.632592592592593e-06,
      "loss": 0.1473,
      "step": 8350
    },
    {
      "epoch": 61.92936802973978,
      "grad_norm": 0.9489088654518127,
      "learning_rate": 7.617777777777778e-06,
      "loss": 0.1336,
      "step": 8360
    },
    {
      "epoch": 62.0,
      "grad_norm": 1.381860375404358,
      "learning_rate": 7.602962962962963e-06,
      "loss": 0.1481,
      "step": 8370
    },
    {
      "epoch": 62.07434944237918,
      "grad_norm": 1.1304479837417603,
      "learning_rate": 7.588148148148149e-06,
      "loss": 0.1408,
      "step": 8380
    },
    {
      "epoch": 62.14869888475837,
      "grad_norm": 0.7963417172431946,
      "learning_rate": 7.573333333333333e-06,
      "loss": 0.1272,
      "step": 8390
    },
    {
      "epoch": 62.22304832713755,
      "grad_norm": 0.9765464067459106,
      "learning_rate": 7.55851851851852e-06,
      "loss": 0.1371,
      "step": 8400
    },
    {
      "epoch": 62.29739776951673,
      "grad_norm": 1.0162264108657837,
      "learning_rate": 7.543703703703704e-06,
      "loss": 0.1454,
      "step": 8410
    },
    {
      "epoch": 62.37174721189591,
      "grad_norm": 1.138289213180542,
      "learning_rate": 7.52888888888889e-06,
      "loss": 0.1334,
      "step": 8420
    },
    {
      "epoch": 62.446096654275095,
      "grad_norm": 1.0641546249389648,
      "learning_rate": 7.514074074074075e-06,
      "loss": 0.1341,
      "step": 8430
    },
    {
      "epoch": 62.520446096654275,
      "grad_norm": 1.101943016052246,
      "learning_rate": 7.499259259259259e-06,
      "loss": 0.1332,
      "step": 8440
    },
    {
      "epoch": 62.594795539033456,
      "grad_norm": 0.995248556137085,
      "learning_rate": 7.4844444444444455e-06,
      "loss": 0.134,
      "step": 8450
    },
    {
      "epoch": 62.66914498141264,
      "grad_norm": 0.7773012518882751,
      "learning_rate": 7.46962962962963e-06,
      "loss": 0.1187,
      "step": 8460
    },
    {
      "epoch": 62.74349442379182,
      "grad_norm": 1.182884931564331,
      "learning_rate": 7.454814814814816e-06,
      "loss": 0.154,
      "step": 8470
    },
    {
      "epoch": 62.817843866171,
      "grad_norm": 1.180118441581726,
      "learning_rate": 7.440000000000001e-06,
      "loss": 0.1262,
      "step": 8480
    },
    {
      "epoch": 62.89219330855018,
      "grad_norm": 0.9316302537918091,
      "learning_rate": 7.4251851851851856e-06,
      "loss": 0.1174,
      "step": 8490
    },
    {
      "epoch": 62.96654275092937,
      "grad_norm": 1.2594478130340576,
      "learning_rate": 7.410370370370371e-06,
      "loss": 0.1484,
      "step": 8500
    },
    {
      "epoch": 63.037174721189594,
      "grad_norm": 1.1282460689544678,
      "learning_rate": 7.395555555555556e-06,
      "loss": 0.1469,
      "step": 8510
    },
    {
      "epoch": 63.111524163568774,
      "grad_norm": 0.8639364838600159,
      "learning_rate": 7.380740740740742e-06,
      "loss": 0.149,
      "step": 8520
    },
    {
      "epoch": 63.185873605947954,
      "grad_norm": 1.015425443649292,
      "learning_rate": 7.3659259259259264e-06,
      "loss": 0.1355,
      "step": 8530
    },
    {
      "epoch": 63.260223048327134,
      "grad_norm": 1.1128169298171997,
      "learning_rate": 7.351111111111112e-06,
      "loss": 0.1266,
      "step": 8540
    },
    {
      "epoch": 63.33457249070632,
      "grad_norm": 0.9177049994468689,
      "learning_rate": 7.336296296296297e-06,
      "loss": 0.1379,
      "step": 8550
    },
    {
      "epoch": 63.4089219330855,
      "grad_norm": 1.2514958381652832,
      "learning_rate": 7.321481481481482e-06,
      "loss": 0.1412,
      "step": 8560
    },
    {
      "epoch": 63.48327137546468,
      "grad_norm": 1.0661303997039795,
      "learning_rate": 7.306666666666667e-06,
      "loss": 0.1468,
      "step": 8570
    },
    {
      "epoch": 63.55762081784387,
      "grad_norm": 1.072199821472168,
      "learning_rate": 7.291851851851852e-06,
      "loss": 0.1344,
      "step": 8580
    },
    {
      "epoch": 63.63197026022305,
      "grad_norm": 1.1856422424316406,
      "learning_rate": 7.277037037037038e-06,
      "loss": 0.1258,
      "step": 8590
    },
    {
      "epoch": 63.70631970260223,
      "grad_norm": 0.9614274501800537,
      "learning_rate": 7.2622222222222225e-06,
      "loss": 0.1184,
      "step": 8600
    },
    {
      "epoch": 63.78066914498141,
      "grad_norm": 0.9101842045783997,
      "learning_rate": 7.247407407407407e-06,
      "loss": 0.1445,
      "step": 8610
    },
    {
      "epoch": 63.8550185873606,
      "grad_norm": 0.7890176773071289,
      "learning_rate": 7.232592592592593e-06,
      "loss": 0.131,
      "step": 8620
    },
    {
      "epoch": 63.92936802973978,
      "grad_norm": 1.02229642868042,
      "learning_rate": 7.217777777777778e-06,
      "loss": 0.1255,
      "step": 8630
    },
    {
      "epoch": 64.0,
      "grad_norm": 1.1837830543518066,
      "learning_rate": 7.202962962962964e-06,
      "loss": 0.1323,
      "step": 8640
    },
    {
      "epoch": 64.07434944237919,
      "grad_norm": 0.7761862874031067,
      "learning_rate": 7.188148148148148e-06,
      "loss": 0.1297,
      "step": 8650
    },
    {
      "epoch": 64.14869888475836,
      "grad_norm": 0.8848767876625061,
      "learning_rate": 7.173333333333335e-06,
      "loss": 0.1368,
      "step": 8660
    },
    {
      "epoch": 64.22304832713755,
      "grad_norm": 1.32651948928833,
      "learning_rate": 7.1585185185185195e-06,
      "loss": 0.134,
      "step": 8670
    },
    {
      "epoch": 64.29739776951673,
      "grad_norm": 0.8443943858146667,
      "learning_rate": 7.143703703703703e-06,
      "loss": 0.1375,
      "step": 8680
    },
    {
      "epoch": 64.37174721189591,
      "grad_norm": 1.0567631721496582,
      "learning_rate": 7.12888888888889e-06,
      "loss": 0.1496,
      "step": 8690
    },
    {
      "epoch": 64.4460966542751,
      "grad_norm": 0.8565518856048584,
      "learning_rate": 7.114074074074075e-06,
      "loss": 0.1327,
      "step": 8700
    },
    {
      "epoch": 64.52044609665427,
      "grad_norm": 1.0329868793487549,
      "learning_rate": 7.09925925925926e-06,
      "loss": 0.1254,
      "step": 8710
    },
    {
      "epoch": 64.59479553903346,
      "grad_norm": 0.9739720821380615,
      "learning_rate": 7.084444444444445e-06,
      "loss": 0.1315,
      "step": 8720
    },
    {
      "epoch": 64.66914498141264,
      "grad_norm": 1.0985238552093506,
      "learning_rate": 7.069629629629631e-06,
      "loss": 0.1416,
      "step": 8730
    },
    {
      "epoch": 64.74349442379182,
      "grad_norm": 0.9758565425872803,
      "learning_rate": 7.0548148148148156e-06,
      "loss": 0.1294,
      "step": 8740
    },
    {
      "epoch": 64.817843866171,
      "grad_norm": 1.025909662246704,
      "learning_rate": 7.04e-06,
      "loss": 0.1312,
      "step": 8750
    },
    {
      "epoch": 64.89219330855019,
      "grad_norm": 0.7616497874259949,
      "learning_rate": 7.025185185185186e-06,
      "loss": 0.1415,
      "step": 8760
    },
    {
      "epoch": 64.96654275092936,
      "grad_norm": 1.2052617073059082,
      "learning_rate": 7.010370370370371e-06,
      "loss": 0.1392,
      "step": 8770
    },
    {
      "epoch": 65.03717472118959,
      "grad_norm": 0.946357786655426,
      "learning_rate": 6.9955555555555564e-06,
      "loss": 0.1226,
      "step": 8780
    },
    {
      "epoch": 65.11152416356877,
      "grad_norm": 0.7743598818778992,
      "learning_rate": 6.980740740740741e-06,
      "loss": 0.1373,
      "step": 8790
    },
    {
      "epoch": 65.18587360594796,
      "grad_norm": 1.1915245056152344,
      "learning_rate": 6.965925925925926e-06,
      "loss": 0.1463,
      "step": 8800
    },
    {
      "epoch": 65.26022304832713,
      "grad_norm": 0.9600020051002502,
      "learning_rate": 6.951111111111112e-06,
      "loss": 0.1532,
      "step": 8810
    },
    {
      "epoch": 65.33457249070632,
      "grad_norm": 0.9414460062980652,
      "learning_rate": 6.9362962962962965e-06,
      "loss": 0.1202,
      "step": 8820
    },
    {
      "epoch": 65.40892193308551,
      "grad_norm": 0.8262110948562622,
      "learning_rate": 6.921481481481482e-06,
      "loss": 0.1305,
      "step": 8830
    },
    {
      "epoch": 65.48327137546468,
      "grad_norm": 0.9993442296981812,
      "learning_rate": 6.906666666666667e-06,
      "loss": 0.1362,
      "step": 8840
    },
    {
      "epoch": 65.55762081784387,
      "grad_norm": 1.0756508111953735,
      "learning_rate": 6.8918518518518525e-06,
      "loss": 0.1241,
      "step": 8850
    },
    {
      "epoch": 65.63197026022304,
      "grad_norm": 0.8834173679351807,
      "learning_rate": 6.877037037037037e-06,
      "loss": 0.1227,
      "step": 8860
    },
    {
      "epoch": 65.70631970260223,
      "grad_norm": 1.2517067193984985,
      "learning_rate": 6.862222222222222e-06,
      "loss": 0.1367,
      "step": 8870
    },
    {
      "epoch": 65.78066914498142,
      "grad_norm": 1.0424398183822632,
      "learning_rate": 6.847407407407408e-06,
      "loss": 0.1413,
      "step": 8880
    },
    {
      "epoch": 65.85501858736059,
      "grad_norm": 0.9208388328552246,
      "learning_rate": 6.8325925925925926e-06,
      "loss": 0.1298,
      "step": 8890
    },
    {
      "epoch": 65.92936802973978,
      "grad_norm": 0.9652020931243896,
      "learning_rate": 6.817777777777779e-06,
      "loss": 0.1317,
      "step": 8900
    },
    {
      "epoch": 66.0,
      "grad_norm": 0.9967907667160034,
      "learning_rate": 6.802962962962964e-06,
      "loss": 0.1281,
      "step": 8910
    },
    {
      "epoch": 66.07434944237919,
      "grad_norm": 0.7758052349090576,
      "learning_rate": 6.788148148148148e-06,
      "loss": 0.1252,
      "step": 8920
    },
    {
      "epoch": 66.14869888475836,
      "grad_norm": 0.9959761500358582,
      "learning_rate": 6.773333333333334e-06,
      "loss": 0.1387,
      "step": 8930
    },
    {
      "epoch": 66.22304832713755,
      "grad_norm": 0.9409059286117554,
      "learning_rate": 6.758518518518519e-06,
      "loss": 0.1285,
      "step": 8940
    },
    {
      "epoch": 66.29739776951673,
      "grad_norm": 0.8535480499267578,
      "learning_rate": 6.743703703703705e-06,
      "loss": 0.1343,
      "step": 8950
    },
    {
      "epoch": 66.37174721189591,
      "grad_norm": 1.4685697555541992,
      "learning_rate": 6.7288888888888895e-06,
      "loss": 0.1338,
      "step": 8960
    },
    {
      "epoch": 66.4460966542751,
      "grad_norm": 0.7724242806434631,
      "learning_rate": 6.714074074074075e-06,
      "loss": 0.1509,
      "step": 8970
    },
    {
      "epoch": 66.52044609665427,
      "grad_norm": 0.9362857341766357,
      "learning_rate": 6.69925925925926e-06,
      "loss": 0.158,
      "step": 8980
    },
    {
      "epoch": 66.59479553903346,
      "grad_norm": 1.9659695625305176,
      "learning_rate": 6.684444444444445e-06,
      "loss": 0.1222,
      "step": 8990
    },
    {
      "epoch": 66.66914498141264,
      "grad_norm": 1.051537036895752,
      "learning_rate": 6.66962962962963e-06,
      "loss": 0.1235,
      "step": 9000
    },
    {
      "epoch": 66.74349442379182,
      "grad_norm": 0.8072078824043274,
      "learning_rate": 6.654814814814815e-06,
      "loss": 0.1199,
      "step": 9010
    },
    {
      "epoch": 66.817843866171,
      "grad_norm": 1.11944580078125,
      "learning_rate": 6.640000000000001e-06,
      "loss": 0.1431,
      "step": 9020
    },
    {
      "epoch": 66.89219330855019,
      "grad_norm": 0.8205037117004395,
      "learning_rate": 6.625185185185186e-06,
      "loss": 0.1387,
      "step": 9030
    },
    {
      "epoch": 66.96654275092936,
      "grad_norm": 0.9353682398796082,
      "learning_rate": 6.610370370370371e-06,
      "loss": 0.1256,
      "step": 9040
    },
    {
      "epoch": 67.03717472118959,
      "grad_norm": 1.0163203477859497,
      "learning_rate": 6.595555555555556e-06,
      "loss": 0.1342,
      "step": 9050
    },
    {
      "epoch": 67.11152416356877,
      "grad_norm": 1.1552540063858032,
      "learning_rate": 6.580740740740741e-06,
      "loss": 0.1328,
      "step": 9060
    },
    {
      "epoch": 67.18587360594796,
      "grad_norm": 0.954127311706543,
      "learning_rate": 6.5659259259259265e-06,
      "loss": 0.1218,
      "step": 9070
    },
    {
      "epoch": 67.26022304832713,
      "grad_norm": 0.978035569190979,
      "learning_rate": 6.551111111111111e-06,
      "loss": 0.1288,
      "step": 9080
    },
    {
      "epoch": 67.33457249070632,
      "grad_norm": 0.7612384557723999,
      "learning_rate": 6.536296296296297e-06,
      "loss": 0.1506,
      "step": 9090
    },
    {
      "epoch": 67.40892193308551,
      "grad_norm": 0.894459068775177,
      "learning_rate": 6.521481481481482e-06,
      "loss": 0.1241,
      "step": 9100
    },
    {
      "epoch": 67.48327137546468,
      "grad_norm": 0.8200941681861877,
      "learning_rate": 6.5066666666666665e-06,
      "loss": 0.1346,
      "step": 9110
    },
    {
      "epoch": 67.55762081784387,
      "grad_norm": 1.0807313919067383,
      "learning_rate": 6.491851851851852e-06,
      "loss": 0.1207,
      "step": 9120
    },
    {
      "epoch": 67.63197026022304,
      "grad_norm": 0.8641814589500427,
      "learning_rate": 6.477037037037037e-06,
      "loss": 0.136,
      "step": 9130
    },
    {
      "epoch": 67.70631970260223,
      "grad_norm": 1.1546831130981445,
      "learning_rate": 6.462222222222223e-06,
      "loss": 0.1499,
      "step": 9140
    },
    {
      "epoch": 67.78066914498142,
      "grad_norm": 1.349233865737915,
      "learning_rate": 6.447407407407408e-06,
      "loss": 0.1346,
      "step": 9150
    },
    {
      "epoch": 67.85501858736059,
      "grad_norm": 1.0393180847167969,
      "learning_rate": 6.432592592592594e-06,
      "loss": 0.1374,
      "step": 9160
    },
    {
      "epoch": 67.92936802973978,
      "grad_norm": 1.0962638854980469,
      "learning_rate": 6.417777777777779e-06,
      "loss": 0.127,
      "step": 9170
    },
    {
      "epoch": 68.0,
      "grad_norm": 1.1359755992889404,
      "learning_rate": 6.4029629629629634e-06,
      "loss": 0.1372,
      "step": 9180
    },
    {
      "epoch": 68.07434944237919,
      "grad_norm": 1.017836332321167,
      "learning_rate": 6.388148148148149e-06,
      "loss": 0.1357,
      "step": 9190
    },
    {
      "epoch": 68.14869888475836,
      "grad_norm": 1.0454665422439575,
      "learning_rate": 6.373333333333334e-06,
      "loss": 0.1557,
      "step": 9200
    },
    {
      "epoch": 68.22304832713755,
      "grad_norm": 0.927363932132721,
      "learning_rate": 6.3585185185185195e-06,
      "loss": 0.1478,
      "step": 9210
    },
    {
      "epoch": 68.29739776951673,
      "grad_norm": 0.8776764869689941,
      "learning_rate": 6.343703703703704e-06,
      "loss": 0.1329,
      "step": 9220
    },
    {
      "epoch": 68.37174721189591,
      "grad_norm": 1.1376628875732422,
      "learning_rate": 6.328888888888889e-06,
      "loss": 0.1256,
      "step": 9230
    },
    {
      "epoch": 68.4460966542751,
      "grad_norm": 0.9308984875679016,
      "learning_rate": 6.314074074074075e-06,
      "loss": 0.1327,
      "step": 9240
    },
    {
      "epoch": 68.52044609665427,
      "grad_norm": 1.1032627820968628,
      "learning_rate": 6.2992592592592595e-06,
      "loss": 0.1348,
      "step": 9250
    },
    {
      "epoch": 68.59479553903346,
      "grad_norm": 1.0186938047409058,
      "learning_rate": 6.284444444444445e-06,
      "loss": 0.1254,
      "step": 9260
    },
    {
      "epoch": 68.66914498141264,
      "grad_norm": 0.8522061109542847,
      "learning_rate": 6.26962962962963e-06,
      "loss": 0.1353,
      "step": 9270
    },
    {
      "epoch": 68.74349442379182,
      "grad_norm": 0.8836902976036072,
      "learning_rate": 6.254814814814816e-06,
      "loss": 0.1252,
      "step": 9280
    },
    {
      "epoch": 68.817843866171,
      "grad_norm": 1.199834942817688,
      "learning_rate": 6.24e-06,
      "loss": 0.1249,
      "step": 9290
    },
    {
      "epoch": 68.89219330855019,
      "grad_norm": 0.851369321346283,
      "learning_rate": 6.225185185185185e-06,
      "loss": 0.1361,
      "step": 9300
    },
    {
      "epoch": 68.96654275092936,
      "grad_norm": 0.8829525709152222,
      "learning_rate": 6.210370370370371e-06,
      "loss": 0.1274,
      "step": 9310
    },
    {
      "epoch": 69.03717472118959,
      "grad_norm": 0.8263338804244995,
      "learning_rate": 6.195555555555556e-06,
      "loss": 0.1454,
      "step": 9320
    },
    {
      "epoch": 69.11152416356877,
      "grad_norm": 0.9754106998443604,
      "learning_rate": 6.180740740740741e-06,
      "loss": 0.138,
      "step": 9330
    },
    {
      "epoch": 69.18587360594796,
      "grad_norm": 0.8180338740348816,
      "learning_rate": 6.165925925925926e-06,
      "loss": 0.1422,
      "step": 9340
    },
    {
      "epoch": 69.26022304832713,
      "grad_norm": 1.0573887825012207,
      "learning_rate": 6.1511111111111125e-06,
      "loss": 0.1362,
      "step": 9350
    },
    {
      "epoch": 69.33457249070632,
      "grad_norm": 1.2003453969955444,
      "learning_rate": 6.1362962962962965e-06,
      "loss": 0.1346,
      "step": 9360
    },
    {
      "epoch": 69.40892193308551,
      "grad_norm": 1.0318857431411743,
      "learning_rate": 6.121481481481481e-06,
      "loss": 0.1246,
      "step": 9370
    },
    {
      "epoch": 69.48327137546468,
      "grad_norm": 0.8370306491851807,
      "learning_rate": 6.106666666666668e-06,
      "loss": 0.1258,
      "step": 9380
    },
    {
      "epoch": 69.55762081784387,
      "grad_norm": 1.029656171798706,
      "learning_rate": 6.091851851851852e-06,
      "loss": 0.1202,
      "step": 9390
    },
    {
      "epoch": 69.63197026022304,
      "grad_norm": 0.927960216999054,
      "learning_rate": 6.077037037037038e-06,
      "loss": 0.1227,
      "step": 9400
    },
    {
      "epoch": 69.70631970260223,
      "grad_norm": 0.959685206413269,
      "learning_rate": 6.062222222222223e-06,
      "loss": 0.1221,
      "step": 9410
    },
    {
      "epoch": 69.78066914498142,
      "grad_norm": 0.8494856953620911,
      "learning_rate": 6.047407407407408e-06,
      "loss": 0.1182,
      "step": 9420
    },
    {
      "epoch": 69.85501858736059,
      "grad_norm": 0.8887231349945068,
      "learning_rate": 6.0325925925925934e-06,
      "loss": 0.1488,
      "step": 9430
    },
    {
      "epoch": 69.92936802973978,
      "grad_norm": 0.7621200084686279,
      "learning_rate": 6.017777777777778e-06,
      "loss": 0.1519,
      "step": 9440
    },
    {
      "epoch": 70.0,
      "grad_norm": 1.4550621509552002,
      "learning_rate": 6.002962962962964e-06,
      "loss": 0.1377,
      "step": 9450
    },
    {
      "epoch": 70.07434944237919,
      "grad_norm": 0.9547573328018188,
      "learning_rate": 5.988148148148149e-06,
      "loss": 0.129,
      "step": 9460
    },
    {
      "epoch": 70.14869888475836,
      "grad_norm": 0.8644211888313293,
      "learning_rate": 5.973333333333334e-06,
      "loss": 0.1285,
      "step": 9470
    },
    {
      "epoch": 70.22304832713755,
      "grad_norm": 0.9329627156257629,
      "learning_rate": 5.958518518518519e-06,
      "loss": 0.1366,
      "step": 9480
    },
    {
      "epoch": 70.29739776951673,
      "grad_norm": 1.6598942279815674,
      "learning_rate": 5.943703703703704e-06,
      "loss": 0.1241,
      "step": 9490
    },
    {
      "epoch": 70.37174721189591,
      "grad_norm": 0.8745046257972717,
      "learning_rate": 5.9288888888888895e-06,
      "loss": 0.1323,
      "step": 9500
    },
    {
      "epoch": 70.4460966542751,
      "grad_norm": 1.1805603504180908,
      "learning_rate": 5.914074074074074e-06,
      "loss": 0.1289,
      "step": 9510
    },
    {
      "epoch": 70.52044609665427,
      "grad_norm": 1.0271834135055542,
      "learning_rate": 5.89925925925926e-06,
      "loss": 0.1204,
      "step": 9520
    },
    {
      "epoch": 70.59479553903346,
      "grad_norm": 1.227020025253296,
      "learning_rate": 5.884444444444445e-06,
      "loss": 0.1472,
      "step": 9530
    },
    {
      "epoch": 70.66914498141264,
      "grad_norm": 0.8704546093940735,
      "learning_rate": 5.8696296296296296e-06,
      "loss": 0.1371,
      "step": 9540
    },
    {
      "epoch": 70.74349442379182,
      "grad_norm": 0.8364201784133911,
      "learning_rate": 5.854814814814815e-06,
      "loss": 0.1313,
      "step": 9550
    },
    {
      "epoch": 70.817843866171,
      "grad_norm": 0.8792798519134521,
      "learning_rate": 5.84e-06,
      "loss": 0.1504,
      "step": 9560
    },
    {
      "epoch": 70.89219330855019,
      "grad_norm": 0.8921117782592773,
      "learning_rate": 5.825185185185186e-06,
      "loss": 0.1342,
      "step": 9570
    },
    {
      "epoch": 70.96654275092936,
      "grad_norm": 1.071223258972168,
      "learning_rate": 5.8103703703703704e-06,
      "loss": 0.141,
      "step": 9580
    },
    {
      "epoch": 71.03717472118959,
      "grad_norm": 0.8828355669975281,
      "learning_rate": 5.7970370370370375e-06,
      "loss": 0.1306,
      "step": 9590
    },
    {
      "epoch": 71.11152416356877,
      "grad_norm": 0.7054822444915771,
      "learning_rate": 5.782222222222222e-06,
      "loss": 0.1327,
      "step": 9600
    },
    {
      "epoch": 71.18587360594796,
      "grad_norm": 0.9680251479148865,
      "learning_rate": 5.767407407407408e-06,
      "loss": 0.119,
      "step": 9610
    },
    {
      "epoch": 71.26022304832713,
      "grad_norm": 0.8347944617271423,
      "learning_rate": 5.752592592592593e-06,
      "loss": 0.1353,
      "step": 9620
    },
    {
      "epoch": 71.33457249070632,
      "grad_norm": 0.8709137439727783,
      "learning_rate": 5.737777777777778e-06,
      "loss": 0.1302,
      "step": 9630
    },
    {
      "epoch": 71.40892193308551,
      "grad_norm": 1.1209731101989746,
      "learning_rate": 5.722962962962963e-06,
      "loss": 0.1268,
      "step": 9640
    },
    {
      "epoch": 71.48327137546468,
      "grad_norm": 0.8695209622383118,
      "learning_rate": 5.708148148148148e-06,
      "loss": 0.135,
      "step": 9650
    },
    {
      "epoch": 71.55762081784387,
      "grad_norm": 1.2056941986083984,
      "learning_rate": 5.6933333333333344e-06,
      "loss": 0.1377,
      "step": 9660
    },
    {
      "epoch": 71.63197026022304,
      "grad_norm": 0.8682478070259094,
      "learning_rate": 5.678518518518518e-06,
      "loss": 0.1405,
      "step": 9670
    },
    {
      "epoch": 71.70631970260223,
      "grad_norm": 0.8230230212211609,
      "learning_rate": 5.663703703703705e-06,
      "loss": 0.1538,
      "step": 9680
    },
    {
      "epoch": 71.78066914498142,
      "grad_norm": 1.1058135032653809,
      "learning_rate": 5.64888888888889e-06,
      "loss": 0.1257,
      "step": 9690
    },
    {
      "epoch": 71.85501858736059,
      "grad_norm": 0.9285714626312256,
      "learning_rate": 5.634074074074074e-06,
      "loss": 0.1314,
      "step": 9700
    },
    {
      "epoch": 71.92936802973978,
      "grad_norm": 1.1732869148254395,
      "learning_rate": 5.61925925925926e-06,
      "loss": 0.1296,
      "step": 9710
    },
    {
      "epoch": 72.0,
      "grad_norm": 1.4525128602981567,
      "learning_rate": 5.604444444444445e-06,
      "loss": 0.1283,
      "step": 9720
    },
    {
      "epoch": 72.07434944237919,
      "grad_norm": 0.9736058115959167,
      "learning_rate": 5.5896296296296305e-06,
      "loss": 0.1182,
      "step": 9730
    },
    {
      "epoch": 72.14869888475836,
      "grad_norm": 1.0456651449203491,
      "learning_rate": 5.574814814814815e-06,
      "loss": 0.1275,
      "step": 9740
    },
    {
      "epoch": 72.22304832713755,
      "grad_norm": 0.8918138742446899,
      "learning_rate": 5.560000000000001e-06,
      "loss": 0.1271,
      "step": 9750
    },
    {
      "epoch": 72.29739776951673,
      "grad_norm": 0.9155603051185608,
      "learning_rate": 5.545185185185186e-06,
      "loss": 0.151,
      "step": 9760
    },
    {
      "epoch": 72.37174721189591,
      "grad_norm": 0.8952556252479553,
      "learning_rate": 5.5303703703703706e-06,
      "loss": 0.1449,
      "step": 9770
    },
    {
      "epoch": 72.4460966542751,
      "grad_norm": 1.0868334770202637,
      "learning_rate": 5.515555555555556e-06,
      "loss": 0.1411,
      "step": 9780
    },
    {
      "epoch": 72.52044609665427,
      "grad_norm": 0.8995146155357361,
      "learning_rate": 5.500740740740741e-06,
      "loss": 0.1273,
      "step": 9790
    },
    {
      "epoch": 72.59479553903346,
      "grad_norm": 1.0130406618118286,
      "learning_rate": 5.485925925925927e-06,
      "loss": 0.11,
      "step": 9800
    },
    {
      "epoch": 72.66914498141264,
      "grad_norm": 0.8626880049705505,
      "learning_rate": 5.4711111111111114e-06,
      "loss": 0.1343,
      "step": 9810
    },
    {
      "epoch": 72.74349442379182,
      "grad_norm": 0.8818926811218262,
      "learning_rate": 5.456296296296296e-06,
      "loss": 0.1282,
      "step": 9820
    },
    {
      "epoch": 72.817843866171,
      "grad_norm": 0.8680515289306641,
      "learning_rate": 5.441481481481482e-06,
      "loss": 0.1436,
      "step": 9830
    },
    {
      "epoch": 72.89219330855019,
      "grad_norm": 0.8087373375892639,
      "learning_rate": 5.426666666666667e-06,
      "loss": 0.1335,
      "step": 9840
    },
    {
      "epoch": 72.96654275092936,
      "grad_norm": 1.0804170370101929,
      "learning_rate": 5.411851851851852e-06,
      "loss": 0.1283,
      "step": 9850
    },
    {
      "epoch": 73.03717472118959,
      "grad_norm": 0.9995390772819519,
      "learning_rate": 5.397037037037037e-06,
      "loss": 0.1384,
      "step": 9860
    },
    {
      "epoch": 73.11152416356877,
      "grad_norm": 1.2658090591430664,
      "learning_rate": 5.382222222222223e-06,
      "loss": 0.134,
      "step": 9870
    },
    {
      "epoch": 73.18587360594796,
      "grad_norm": 0.896478533744812,
      "learning_rate": 5.3674074074074075e-06,
      "loss": 0.1377,
      "step": 9880
    },
    {
      "epoch": 73.26022304832713,
      "grad_norm": 0.9726081490516663,
      "learning_rate": 5.352592592592592e-06,
      "loss": 0.1314,
      "step": 9890
    },
    {
      "epoch": 73.33457249070632,
      "grad_norm": 1.019861102104187,
      "learning_rate": 5.337777777777779e-06,
      "loss": 0.1374,
      "step": 9900
    },
    {
      "epoch": 73.40892193308551,
      "grad_norm": 0.9985848069190979,
      "learning_rate": 5.322962962962963e-06,
      "loss": 0.1467,
      "step": 9910
    },
    {
      "epoch": 73.48327137546468,
      "grad_norm": 0.81931471824646,
      "learning_rate": 5.308148148148149e-06,
      "loss": 0.1359,
      "step": 9920
    },
    {
      "epoch": 73.55762081784387,
      "grad_norm": 0.8976685404777527,
      "learning_rate": 5.293333333333334e-06,
      "loss": 0.1289,
      "step": 9930
    },
    {
      "epoch": 73.63197026022304,
      "grad_norm": 1.0792251825332642,
      "learning_rate": 5.27851851851852e-06,
      "loss": 0.1484,
      "step": 9940
    },
    {
      "epoch": 73.70631970260223,
      "grad_norm": 1.076115369796753,
      "learning_rate": 5.2637037037037045e-06,
      "loss": 0.1315,
      "step": 9950
    },
    {
      "epoch": 73.78066914498142,
      "grad_norm": 1.0834977626800537,
      "learning_rate": 5.248888888888889e-06,
      "loss": 0.1128,
      "step": 9960
    },
    {
      "epoch": 73.85501858736059,
      "grad_norm": 0.9610857963562012,
      "learning_rate": 5.234074074074075e-06,
      "loss": 0.1277,
      "step": 9970
    },
    {
      "epoch": 73.92936802973978,
      "grad_norm": 0.998842716217041,
      "learning_rate": 5.21925925925926e-06,
      "loss": 0.1403,
      "step": 9980
    },
    {
      "epoch": 74.0,
      "grad_norm": 1.3068432807922363,
      "learning_rate": 5.204444444444445e-06,
      "loss": 0.1177,
      "step": 9990
    },
    {
      "epoch": 74.07434944237919,
      "grad_norm": 0.9460587501525879,
      "learning_rate": 5.18962962962963e-06,
      "loss": 0.1431,
      "step": 10000
    },
    {
      "epoch": 74.14869888475836,
      "grad_norm": 0.9319955110549927,
      "learning_rate": 5.174814814814815e-06,
      "loss": 0.1134,
      "step": 10010
    },
    {
      "epoch": 74.22304832713755,
      "grad_norm": 0.9707612991333008,
      "learning_rate": 5.1600000000000006e-06,
      "loss": 0.1373,
      "step": 10020
    },
    {
      "epoch": 74.29739776951673,
      "grad_norm": 1.0662429332733154,
      "learning_rate": 5.145185185185185e-06,
      "loss": 0.1411,
      "step": 10030
    },
    {
      "epoch": 74.37174721189591,
      "grad_norm": 1.0434279441833496,
      "learning_rate": 5.130370370370371e-06,
      "loss": 0.1319,
      "step": 10040
    },
    {
      "epoch": 74.4460966542751,
      "grad_norm": 0.9531933069229126,
      "learning_rate": 5.115555555555556e-06,
      "loss": 0.1359,
      "step": 10050
    },
    {
      "epoch": 74.52044609665427,
      "grad_norm": 1.1429095268249512,
      "learning_rate": 5.1007407407407414e-06,
      "loss": 0.1308,
      "step": 10060
    },
    {
      "epoch": 74.59479553903346,
      "grad_norm": 0.795495331287384,
      "learning_rate": 5.085925925925926e-06,
      "loss": 0.1228,
      "step": 10070
    },
    {
      "epoch": 74.66914498141264,
      "grad_norm": 0.8936681747436523,
      "learning_rate": 5.071111111111111e-06,
      "loss": 0.1273,
      "step": 10080
    },
    {
      "epoch": 74.74349442379182,
      "grad_norm": 0.9503089189529419,
      "learning_rate": 5.056296296296297e-06,
      "loss": 0.1308,
      "step": 10090
    },
    {
      "epoch": 74.817843866171,
      "grad_norm": 1.2921439409255981,
      "learning_rate": 5.0414814814814815e-06,
      "loss": 0.1333,
      "step": 10100
    },
    {
      "epoch": 74.89219330855019,
      "grad_norm": 0.9991278052330017,
      "learning_rate": 5.026666666666667e-06,
      "loss": 0.1302,
      "step": 10110
    },
    {
      "epoch": 74.96654275092936,
      "grad_norm": 1.0444844961166382,
      "learning_rate": 5.011851851851852e-06,
      "loss": 0.1493,
      "step": 10120
    },
    {
      "epoch": 75.03717472118959,
      "grad_norm": 1.316400408744812,
      "learning_rate": 4.9970370370370375e-06,
      "loss": 0.1183,
      "step": 10130
    },
    {
      "epoch": 75.11152416356877,
      "grad_norm": 0.8280606269836426,
      "learning_rate": 4.982222222222222e-06,
      "loss": 0.1315,
      "step": 10140
    },
    {
      "epoch": 75.18587360594796,
      "grad_norm": 0.9674552083015442,
      "learning_rate": 4.967407407407408e-06,
      "loss": 0.1265,
      "step": 10150
    },
    {
      "epoch": 75.26022304832713,
      "grad_norm": 1.4145972728729248,
      "learning_rate": 4.952592592592593e-06,
      "loss": 0.1477,
      "step": 10160
    },
    {
      "epoch": 75.33457249070632,
      "grad_norm": 1.249153733253479,
      "learning_rate": 4.937777777777778e-06,
      "loss": 0.1341,
      "step": 10170
    },
    {
      "epoch": 75.40892193308551,
      "grad_norm": 0.8443564772605896,
      "learning_rate": 4.922962962962963e-06,
      "loss": 0.1351,
      "step": 10180
    },
    {
      "epoch": 75.48327137546468,
      "grad_norm": 0.8735051155090332,
      "learning_rate": 4.908148148148149e-06,
      "loss": 0.1326,
      "step": 10190
    },
    {
      "epoch": 75.55762081784387,
      "grad_norm": 1.1477917432785034,
      "learning_rate": 4.893333333333334e-06,
      "loss": 0.1273,
      "step": 10200
    },
    {
      "epoch": 75.63197026022304,
      "grad_norm": 1.0335309505462646,
      "learning_rate": 4.878518518518519e-06,
      "loss": 0.1297,
      "step": 10210
    },
    {
      "epoch": 75.70631970260223,
      "grad_norm": 0.8670462965965271,
      "learning_rate": 4.863703703703704e-06,
      "loss": 0.1228,
      "step": 10220
    },
    {
      "epoch": 75.78066914498142,
      "grad_norm": 0.9979777336120605,
      "learning_rate": 4.848888888888889e-06,
      "loss": 0.1353,
      "step": 10230
    },
    {
      "epoch": 75.85501858736059,
      "grad_norm": 1.2502318620681763,
      "learning_rate": 4.8340740740740745e-06,
      "loss": 0.1328,
      "step": 10240
    },
    {
      "epoch": 75.92936802973978,
      "grad_norm": 0.8435818552970886,
      "learning_rate": 4.819259259259259e-06,
      "loss": 0.141,
      "step": 10250
    },
    {
      "epoch": 76.0,
      "grad_norm": 1.2600972652435303,
      "learning_rate": 4.804444444444445e-06,
      "loss": 0.1378,
      "step": 10260
    },
    {
      "epoch": 76.07434944237919,
      "grad_norm": 1.0339683294296265,
      "learning_rate": 4.789629629629631e-06,
      "loss": 0.1265,
      "step": 10270
    },
    {
      "epoch": 76.14869888475836,
      "grad_norm": 1.2904934883117676,
      "learning_rate": 4.774814814814815e-06,
      "loss": 0.1401,
      "step": 10280
    },
    {
      "epoch": 76.22304832713755,
      "grad_norm": 1.04734468460083,
      "learning_rate": 4.76e-06,
      "loss": 0.139,
      "step": 10290
    },
    {
      "epoch": 76.29739776951673,
      "grad_norm": 1.0817610025405884,
      "learning_rate": 4.745185185185186e-06,
      "loss": 0.1443,
      "step": 10300
    },
    {
      "epoch": 76.37174721189591,
      "grad_norm": 1.0421062707901,
      "learning_rate": 4.730370370370371e-06,
      "loss": 0.116,
      "step": 10310
    },
    {
      "epoch": 76.4460966542751,
      "grad_norm": 1.1411939859390259,
      "learning_rate": 4.715555555555556e-06,
      "loss": 0.1332,
      "step": 10320
    },
    {
      "epoch": 76.52044609665427,
      "grad_norm": 1.0253641605377197,
      "learning_rate": 4.700740740740741e-06,
      "loss": 0.148,
      "step": 10330
    },
    {
      "epoch": 76.59479553903346,
      "grad_norm": 1.1618826389312744,
      "learning_rate": 4.685925925925927e-06,
      "loss": 0.1284,
      "step": 10340
    },
    {
      "epoch": 76.66914498141264,
      "grad_norm": 0.9593906402587891,
      "learning_rate": 4.6711111111111115e-06,
      "loss": 0.1321,
      "step": 10350
    },
    {
      "epoch": 76.74349442379182,
      "grad_norm": 0.9517806768417358,
      "learning_rate": 4.656296296296296e-06,
      "loss": 0.1259,
      "step": 10360
    },
    {
      "epoch": 76.817843866171,
      "grad_norm": 1.0200588703155518,
      "learning_rate": 4.641481481481482e-06,
      "loss": 0.1354,
      "step": 10370
    },
    {
      "epoch": 76.89219330855019,
      "grad_norm": 1.203471302986145,
      "learning_rate": 4.626666666666667e-06,
      "loss": 0.1254,
      "step": 10380
    },
    {
      "epoch": 76.96654275092936,
      "grad_norm": 0.8463445901870728,
      "learning_rate": 4.611851851851852e-06,
      "loss": 0.1318,
      "step": 10390
    },
    {
      "epoch": 77.03717472118959,
      "grad_norm": 0.7741932272911072,
      "learning_rate": 4.597037037037038e-06,
      "loss": 0.1192,
      "step": 10400
    },
    {
      "epoch": 77.11152416356877,
      "grad_norm": 1.0009827613830566,
      "learning_rate": 4.582222222222223e-06,
      "loss": 0.1575,
      "step": 10410
    },
    {
      "epoch": 77.18587360594796,
      "grad_norm": 1.1071760654449463,
      "learning_rate": 4.5674074074074076e-06,
      "loss": 0.1243,
      "step": 10420
    },
    {
      "epoch": 77.26022304832713,
      "grad_norm": 1.2478753328323364,
      "learning_rate": 4.552592592592593e-06,
      "loss": 0.1387,
      "step": 10430
    },
    {
      "epoch": 77.33457249070632,
      "grad_norm": 1.012068510055542,
      "learning_rate": 4.537777777777778e-06,
      "loss": 0.1351,
      "step": 10440
    },
    {
      "epoch": 77.40892193308551,
      "grad_norm": 0.816733717918396,
      "learning_rate": 4.522962962962964e-06,
      "loss": 0.1436,
      "step": 10450
    },
    {
      "epoch": 77.48327137546468,
      "grad_norm": 1.3707910776138306,
      "learning_rate": 4.5081481481481484e-06,
      "loss": 0.135,
      "step": 10460
    },
    {
      "epoch": 77.55762081784387,
      "grad_norm": 0.9893894791603088,
      "learning_rate": 4.493333333333333e-06,
      "loss": 0.1234,
      "step": 10470
    },
    {
      "epoch": 77.63197026022304,
      "grad_norm": 1.332150936126709,
      "learning_rate": 4.478518518518519e-06,
      "loss": 0.1263,
      "step": 10480
    },
    {
      "epoch": 77.70631970260223,
      "grad_norm": 0.9068782329559326,
      "learning_rate": 4.463703703703704e-06,
      "loss": 0.1218,
      "step": 10490
    },
    {
      "epoch": 77.78066914498142,
      "grad_norm": 0.916117250919342,
      "learning_rate": 4.448888888888889e-06,
      "loss": 0.1313,
      "step": 10500
    },
    {
      "epoch": 77.85501858736059,
      "grad_norm": 1.0368441343307495,
      "learning_rate": 4.434074074074075e-06,
      "loss": 0.1446,
      "step": 10510
    },
    {
      "epoch": 77.92936802973978,
      "grad_norm": 1.012446641921997,
      "learning_rate": 4.41925925925926e-06,
      "loss": 0.1227,
      "step": 10520
    },
    {
      "epoch": 78.0,
      "grad_norm": 2.035193920135498,
      "learning_rate": 4.404444444444445e-06,
      "loss": 0.1257,
      "step": 10530
    },
    {
      "epoch": 78.07434944237919,
      "grad_norm": 1.183709979057312,
      "learning_rate": 4.38962962962963e-06,
      "loss": 0.1388,
      "step": 10540
    },
    {
      "epoch": 78.14869888475836,
      "grad_norm": 0.9196509122848511,
      "learning_rate": 4.374814814814815e-06,
      "loss": 0.1244,
      "step": 10550
    },
    {
      "epoch": 78.22304832713755,
      "grad_norm": 0.9531586170196533,
      "learning_rate": 4.360000000000001e-06,
      "loss": 0.1306,
      "step": 10560
    },
    {
      "epoch": 78.29739776951673,
      "grad_norm": 0.9682340025901794,
      "learning_rate": 4.345185185185185e-06,
      "loss": 0.1249,
      "step": 10570
    },
    {
      "epoch": 78.37174721189591,
      "grad_norm": 0.8896783590316772,
      "learning_rate": 4.330370370370371e-06,
      "loss": 0.1291,
      "step": 10580
    },
    {
      "epoch": 78.4460966542751,
      "grad_norm": 0.9327057600021362,
      "learning_rate": 4.315555555555556e-06,
      "loss": 0.1248,
      "step": 10590
    },
    {
      "epoch": 78.52044609665427,
      "grad_norm": 1.0116671323776245,
      "learning_rate": 4.300740740740741e-06,
      "loss": 0.1217,
      "step": 10600
    },
    {
      "epoch": 78.59479553903346,
      "grad_norm": 1.0690984725952148,
      "learning_rate": 4.285925925925926e-06,
      "loss": 0.1377,
      "step": 10610
    },
    {
      "epoch": 78.66914498141264,
      "grad_norm": 1.1673985719680786,
      "learning_rate": 4.271111111111111e-06,
      "loss": 0.1423,
      "step": 10620
    },
    {
      "epoch": 78.74349442379182,
      "grad_norm": 0.9697321057319641,
      "learning_rate": 4.256296296296297e-06,
      "loss": 0.1323,
      "step": 10630
    },
    {
      "epoch": 78.817843866171,
      "grad_norm": 1.1139659881591797,
      "learning_rate": 4.241481481481482e-06,
      "loss": 0.1222,
      "step": 10640
    },
    {
      "epoch": 78.89219330855019,
      "grad_norm": 0.9218722581863403,
      "learning_rate": 4.226666666666667e-06,
      "loss": 0.1507,
      "step": 10650
    },
    {
      "epoch": 78.96654275092936,
      "grad_norm": 1.4712673425674438,
      "learning_rate": 4.211851851851852e-06,
      "loss": 0.1324,
      "step": 10660
    },
    {
      "epoch": 79.03717472118959,
      "grad_norm": 0.9425797462463379,
      "learning_rate": 4.1970370370370376e-06,
      "loss": 0.1424,
      "step": 10670
    },
    {
      "epoch": 79.11152416356877,
      "grad_norm": 0.9203845262527466,
      "learning_rate": 4.182222222222222e-06,
      "loss": 0.1353,
      "step": 10680
    },
    {
      "epoch": 79.18587360594796,
      "grad_norm": 1.0539754629135132,
      "learning_rate": 4.167407407407408e-06,
      "loss": 0.1209,
      "step": 10690
    },
    {
      "epoch": 79.26022304832713,
      "grad_norm": 1.0001217126846313,
      "learning_rate": 4.152592592592593e-06,
      "loss": 0.1266,
      "step": 10700
    },
    {
      "epoch": 79.33457249070632,
      "grad_norm": 1.2179017066955566,
      "learning_rate": 4.1377777777777784e-06,
      "loss": 0.1478,
      "step": 10710
    },
    {
      "epoch": 79.40892193308551,
      "grad_norm": 0.9398283362388611,
      "learning_rate": 4.122962962962963e-06,
      "loss": 0.1397,
      "step": 10720
    },
    {
      "epoch": 79.48327137546468,
      "grad_norm": 1.0587700605392456,
      "learning_rate": 4.108148148148148e-06,
      "loss": 0.124,
      "step": 10730
    },
    {
      "epoch": 79.55762081784387,
      "grad_norm": 1.113147497177124,
      "learning_rate": 4.093333333333334e-06,
      "loss": 0.1281,
      "step": 10740
    },
    {
      "epoch": 79.63197026022304,
      "grad_norm": 1.170055866241455,
      "learning_rate": 4.0785185185185185e-06,
      "loss": 0.1321,
      "step": 10750
    },
    {
      "epoch": 79.70631970260223,
      "grad_norm": 1.007744550704956,
      "learning_rate": 4.063703703703704e-06,
      "loss": 0.1332,
      "step": 10760
    },
    {
      "epoch": 79.78066914498142,
      "grad_norm": 1.0417451858520508,
      "learning_rate": 4.04888888888889e-06,
      "loss": 0.1353,
      "step": 10770
    },
    {
      "epoch": 79.85501858736059,
      "grad_norm": 0.8927367925643921,
      "learning_rate": 4.0340740740740745e-06,
      "loss": 0.1264,
      "step": 10780
    },
    {
      "epoch": 79.92936802973978,
      "grad_norm": 0.9647541046142578,
      "learning_rate": 4.019259259259259e-06,
      "loss": 0.1289,
      "step": 10790
    },
    {
      "epoch": 80.0,
      "grad_norm": 1.4922772645950317,
      "learning_rate": 4.004444444444445e-06,
      "loss": 0.1373,
      "step": 10800
    },
    {
      "epoch": 80.07434944237919,
      "grad_norm": 0.9761902093887329,
      "learning_rate": 3.98962962962963e-06,
      "loss": 0.1423,
      "step": 10810
    },
    {
      "epoch": 80.14869888475836,
      "grad_norm": 1.3514829874038696,
      "learning_rate": 3.974814814814815e-06,
      "loss": 0.1284,
      "step": 10820
    },
    {
      "epoch": 80.22304832713755,
      "grad_norm": 0.9675732851028442,
      "learning_rate": 3.96e-06,
      "loss": 0.1179,
      "step": 10830
    },
    {
      "epoch": 80.29739776951673,
      "grad_norm": 1.0298280715942383,
      "learning_rate": 3.945185185185186e-06,
      "loss": 0.1276,
      "step": 10840
    },
    {
      "epoch": 80.37174721189591,
      "grad_norm": 1.2481470108032227,
      "learning_rate": 3.930370370370371e-06,
      "loss": 0.1441,
      "step": 10850
    },
    {
      "epoch": 80.4460966542751,
      "grad_norm": 1.0604276657104492,
      "learning_rate": 3.9155555555555554e-06,
      "loss": 0.1457,
      "step": 10860
    },
    {
      "epoch": 80.52044609665427,
      "grad_norm": 1.0653769969940186,
      "learning_rate": 3.900740740740741e-06,
      "loss": 0.1265,
      "step": 10870
    },
    {
      "epoch": 80.59479553903346,
      "grad_norm": 1.0588191747665405,
      "learning_rate": 3.885925925925927e-06,
      "loss": 0.1194,
      "step": 10880
    },
    {
      "epoch": 80.66914498141264,
      "grad_norm": 1.0877771377563477,
      "learning_rate": 3.8711111111111115e-06,
      "loss": 0.1247,
      "step": 10890
    },
    {
      "epoch": 80.74349442379182,
      "grad_norm": 0.9206658005714417,
      "learning_rate": 3.856296296296297e-06,
      "loss": 0.1249,
      "step": 10900
    },
    {
      "epoch": 80.817843866171,
      "grad_norm": 0.8940345048904419,
      "learning_rate": 3.841481481481482e-06,
      "loss": 0.1338,
      "step": 10910
    },
    {
      "epoch": 80.89219330855019,
      "grad_norm": 1.078391432762146,
      "learning_rate": 3.826666666666667e-06,
      "loss": 0.1476,
      "step": 10920
    },
    {
      "epoch": 80.96654275092936,
      "grad_norm": 0.8592806458473206,
      "learning_rate": 3.811851851851852e-06,
      "loss": 0.1303,
      "step": 10930
    },
    {
      "epoch": 81.03717472118959,
      "grad_norm": 1.204215168952942,
      "learning_rate": 3.7970370370370376e-06,
      "loss": 0.1265,
      "step": 10940
    },
    {
      "epoch": 81.11152416356877,
      "grad_norm": 0.8147901296615601,
      "learning_rate": 3.782222222222223e-06,
      "loss": 0.1258,
      "step": 10950
    },
    {
      "epoch": 81.18587360594796,
      "grad_norm": 0.9311184883117676,
      "learning_rate": 3.767407407407408e-06,
      "loss": 0.136,
      "step": 10960
    },
    {
      "epoch": 81.26022304832713,
      "grad_norm": 1.2787083387374878,
      "learning_rate": 3.752592592592593e-06,
      "loss": 0.122,
      "step": 10970
    },
    {
      "epoch": 81.33457249070632,
      "grad_norm": 1.0417792797088623,
      "learning_rate": 3.737777777777778e-06,
      "loss": 0.1481,
      "step": 10980
    },
    {
      "epoch": 81.40892193308551,
      "grad_norm": 1.0412253141403198,
      "learning_rate": 3.7229629629629633e-06,
      "loss": 0.1329,
      "step": 10990
    },
    {
      "epoch": 81.48327137546468,
      "grad_norm": 0.9512044191360474,
      "learning_rate": 3.7081481481481485e-06,
      "loss": 0.1314,
      "step": 11000
    },
    {
      "epoch": 81.55762081784387,
      "grad_norm": 1.599938988685608,
      "learning_rate": 3.6933333333333337e-06,
      "loss": 0.1464,
      "step": 11010
    },
    {
      "epoch": 81.63197026022304,
      "grad_norm": 1.3419454097747803,
      "learning_rate": 3.678518518518519e-06,
      "loss": 0.1376,
      "step": 11020
    },
    {
      "epoch": 81.70631970260223,
      "grad_norm": 1.227866291999817,
      "learning_rate": 3.6637037037037037e-06,
      "loss": 0.126,
      "step": 11030
    },
    {
      "epoch": 81.78066914498142,
      "grad_norm": 0.9003969430923462,
      "learning_rate": 3.648888888888889e-06,
      "loss": 0.1222,
      "step": 11040
    },
    {
      "epoch": 81.85501858736059,
      "grad_norm": 0.8109151124954224,
      "learning_rate": 3.634074074074074e-06,
      "loss": 0.1261,
      "step": 11050
    },
    {
      "epoch": 81.92936802973978,
      "grad_norm": 0.870663583278656,
      "learning_rate": 3.6192592592592598e-06,
      "loss": 0.1341,
      "step": 11060
    },
    {
      "epoch": 82.0,
      "grad_norm": 1.6762784719467163,
      "learning_rate": 3.604444444444445e-06,
      "loss": 0.1296,
      "step": 11070
    },
    {
      "epoch": 82.07434944237919,
      "grad_norm": 1.3350549936294556,
      "learning_rate": 3.58962962962963e-06,
      "loss": 0.1343,
      "step": 11080
    },
    {
      "epoch": 82.14869888475836,
      "grad_norm": 0.8967548608779907,
      "learning_rate": 3.574814814814815e-06,
      "loss": 0.1174,
      "step": 11090
    },
    {
      "epoch": 82.22304832713755,
      "grad_norm": 0.946350634098053,
      "learning_rate": 3.5600000000000002e-06,
      "loss": 0.1459,
      "step": 11100
    },
    {
      "epoch": 82.29739776951673,
      "grad_norm": 1.2275902032852173,
      "learning_rate": 3.5451851851851854e-06,
      "loss": 0.1367,
      "step": 11110
    },
    {
      "epoch": 82.37174721189591,
      "grad_norm": 1.0175108909606934,
      "learning_rate": 3.5303703703703707e-06,
      "loss": 0.1406,
      "step": 11120
    },
    {
      "epoch": 82.4460966542751,
      "grad_norm": 0.9394354820251465,
      "learning_rate": 3.515555555555556e-06,
      "loss": 0.1352,
      "step": 11130
    },
    {
      "epoch": 82.52044609665427,
      "grad_norm": 1.207950234413147,
      "learning_rate": 3.500740740740741e-06,
      "loss": 0.1298,
      "step": 11140
    },
    {
      "epoch": 82.59479553903346,
      "grad_norm": 1.8742997646331787,
      "learning_rate": 3.485925925925926e-06,
      "loss": 0.1259,
      "step": 11150
    },
    {
      "epoch": 82.66914498141264,
      "grad_norm": 1.0273284912109375,
      "learning_rate": 3.471111111111111e-06,
      "loss": 0.1497,
      "step": 11160
    },
    {
      "epoch": 82.74349442379182,
      "grad_norm": 1.3326594829559326,
      "learning_rate": 3.4562962962962963e-06,
      "loss": 0.1294,
      "step": 11170
    },
    {
      "epoch": 82.817843866171,
      "grad_norm": 1.2223114967346191,
      "learning_rate": 3.441481481481482e-06,
      "loss": 0.1294,
      "step": 11180
    },
    {
      "epoch": 82.89219330855019,
      "grad_norm": 1.11283278465271,
      "learning_rate": 3.426666666666667e-06,
      "loss": 0.1388,
      "step": 11190
    },
    {
      "epoch": 82.96654275092936,
      "grad_norm": 1.158312439918518,
      "learning_rate": 3.4118518518518524e-06,
      "loss": 0.1109,
      "step": 11200
    },
    {
      "epoch": 83.03717472118959,
      "grad_norm": 0.9583567976951599,
      "learning_rate": 3.3970370370370376e-06,
      "loss": 0.118,
      "step": 11210
    },
    {
      "epoch": 83.11152416356877,
      "grad_norm": 0.8566401600837708,
      "learning_rate": 3.3822222222222224e-06,
      "loss": 0.1207,
      "step": 11220
    },
    {
      "epoch": 83.18587360594796,
      "grad_norm": 1.1720391511917114,
      "learning_rate": 3.3674074074074076e-06,
      "loss": 0.1268,
      "step": 11230
    },
    {
      "epoch": 83.26022304832713,
      "grad_norm": 1.0112907886505127,
      "learning_rate": 3.352592592592593e-06,
      "loss": 0.1399,
      "step": 11240
    },
    {
      "epoch": 83.33457249070632,
      "grad_norm": 0.9410423040390015,
      "learning_rate": 3.337777777777778e-06,
      "loss": 0.1283,
      "step": 11250
    },
    {
      "epoch": 83.40892193308551,
      "grad_norm": 1.1940737962722778,
      "learning_rate": 3.3229629629629633e-06,
      "loss": 0.1286,
      "step": 11260
    },
    {
      "epoch": 83.48327137546468,
      "grad_norm": 0.9011537432670593,
      "learning_rate": 3.3081481481481485e-06,
      "loss": 0.1276,
      "step": 11270
    },
    {
      "epoch": 83.55762081784387,
      "grad_norm": 0.8286657929420471,
      "learning_rate": 3.2933333333333333e-06,
      "loss": 0.1478,
      "step": 11280
    },
    {
      "epoch": 83.63197026022304,
      "grad_norm": 1.2714929580688477,
      "learning_rate": 3.2785185185185185e-06,
      "loss": 0.1377,
      "step": 11290
    },
    {
      "epoch": 83.70631970260223,
      "grad_norm": 1.1115643978118896,
      "learning_rate": 3.263703703703704e-06,
      "loss": 0.1307,
      "step": 11300
    },
    {
      "epoch": 83.78066914498142,
      "grad_norm": 0.9540330171585083,
      "learning_rate": 3.2488888888888894e-06,
      "loss": 0.1198,
      "step": 11310
    },
    {
      "epoch": 83.85501858736059,
      "grad_norm": 1.02802574634552,
      "learning_rate": 3.2340740740740746e-06,
      "loss": 0.1385,
      "step": 11320
    },
    {
      "epoch": 83.92936802973978,
      "grad_norm": 1.2890301942825317,
      "learning_rate": 3.21925925925926e-06,
      "loss": 0.1419,
      "step": 11330
    },
    {
      "epoch": 84.0,
      "grad_norm": 1.8963011503219604,
      "learning_rate": 3.2044444444444446e-06,
      "loss": 0.1194,
      "step": 11340
    },
    {
      "epoch": 84.07434944237919,
      "grad_norm": 1.0885872840881348,
      "learning_rate": 3.18962962962963e-06,
      "loss": 0.1309,
      "step": 11350
    },
    {
      "epoch": 84.14869888475836,
      "grad_norm": 1.1150691509246826,
      "learning_rate": 3.174814814814815e-06,
      "loss": 0.1266,
      "step": 11360
    },
    {
      "epoch": 84.22304832713755,
      "grad_norm": 1.3153085708618164,
      "learning_rate": 3.1600000000000002e-06,
      "loss": 0.1375,
      "step": 11370
    },
    {
      "epoch": 84.29739776951673,
      "grad_norm": 1.0077635049819946,
      "learning_rate": 3.1451851851851855e-06,
      "loss": 0.1318,
      "step": 11380
    },
    {
      "epoch": 84.37174721189591,
      "grad_norm": 0.8584081530570984,
      "learning_rate": 3.1303703703703707e-06,
      "loss": 0.1318,
      "step": 11390
    },
    {
      "epoch": 84.4460966542751,
      "grad_norm": 1.2493155002593994,
      "learning_rate": 3.1155555555555555e-06,
      "loss": 0.1358,
      "step": 11400
    },
    {
      "epoch": 84.52044609665427,
      "grad_norm": 1.116055965423584,
      "learning_rate": 3.1007407407407407e-06,
      "loss": 0.126,
      "step": 11410
    },
    {
      "epoch": 84.59479553903346,
      "grad_norm": 1.0132757425308228,
      "learning_rate": 3.085925925925926e-06,
      "loss": 0.1322,
      "step": 11420
    },
    {
      "epoch": 84.66914498141264,
      "grad_norm": 1.1269406080245972,
      "learning_rate": 3.0711111111111115e-06,
      "loss": 0.1307,
      "step": 11430
    },
    {
      "epoch": 84.74349442379182,
      "grad_norm": 0.9278567433357239,
      "learning_rate": 3.0562962962962968e-06,
      "loss": 0.1394,
      "step": 11440
    },
    {
      "epoch": 84.817843866171,
      "grad_norm": 1.0787856578826904,
      "learning_rate": 3.041481481481482e-06,
      "loss": 0.1177,
      "step": 11450
    },
    {
      "epoch": 84.89219330855019,
      "grad_norm": 0.9208582639694214,
      "learning_rate": 3.0266666666666668e-06,
      "loss": 0.1389,
      "step": 11460
    },
    {
      "epoch": 84.96654275092936,
      "grad_norm": 1.0019750595092773,
      "learning_rate": 3.011851851851852e-06,
      "loss": 0.1242,
      "step": 11470
    },
    {
      "epoch": 85.03717472118959,
      "grad_norm": 1.0016405582427979,
      "learning_rate": 2.997037037037037e-06,
      "loss": 0.1323,
      "step": 11480
    },
    {
      "epoch": 85.11152416356877,
      "grad_norm": 1.0301657915115356,
      "learning_rate": 2.9822222222222224e-06,
      "loss": 0.1137,
      "step": 11490
    },
    {
      "epoch": 85.18587360594796,
      "grad_norm": 1.205398440361023,
      "learning_rate": 2.9674074074074076e-06,
      "loss": 0.1342,
      "step": 11500
    },
    {
      "epoch": 85.26022304832713,
      "grad_norm": 0.9548113346099854,
      "learning_rate": 2.952592592592593e-06,
      "loss": 0.1214,
      "step": 11510
    },
    {
      "epoch": 85.33457249070632,
      "grad_norm": 0.9445570707321167,
      "learning_rate": 2.937777777777778e-06,
      "loss": 0.13,
      "step": 11520
    },
    {
      "epoch": 85.40892193308551,
      "grad_norm": 0.8533211946487427,
      "learning_rate": 2.922962962962963e-06,
      "loss": 0.1336,
      "step": 11530
    },
    {
      "epoch": 85.48327137546468,
      "grad_norm": 0.982537567615509,
      "learning_rate": 2.908148148148148e-06,
      "loss": 0.1212,
      "step": 11540
    },
    {
      "epoch": 85.55762081784387,
      "grad_norm": 0.9674367904663086,
      "learning_rate": 2.8933333333333337e-06,
      "loss": 0.1273,
      "step": 11550
    },
    {
      "epoch": 85.63197026022304,
      "grad_norm": 1.0017446279525757,
      "learning_rate": 2.878518518518519e-06,
      "loss": 0.1331,
      "step": 11560
    },
    {
      "epoch": 85.70631970260223,
      "grad_norm": 1.1908591985702515,
      "learning_rate": 2.863703703703704e-06,
      "loss": 0.1422,
      "step": 11570
    },
    {
      "epoch": 85.78066914498142,
      "grad_norm": 1.1426571607589722,
      "learning_rate": 2.8488888888888894e-06,
      "loss": 0.1322,
      "step": 11580
    },
    {
      "epoch": 85.85501858736059,
      "grad_norm": 1.2913968563079834,
      "learning_rate": 2.834074074074074e-06,
      "loss": 0.1464,
      "step": 11590
    },
    {
      "epoch": 85.92936802973978,
      "grad_norm": 1.1748099327087402,
      "learning_rate": 2.8192592592592594e-06,
      "loss": 0.1381,
      "step": 11600
    },
    {
      "epoch": 86.0,
      "grad_norm": NaN,
      "learning_rate": 2.8044444444444446e-06,
      "loss": 0.135,
      "step": 11610
    },
    {
      "epoch": 86.07434944237919,
      "grad_norm": 0.9328320026397705,
      "learning_rate": 2.7911111111111113e-06,
      "loss": 0.1477,
      "step": 11620
    },
    {
      "epoch": 86.14869888475836,
      "grad_norm": 0.9718354344367981,
      "learning_rate": 2.7762962962962965e-06,
      "loss": 0.1148,
      "step": 11630
    },
    {
      "epoch": 86.22304832713755,
      "grad_norm": 0.9573171734809875,
      "learning_rate": 2.7614814814814817e-06,
      "loss": 0.126,
      "step": 11640
    },
    {
      "epoch": 86.29739776951673,
      "grad_norm": 1.2331531047821045,
      "learning_rate": 2.746666666666667e-06,
      "loss": 0.1233,
      "step": 11650
    },
    {
      "epoch": 86.37174721189591,
      "grad_norm": 0.8939571380615234,
      "learning_rate": 2.731851851851852e-06,
      "loss": 0.1249,
      "step": 11660
    },
    {
      "epoch": 86.4460966542751,
      "grad_norm": 1.275467872619629,
      "learning_rate": 2.7170370370370373e-06,
      "loss": 0.1296,
      "step": 11670
    },
    {
      "epoch": 86.52044609665427,
      "grad_norm": 1.0043741464614868,
      "learning_rate": 2.702222222222222e-06,
      "loss": 0.1287,
      "step": 11680
    },
    {
      "epoch": 86.59479553903346,
      "grad_norm": 0.993923008441925,
      "learning_rate": 2.6874074074074074e-06,
      "loss": 0.1214,
      "step": 11690
    },
    {
      "epoch": 86.66914498141264,
      "grad_norm": 0.9670910835266113,
      "learning_rate": 2.6725925925925926e-06,
      "loss": 0.1368,
      "step": 11700
    },
    {
      "epoch": 86.74349442379182,
      "grad_norm": 0.8045608997344971,
      "learning_rate": 2.6577777777777782e-06,
      "loss": 0.1246,
      "step": 11710
    },
    {
      "epoch": 86.817843866171,
      "grad_norm": 0.9724733829498291,
      "learning_rate": 2.6429629629629634e-06,
      "loss": 0.1332,
      "step": 11720
    },
    {
      "epoch": 86.89219330855019,
      "grad_norm": 1.0160471200942993,
      "learning_rate": 2.6281481481481486e-06,
      "loss": 0.1351,
      "step": 11730
    },
    {
      "epoch": 86.96654275092936,
      "grad_norm": 1.1508818864822388,
      "learning_rate": 2.6133333333333334e-06,
      "loss": 0.1467,
      "step": 11740
    },
    {
      "epoch": 87.03717472118959,
      "grad_norm": 1.0873217582702637,
      "learning_rate": 2.5985185185185187e-06,
      "loss": 0.1468,
      "step": 11750
    },
    {
      "epoch": 87.11152416356877,
      "grad_norm": 1.145770788192749,
      "learning_rate": 2.583703703703704e-06,
      "loss": 0.1281,
      "step": 11760
    },
    {
      "epoch": 87.18587360594796,
      "grad_norm": 1.0771654844284058,
      "learning_rate": 2.568888888888889e-06,
      "loss": 0.1478,
      "step": 11770
    },
    {
      "epoch": 87.26022304832713,
      "grad_norm": 0.8780266046524048,
      "learning_rate": 2.5540740740740743e-06,
      "loss": 0.1282,
      "step": 11780
    },
    {
      "epoch": 87.33457249070632,
      "grad_norm": 1.113169550895691,
      "learning_rate": 2.5392592592592595e-06,
      "loss": 0.1227,
      "step": 11790
    },
    {
      "epoch": 87.40892193308551,
      "grad_norm": 1.086583137512207,
      "learning_rate": 2.5244444444444447e-06,
      "loss": 0.1461,
      "step": 11800
    },
    {
      "epoch": 87.48327137546468,
      "grad_norm": 1.2397568225860596,
      "learning_rate": 2.5096296296296295e-06,
      "loss": 0.1333,
      "step": 11810
    },
    {
      "epoch": 87.55762081784387,
      "grad_norm": 0.8175050020217896,
      "learning_rate": 2.4948148148148148e-06,
      "loss": 0.1273,
      "step": 11820
    },
    {
      "epoch": 87.63197026022304,
      "grad_norm": 0.9653971791267395,
      "learning_rate": 2.4800000000000004e-06,
      "loss": 0.1346,
      "step": 11830
    },
    {
      "epoch": 87.70631970260223,
      "grad_norm": 1.1250431537628174,
      "learning_rate": 2.4651851851851856e-06,
      "loss": 0.1387,
      "step": 11840
    },
    {
      "epoch": 87.78066914498142,
      "grad_norm": 1.149336338043213,
      "learning_rate": 2.4503703703703704e-06,
      "loss": 0.1196,
      "step": 11850
    },
    {
      "epoch": 87.85501858736059,
      "grad_norm": 1.186930775642395,
      "learning_rate": 2.4355555555555556e-06,
      "loss": 0.1422,
      "step": 11860
    },
    {
      "epoch": 87.92936802973978,
      "grad_norm": 1.2216190099716187,
      "learning_rate": 2.420740740740741e-06,
      "loss": 0.1202,
      "step": 11870
    },
    {
      "epoch": 88.0,
      "grad_norm": 1.4857200384140015,
      "learning_rate": 2.405925925925926e-06,
      "loss": 0.1162,
      "step": 11880
    },
    {
      "epoch": 88.07434944237919,
      "grad_norm": 1.2202932834625244,
      "learning_rate": 2.3911111111111113e-06,
      "loss": 0.1438,
      "step": 11890
    },
    {
      "epoch": 88.14869888475836,
      "grad_norm": 1.2776814699172974,
      "learning_rate": 2.3762962962962965e-06,
      "loss": 0.1244,
      "step": 11900
    },
    {
      "epoch": 88.22304832713755,
      "grad_norm": 1.057978630065918,
      "learning_rate": 2.3614814814814817e-06,
      "loss": 0.1211,
      "step": 11910
    },
    {
      "epoch": 88.29739776951673,
      "grad_norm": 1.2046836614608765,
      "learning_rate": 2.346666666666667e-06,
      "loss": 0.1315,
      "step": 11920
    },
    {
      "epoch": 88.37174721189591,
      "grad_norm": 1.0794756412506104,
      "learning_rate": 2.331851851851852e-06,
      "loss": 0.1363,
      "step": 11930
    },
    {
      "epoch": 88.4460966542751,
      "grad_norm": 0.9158645868301392,
      "learning_rate": 2.3170370370370374e-06,
      "loss": 0.1386,
      "step": 11940
    },
    {
      "epoch": 88.52044609665427,
      "grad_norm": 0.9915268421173096,
      "learning_rate": 2.302222222222222e-06,
      "loss": 0.1358,
      "step": 11950
    },
    {
      "epoch": 88.59479553903346,
      "grad_norm": 1.135282278060913,
      "learning_rate": 2.287407407407408e-06,
      "loss": 0.1404,
      "step": 11960
    },
    {
      "epoch": 88.66914498141264,
      "grad_norm": 1.0694622993469238,
      "learning_rate": 2.272592592592593e-06,
      "loss": 0.1287,
      "step": 11970
    },
    {
      "epoch": 88.74349442379182,
      "grad_norm": 0.8602538704872131,
      "learning_rate": 2.257777777777778e-06,
      "loss": 0.123,
      "step": 11980
    },
    {
      "epoch": 88.817843866171,
      "grad_norm": 1.0821051597595215,
      "learning_rate": 2.242962962962963e-06,
      "loss": 0.1286,
      "step": 11990
    },
    {
      "epoch": 88.89219330855019,
      "grad_norm": 1.1401093006134033,
      "learning_rate": 2.2281481481481482e-06,
      "loss": 0.1425,
      "step": 12000
    },
    {
      "epoch": 88.96654275092936,
      "grad_norm": 0.8647534847259521,
      "learning_rate": 2.2133333333333335e-06,
      "loss": 0.1213,
      "step": 12010
    },
    {
      "epoch": 89.03717472118959,
      "grad_norm": 0.9192796945571899,
      "learning_rate": 2.1985185185185187e-06,
      "loss": 0.1263,
      "step": 12020
    },
    {
      "epoch": 89.11152416356877,
      "grad_norm": 1.0384204387664795,
      "learning_rate": 2.183703703703704e-06,
      "loss": 0.1301,
      "step": 12030
    },
    {
      "epoch": 89.18587360594796,
      "grad_norm": 0.9639797210693359,
      "learning_rate": 2.168888888888889e-06,
      "loss": 0.1386,
      "step": 12040
    },
    {
      "epoch": 89.26022304832713,
      "grad_norm": 1.0259079933166504,
      "learning_rate": 2.1540740740740743e-06,
      "loss": 0.1297,
      "step": 12050
    },
    {
      "epoch": 89.33457249070632,
      "grad_norm": 1.291474461555481,
      "learning_rate": 2.1392592592592595e-06,
      "loss": 0.1366,
      "step": 12060
    },
    {
      "epoch": 89.40892193308551,
      "grad_norm": 1.1416171789169312,
      "learning_rate": 2.1244444444444443e-06,
      "loss": 0.1313,
      "step": 12070
    },
    {
      "epoch": 89.48327137546468,
      "grad_norm": 1.3174993991851807,
      "learning_rate": 2.10962962962963e-06,
      "loss": 0.1381,
      "step": 12080
    },
    {
      "epoch": 89.55762081784387,
      "grad_norm": 1.0791571140289307,
      "learning_rate": 2.094814814814815e-06,
      "loss": 0.1149,
      "step": 12090
    },
    {
      "epoch": 89.63197026022304,
      "grad_norm": 1.095993161201477,
      "learning_rate": 2.08e-06,
      "loss": 0.1338,
      "step": 12100
    },
    {
      "epoch": 89.70631970260223,
      "grad_norm": 1.0644862651824951,
      "learning_rate": 2.065185185185185e-06,
      "loss": 0.1337,
      "step": 12110
    },
    {
      "epoch": 89.78066914498142,
      "grad_norm": 0.7301735281944275,
      "learning_rate": 2.0503703703703704e-06,
      "loss": 0.1233,
      "step": 12120
    },
    {
      "epoch": 89.85501858736059,
      "grad_norm": 0.9551250338554382,
      "learning_rate": 2.0355555555555556e-06,
      "loss": 0.1245,
      "step": 12130
    },
    {
      "epoch": 89.92936802973978,
      "grad_norm": 0.8130927085876465,
      "learning_rate": 2.020740740740741e-06,
      "loss": 0.1293,
      "step": 12140
    },
    {
      "epoch": 90.0,
      "grad_norm": 1.4579576253890991,
      "learning_rate": 2.005925925925926e-06,
      "loss": 0.1351,
      "step": 12150
    },
    {
      "epoch": 90.07434944237919,
      "grad_norm": 1.0650908946990967,
      "learning_rate": 1.9911111111111113e-06,
      "loss": 0.1456,
      "step": 12160
    },
    {
      "epoch": 90.14869888475836,
      "grad_norm": 0.9273539781570435,
      "learning_rate": 1.9762962962962965e-06,
      "loss": 0.1283,
      "step": 12170
    },
    {
      "epoch": 90.22304832713755,
      "grad_norm": 1.0094444751739502,
      "learning_rate": 1.9614814814814817e-06,
      "loss": 0.1297,
      "step": 12180
    },
    {
      "epoch": 90.29739776951673,
      "grad_norm": 1.0813202857971191,
      "learning_rate": 1.9466666666666665e-06,
      "loss": 0.1258,
      "step": 12190
    },
    {
      "epoch": 90.37174721189591,
      "grad_norm": 0.811021625995636,
      "learning_rate": 1.931851851851852e-06,
      "loss": 0.1318,
      "step": 12200
    },
    {
      "epoch": 90.4460966542751,
      "grad_norm": 1.125410795211792,
      "learning_rate": 1.9170370370370374e-06,
      "loss": 0.1148,
      "step": 12210
    },
    {
      "epoch": 90.52044609665427,
      "grad_norm": 0.9443427324295044,
      "learning_rate": 1.9022222222222222e-06,
      "loss": 0.1231,
      "step": 12220
    },
    {
      "epoch": 90.59479553903346,
      "grad_norm": 0.8876029849052429,
      "learning_rate": 1.8874074074074076e-06,
      "loss": 0.129,
      "step": 12230
    },
    {
      "epoch": 90.66914498141264,
      "grad_norm": 1.0852737426757812,
      "learning_rate": 1.8725925925925928e-06,
      "loss": 0.1319,
      "step": 12240
    },
    {
      "epoch": 90.74349442379182,
      "grad_norm": 1.1344600915908813,
      "learning_rate": 1.8577777777777778e-06,
      "loss": 0.1383,
      "step": 12250
    },
    {
      "epoch": 90.817843866171,
      "grad_norm": 1.1164113283157349,
      "learning_rate": 1.842962962962963e-06,
      "loss": 0.1475,
      "step": 12260
    },
    {
      "epoch": 90.89219330855019,
      "grad_norm": 1.2504827976226807,
      "learning_rate": 1.8281481481481483e-06,
      "loss": 0.1322,
      "step": 12270
    },
    {
      "epoch": 90.96654275092936,
      "grad_norm": 0.9316264986991882,
      "learning_rate": 1.8133333333333337e-06,
      "loss": 0.1352,
      "step": 12280
    },
    {
      "epoch": 91.03717472118959,
      "grad_norm": 1.0650142431259155,
      "learning_rate": 1.7985185185185187e-06,
      "loss": 0.1124,
      "step": 12290
    },
    {
      "epoch": 91.11152416356877,
      "grad_norm": 1.0153615474700928,
      "learning_rate": 1.783703703703704e-06,
      "loss": 0.1356,
      "step": 12300
    },
    {
      "epoch": 91.18587360594796,
      "grad_norm": 0.9717698693275452,
      "learning_rate": 1.7688888888888891e-06,
      "loss": 0.1365,
      "step": 12310
    },
    {
      "epoch": 91.26022304832713,
      "grad_norm": 0.8864439129829407,
      "learning_rate": 1.7540740740740741e-06,
      "loss": 0.1304,
      "step": 12320
    },
    {
      "epoch": 91.33457249070632,
      "grad_norm": 1.1067529916763306,
      "learning_rate": 1.7392592592592594e-06,
      "loss": 0.1403,
      "step": 12330
    },
    {
      "epoch": 91.40892193308551,
      "grad_norm": 1.1238598823547363,
      "learning_rate": 1.7244444444444448e-06,
      "loss": 0.1261,
      "step": 12340
    },
    {
      "epoch": 91.48327137546468,
      "grad_norm": 1.2042996883392334,
      "learning_rate": 1.7096296296296298e-06,
      "loss": 0.1198,
      "step": 12350
    },
    {
      "epoch": 91.55762081784387,
      "grad_norm": 1.096350908279419,
      "learning_rate": 1.694814814814815e-06,
      "loss": 0.122,
      "step": 12360
    },
    {
      "epoch": 91.63197026022304,
      "grad_norm": 0.9388993382453918,
      "learning_rate": 1.6800000000000002e-06,
      "loss": 0.1357,
      "step": 12370
    },
    {
      "epoch": 91.70631970260223,
      "grad_norm": 1.0770013332366943,
      "learning_rate": 1.6651851851851852e-06,
      "loss": 0.149,
      "step": 12380
    },
    {
      "epoch": 91.78066914498142,
      "grad_norm": 1.207099437713623,
      "learning_rate": 1.6503703703703704e-06,
      "loss": 0.1273,
      "step": 12390
    },
    {
      "epoch": 91.85501858736059,
      "grad_norm": 0.9668208360671997,
      "learning_rate": 1.6355555555555559e-06,
      "loss": 0.1307,
      "step": 12400
    },
    {
      "epoch": 91.92936802973978,
      "grad_norm": 0.8856545090675354,
      "learning_rate": 1.6207407407407409e-06,
      "loss": 0.1267,
      "step": 12410
    },
    {
      "epoch": 92.0,
      "grad_norm": 1.6350629329681396,
      "learning_rate": 1.605925925925926e-06,
      "loss": 0.132,
      "step": 12420
    },
    {
      "epoch": 92.07434944237919,
      "grad_norm": 1.0018583536148071,
      "learning_rate": 1.5911111111111113e-06,
      "loss": 0.1334,
      "step": 12430
    },
    {
      "epoch": 92.14869888475836,
      "grad_norm": 0.9652614593505859,
      "learning_rate": 1.5762962962962963e-06,
      "loss": 0.1299,
      "step": 12440
    },
    {
      "epoch": 92.22304832713755,
      "grad_norm": 1.323471188545227,
      "learning_rate": 1.5614814814814815e-06,
      "loss": 0.126,
      "step": 12450
    },
    {
      "epoch": 92.29739776951673,
      "grad_norm": 0.835004448890686,
      "learning_rate": 1.546666666666667e-06,
      "loss": 0.1325,
      "step": 12460
    },
    {
      "epoch": 92.37174721189591,
      "grad_norm": 1.163838267326355,
      "learning_rate": 1.531851851851852e-06,
      "loss": 0.1406,
      "step": 12470
    },
    {
      "epoch": 92.4460966542751,
      "grad_norm": 1.3230931758880615,
      "learning_rate": 1.5170370370370372e-06,
      "loss": 0.1549,
      "step": 12480
    },
    {
      "epoch": 92.52044609665427,
      "grad_norm": 1.1271703243255615,
      "learning_rate": 1.5022222222222224e-06,
      "loss": 0.1409,
      "step": 12490
    },
    {
      "epoch": 92.59479553903346,
      "grad_norm": 0.8717882037162781,
      "learning_rate": 1.4874074074074074e-06,
      "loss": 0.1215,
      "step": 12500
    },
    {
      "epoch": 92.66914498141264,
      "grad_norm": 1.148881435394287,
      "learning_rate": 1.4725925925925926e-06,
      "loss": 0.1405,
      "step": 12510
    },
    {
      "epoch": 92.74349442379182,
      "grad_norm": 0.9797939658164978,
      "learning_rate": 1.457777777777778e-06,
      "loss": 0.1226,
      "step": 12520
    },
    {
      "epoch": 92.817843866171,
      "grad_norm": 1.0659946203231812,
      "learning_rate": 1.4429629629629628e-06,
      "loss": 0.1177,
      "step": 12530
    },
    {
      "epoch": 92.89219330855019,
      "grad_norm": 1.0118106603622437,
      "learning_rate": 1.4281481481481483e-06,
      "loss": 0.14,
      "step": 12540
    },
    {
      "epoch": 92.96654275092936,
      "grad_norm": 0.7357839345932007,
      "learning_rate": 1.4133333333333335e-06,
      "loss": 0.1112,
      "step": 12550
    },
    {
      "epoch": 93.03717472118959,
      "grad_norm": 1.0641199350357056,
      "learning_rate": 1.3985185185185185e-06,
      "loss": 0.1147,
      "step": 12560
    },
    {
      "epoch": 93.11152416356877,
      "grad_norm": 0.9236719608306885,
      "learning_rate": 1.3837037037037037e-06,
      "loss": 0.1248,
      "step": 12570
    },
    {
      "epoch": 93.18587360594796,
      "grad_norm": 1.141298532485962,
      "learning_rate": 1.3688888888888891e-06,
      "loss": 0.1245,
      "step": 12580
    },
    {
      "epoch": 93.26022304832713,
      "grad_norm": 1.0873429775238037,
      "learning_rate": 1.3540740740740744e-06,
      "loss": 0.1196,
      "step": 12590
    },
    {
      "epoch": 93.33457249070632,
      "grad_norm": 1.1241199970245361,
      "learning_rate": 1.3392592592592594e-06,
      "loss": 0.1384,
      "step": 12600
    },
    {
      "epoch": 93.40892193308551,
      "grad_norm": 1.276073694229126,
      "learning_rate": 1.3244444444444446e-06,
      "loss": 0.1391,
      "step": 12610
    },
    {
      "epoch": 93.48327137546468,
      "grad_norm": 1.1555910110473633,
      "learning_rate": 1.3096296296296298e-06,
      "loss": 0.1456,
      "step": 12620
    },
    {
      "epoch": 93.55762081784387,
      "grad_norm": 0.8870205283164978,
      "learning_rate": 1.2948148148148148e-06,
      "loss": 0.1352,
      "step": 12630
    },
    {
      "epoch": 93.63197026022304,
      "grad_norm": 1.1544362306594849,
      "learning_rate": 1.28e-06,
      "loss": 0.1172,
      "step": 12640
    },
    {
      "epoch": 93.70631970260223,
      "grad_norm": 1.042495846748352,
      "learning_rate": 1.2651851851851855e-06,
      "loss": 0.1377,
      "step": 12650
    },
    {
      "epoch": 93.78066914498142,
      "grad_norm": 1.2067584991455078,
      "learning_rate": 1.2503703703703705e-06,
      "loss": 0.14,
      "step": 12660
    },
    {
      "epoch": 93.85501858736059,
      "grad_norm": 1.2054884433746338,
      "learning_rate": 1.2355555555555557e-06,
      "loss": 0.1198,
      "step": 12670
    },
    {
      "epoch": 93.92936802973978,
      "grad_norm": 1.123916745185852,
      "learning_rate": 1.220740740740741e-06,
      "loss": 0.1256,
      "step": 12680
    },
    {
      "epoch": 94.0,
      "grad_norm": 1.5239144563674927,
      "learning_rate": 1.2059259259259261e-06,
      "loss": 0.1368,
      "step": 12690
    },
    {
      "epoch": 94.07434944237919,
      "grad_norm": 1.1412638425827026,
      "learning_rate": 1.1911111111111111e-06,
      "loss": 0.1523,
      "step": 12700
    },
    {
      "epoch": 94.14869888475836,
      "grad_norm": 1.1855508089065552,
      "learning_rate": 1.1762962962962963e-06,
      "loss": 0.1275,
      "step": 12710
    },
    {
      "epoch": 94.22304832713755,
      "grad_norm": 1.1636172533035278,
      "learning_rate": 1.1614814814814816e-06,
      "loss": 0.1426,
      "step": 12720
    },
    {
      "epoch": 94.29739776951673,
      "grad_norm": 1.1422455310821533,
      "learning_rate": 1.1466666666666668e-06,
      "loss": 0.1376,
      "step": 12730
    },
    {
      "epoch": 94.37174721189591,
      "grad_norm": 0.8557941317558289,
      "learning_rate": 1.131851851851852e-06,
      "loss": 0.123,
      "step": 12740
    },
    {
      "epoch": 94.4460966542751,
      "grad_norm": 1.098838210105896,
      "learning_rate": 1.1170370370370372e-06,
      "loss": 0.1362,
      "step": 12750
    },
    {
      "epoch": 94.52044609665427,
      "grad_norm": 1.106307029724121,
      "learning_rate": 1.1022222222222222e-06,
      "loss": 0.12,
      "step": 12760
    },
    {
      "epoch": 94.59479553903346,
      "grad_norm": 1.0727509260177612,
      "learning_rate": 1.0874074074074074e-06,
      "loss": 0.1261,
      "step": 12770
    },
    {
      "epoch": 94.66914498141264,
      "grad_norm": 0.8760722875595093,
      "learning_rate": 1.0725925925925926e-06,
      "loss": 0.1254,
      "step": 12780
    },
    {
      "epoch": 94.74349442379182,
      "grad_norm": 1.0715324878692627,
      "learning_rate": 1.0577777777777779e-06,
      "loss": 0.1267,
      "step": 12790
    },
    {
      "epoch": 94.817843866171,
      "grad_norm": 1.4646259546279907,
      "learning_rate": 1.042962962962963e-06,
      "loss": 0.1301,
      "step": 12800
    },
    {
      "epoch": 94.89219330855019,
      "grad_norm": 1.148600459098816,
      "learning_rate": 1.0281481481481483e-06,
      "loss": 0.1075,
      "step": 12810
    },
    {
      "epoch": 94.96654275092936,
      "grad_norm": 1.3971750736236572,
      "learning_rate": 1.0133333333333333e-06,
      "loss": 0.1316,
      "step": 12820
    },
    {
      "epoch": 95.03717472118959,
      "grad_norm": 1.088079571723938,
      "learning_rate": 9.985185185185187e-07,
      "loss": 0.1553,
      "step": 12830
    },
    {
      "epoch": 95.11152416356877,
      "grad_norm": 0.9250969886779785,
      "learning_rate": 9.837037037037037e-07,
      "loss": 0.1325,
      "step": 12840
    },
    {
      "epoch": 95.18587360594796,
      "grad_norm": 1.9145842790603638,
      "learning_rate": 9.68888888888889e-07,
      "loss": 0.1369,
      "step": 12850
    },
    {
      "epoch": 95.26022304832713,
      "grad_norm": 0.7858973741531372,
      "learning_rate": 9.540740740740742e-07,
      "loss": 0.128,
      "step": 12860
    },
    {
      "epoch": 95.33457249070632,
      "grad_norm": 0.907015860080719,
      "learning_rate": 9.392592592592594e-07,
      "loss": 0.1181,
      "step": 12870
    },
    {
      "epoch": 95.40892193308551,
      "grad_norm": 0.9373689889907837,
      "learning_rate": 9.244444444444445e-07,
      "loss": 0.1291,
      "step": 12880
    },
    {
      "epoch": 95.48327137546468,
      "grad_norm": 1.1473052501678467,
      "learning_rate": 9.096296296296297e-07,
      "loss": 0.1317,
      "step": 12890
    },
    {
      "epoch": 95.55762081784387,
      "grad_norm": 1.245475172996521,
      "learning_rate": 8.948148148148149e-07,
      "loss": 0.1219,
      "step": 12900
    },
    {
      "epoch": 95.63197026022304,
      "grad_norm": 0.9590080976486206,
      "learning_rate": 8.8e-07,
      "loss": 0.1332,
      "step": 12910
    },
    {
      "epoch": 95.70631970260223,
      "grad_norm": 1.2197246551513672,
      "learning_rate": 8.651851851851853e-07,
      "loss": 0.142,
      "step": 12920
    },
    {
      "epoch": 95.78066914498142,
      "grad_norm": 1.3037476539611816,
      "learning_rate": 8.503703703703705e-07,
      "loss": 0.1244,
      "step": 12930
    },
    {
      "epoch": 95.85501858736059,
      "grad_norm": 0.9743920564651489,
      "learning_rate": 8.355555555555556e-07,
      "loss": 0.1336,
      "step": 12940
    },
    {
      "epoch": 95.92936802973978,
      "grad_norm": 1.521545648574829,
      "learning_rate": 8.207407407407408e-07,
      "loss": 0.136,
      "step": 12950
    },
    {
      "epoch": 96.0,
      "grad_norm": 1.8138232231140137,
      "learning_rate": 8.05925925925926e-07,
      "loss": 0.1313,
      "step": 12960
    },
    {
      "epoch": 96.07434944237919,
      "grad_norm": 1.0206246376037598,
      "learning_rate": 7.911111111111111e-07,
      "loss": 0.1343,
      "step": 12970
    },
    {
      "epoch": 96.14869888475836,
      "grad_norm": 1.019060492515564,
      "learning_rate": 7.762962962962964e-07,
      "loss": 0.1237,
      "step": 12980
    },
    {
      "epoch": 96.22304832713755,
      "grad_norm": 1.3881916999816895,
      "learning_rate": 7.614814814814815e-07,
      "loss": 0.1324,
      "step": 12990
    },
    {
      "epoch": 96.29739776951673,
      "grad_norm": 0.8078171610832214,
      "learning_rate": 7.466666666666668e-07,
      "loss": 0.1254,
      "step": 13000
    },
    {
      "epoch": 96.37174721189591,
      "grad_norm": 1.0248907804489136,
      "learning_rate": 7.318518518518519e-07,
      "loss": 0.1298,
      "step": 13010
    },
    {
      "epoch": 96.4460966542751,
      "grad_norm": 0.8991374969482422,
      "learning_rate": 7.17037037037037e-07,
      "loss": 0.1185,
      "step": 13020
    },
    {
      "epoch": 96.52044609665427,
      "grad_norm": 1.0233914852142334,
      "learning_rate": 7.022222222222223e-07,
      "loss": 0.1279,
      "step": 13030
    },
    {
      "epoch": 96.59479553903346,
      "grad_norm": 1.1870893239974976,
      "learning_rate": 6.874074074074074e-07,
      "loss": 0.135,
      "step": 13040
    },
    {
      "epoch": 96.66914498141264,
      "grad_norm": 1.028826355934143,
      "learning_rate": 6.725925925925926e-07,
      "loss": 0.1266,
      "step": 13050
    },
    {
      "epoch": 96.74349442379182,
      "grad_norm": 1.1349866390228271,
      "learning_rate": 6.577777777777779e-07,
      "loss": 0.1455,
      "step": 13060
    },
    {
      "epoch": 96.817843866171,
      "grad_norm": 1.0297801494598389,
      "learning_rate": 6.42962962962963e-07,
      "loss": 0.1251,
      "step": 13070
    },
    {
      "epoch": 96.89219330855019,
      "grad_norm": 1.2935044765472412,
      "learning_rate": 6.281481481481481e-07,
      "loss": 0.1412,
      "step": 13080
    },
    {
      "epoch": 96.96654275092936,
      "grad_norm": 1.0536483526229858,
      "learning_rate": 6.133333333333333e-07,
      "loss": 0.1317,
      "step": 13090
    },
    {
      "epoch": 97.03717472118959,
      "grad_norm": 1.0166947841644287,
      "learning_rate": 5.985185185185185e-07,
      "loss": 0.1291,
      "step": 13100
    },
    {
      "epoch": 97.11152416356877,
      "grad_norm": 1.2328182458877563,
      "learning_rate": 5.837037037037038e-07,
      "loss": 0.1305,
      "step": 13110
    },
    {
      "epoch": 97.18587360594796,
      "grad_norm": 1.2252084016799927,
      "learning_rate": 5.68888888888889e-07,
      "loss": 0.1288,
      "step": 13120
    },
    {
      "epoch": 97.26022304832713,
      "grad_norm": 1.0983195304870605,
      "learning_rate": 5.540740740740741e-07,
      "loss": 0.125,
      "step": 13130
    },
    {
      "epoch": 97.33457249070632,
      "grad_norm": 1.1319777965545654,
      "learning_rate": 5.392592592592593e-07,
      "loss": 0.111,
      "step": 13140
    },
    {
      "epoch": 97.40892193308551,
      "grad_norm": 1.0919666290283203,
      "learning_rate": 5.244444444444445e-07,
      "loss": 0.1144,
      "step": 13150
    },
    {
      "epoch": 97.48327137546468,
      "grad_norm": 0.9978897571563721,
      "learning_rate": 5.096296296296296e-07,
      "loss": 0.1232,
      "step": 13160
    },
    {
      "epoch": 97.55762081784387,
      "grad_norm": 1.138744831085205,
      "learning_rate": 4.948148148148148e-07,
      "loss": 0.1354,
      "step": 13170
    },
    {
      "epoch": 97.63197026022304,
      "grad_norm": 1.099071741104126,
      "learning_rate": 4.800000000000001e-07,
      "loss": 0.1411,
      "step": 13180
    },
    {
      "epoch": 97.70631970260223,
      "grad_norm": 1.047254204750061,
      "learning_rate": 4.6518518518518523e-07,
      "loss": 0.1274,
      "step": 13190
    },
    {
      "epoch": 97.78066914498142,
      "grad_norm": 0.914107084274292,
      "learning_rate": 4.503703703703704e-07,
      "loss": 0.1453,
      "step": 13200
    },
    {
      "epoch": 97.85501858736059,
      "grad_norm": 1.2288618087768555,
      "learning_rate": 4.355555555555556e-07,
      "loss": 0.1218,
      "step": 13210
    },
    {
      "epoch": 97.92936802973978,
      "grad_norm": 1.1620545387268066,
      "learning_rate": 4.2074074074074077e-07,
      "loss": 0.163,
      "step": 13220
    },
    {
      "epoch": 98.0,
      "grad_norm": 2.043727159500122,
      "learning_rate": 4.0592592592592594e-07,
      "loss": 0.1315,
      "step": 13230
    },
    {
      "epoch": 98.07434944237919,
      "grad_norm": 1.106590747833252,
      "learning_rate": 3.9111111111111115e-07,
      "loss": 0.1194,
      "step": 13240
    },
    {
      "epoch": 98.14869888475836,
      "grad_norm": 1.0532087087631226,
      "learning_rate": 3.762962962962963e-07,
      "loss": 0.1485,
      "step": 13250
    },
    {
      "epoch": 98.22304832713755,
      "grad_norm": 0.9391377568244934,
      "learning_rate": 3.6148148148148154e-07,
      "loss": 0.1242,
      "step": 13260
    },
    {
      "epoch": 98.29739776951673,
      "grad_norm": 1.309147834777832,
      "learning_rate": 3.466666666666667e-07,
      "loss": 0.1529,
      "step": 13270
    },
    {
      "epoch": 98.37174721189591,
      "grad_norm": 0.9754239320755005,
      "learning_rate": 3.3185185185185186e-07,
      "loss": 0.1389,
      "step": 13280
    },
    {
      "epoch": 98.4460966542751,
      "grad_norm": 1.2894083261489868,
      "learning_rate": 3.170370370370371e-07,
      "loss": 0.1273,
      "step": 13290
    },
    {
      "epoch": 98.52044609665427,
      "grad_norm": 1.0008964538574219,
      "learning_rate": 3.0222222222222225e-07,
      "loss": 0.1188,
      "step": 13300
    },
    {
      "epoch": 98.59479553903346,
      "grad_norm": 0.9119160175323486,
      "learning_rate": 2.874074074074074e-07,
      "loss": 0.1263,
      "step": 13310
    },
    {
      "epoch": 98.66914498141264,
      "grad_norm": 0.9755094647407532,
      "learning_rate": 2.7259259259259263e-07,
      "loss": 0.112,
      "step": 13320
    },
    {
      "epoch": 98.74349442379182,
      "grad_norm": 1.0904332399368286,
      "learning_rate": 2.577777777777778e-07,
      "loss": 0.132,
      "step": 13330
    },
    {
      "epoch": 98.817843866171,
      "grad_norm": 1.1657662391662598,
      "learning_rate": 2.4296296296296296e-07,
      "loss": 0.1289,
      "step": 13340
    },
    {
      "epoch": 98.89219330855019,
      "grad_norm": 1.4477548599243164,
      "learning_rate": 2.2814814814814817e-07,
      "loss": 0.1314,
      "step": 13350
    },
    {
      "epoch": 98.96654275092936,
      "grad_norm": 1.1637165546417236,
      "learning_rate": 2.1333333333333334e-07,
      "loss": 0.1278,
      "step": 13360
    },
    {
      "epoch": 99.03717472118959,
      "grad_norm": 1.3086899518966675,
      "learning_rate": 1.9851851851851856e-07,
      "loss": 0.1566,
      "step": 13370
    },
    {
      "epoch": 99.11152416356877,
      "grad_norm": 1.147420883178711,
      "learning_rate": 1.8370370370370372e-07,
      "loss": 0.1316,
      "step": 13380
    },
    {
      "epoch": 99.18587360594796,
      "grad_norm": 1.087030053138733,
      "learning_rate": 1.6888888888888888e-07,
      "loss": 0.1258,
      "step": 13390
    },
    {
      "epoch": 99.26022304832713,
      "grad_norm": 1.018717646598816,
      "learning_rate": 1.540740740740741e-07,
      "loss": 0.1212,
      "step": 13400
    }
  ],
  "logging_steps": 10,
  "max_steps": 13500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 100,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.352088835256484e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
